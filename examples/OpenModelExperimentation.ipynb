{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namiyousef/argument-mining/blob/develop/examples/OpenModelExperimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2ec16d",
      "metadata": {
        "id": "1c2ec16d"
      },
      "source": [
        "# Open Model Experimentation\n",
        "\n",
        "This notebook allows you to access testing data used for the project, as well as trained models. The original training data is not hosted here.\n",
        "\n",
        "The document will first take you through steps to set-up the notebook so that you can access the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Set up\n",
        "\n",
        "Firstly, please access the following [link](https://drive.google.com/drive/folders/1LzSEc25qZHSB5snig1Ro_pgZOnmZ9p2W?usp=sharing) to get access to the drive. This contains a folder named test/ that includes adversarial examples and a folder named tmpdir/ that includes the trained models. You can look into these in greater detail, however the names are not user friendly. These models are identical to those pushed on HuggingFace under https://huggingface.co/ucabqfe.\n",
        "\n",
        "Once you have done that, please go on your drive under \"Shared with me\", right click the Desktop/ folder and select \"Add shortcut to drive\". This will allow you to access the data from within Colab. You can find more information on this here: https://github.com/namiyousef/argument-mining/issues/38. You can ignore the aspects on Authentication as the repository has now made public and published as a package."
      ],
      "metadata": {
        "id": "XWQS_TUhKKej"
      },
      "id": "XWQS_TUhKKej"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-dev-tools\n",
        "\n",
        "import os\n",
        "from colabtools.utils import mount_drive, get_gpu_utilization\n",
        "\n",
        "# get the path to your drive, e.g. drive/MyDrive\n",
        "DRIVE_PATH = mount_drive()\n",
        "\n",
        "\n",
        "# set paths\n",
        "PATH_TO_MODELS = os.path.join(DRIVE_PATH, 'Desktop/tmpdir/job')\n",
        "PATH_TO_ADVERSARIAL = os.path.join(DRIVE_PATH, 'Desktop/test')"
      ],
      "metadata": {
        "id": "OBzKULKKqG1r",
        "outputId": "14d82090-f47c-470e-fc75-97cfab6af92b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OBzKULKKqG1r",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-dev-tools in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from colab-dev-tools) (7.352.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from colab-dev-tools) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from colab-dev-tools) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->colab-dev-tools) (4.1.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mount successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install argminer\n",
        "\n",
        "# -- public imports\n",
        "\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from pandas.testing import assert_frame_equal\n",
        "import time\n",
        "\n",
        "# -- private imports\n",
        "from colabtools.utils import move_to_device\n",
        "from colabtools.config import DEVICE\n",
        "\n",
        "import argminer\n",
        "from argminer.data import ArgumentMiningDataset, TUDarmstadtProcessor, PersuadeProcessor\n",
        "from argminer.evaluation import inference, _get_scores_agg\n",
        "from argminer.utils import encode_model_name\n",
        "from argminer.config import LABELS_MAP_DICT, MODEL_MAP_DICT"
      ],
      "metadata": {
        "id": "Ok5Lt6ePq-4x",
        "outputId": "aa9e554a-6ea9-4944-d533-3628f4bede43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ok5Lt6ePq-4x",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: argminer in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from argminer) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from argminer) (0.1.96)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from argminer) (0.0)\n",
            "Requirement already satisfied: plac in /usr/local/lib/python3.7/dist-packages (from argminer) (1.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from argminer) (1.21.5)\n",
            "Requirement already satisfied: ml-dev-tools in /usr/local/lib/python3.7/dist-packages (from argminer) (0.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from argminer) (3.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from argminer) (4.64.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from argminer) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from argminer) (1.3.5)\n",
            "Requirement already satisfied: colab-dev-tools in /usr/local/lib/python3.7/dist-packages (from argminer) (0.0.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from argminer) (4.18.0)\n",
            "Requirement already satisfied: connexion[swagger-ui] in /usr/local/lib/python3.7/dist-packages (from argminer) (2.13.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from colab-dev-tools->argminer) (7.352.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (21.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (1.1.0)\n",
            "Requirement already satisfied: jsonschema<5,>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (4.3.3)\n",
            "Requirement already satisfied: clickclick<21,>=1.2 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (20.10.2)\n",
            "Requirement already satisfied: inflection<0.6,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=1 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (4.11.3)\n",
            "Requirement already satisfied: flask<3,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (1.1.4)\n",
            "Requirement already satisfied: werkzeug<3,>=1.0 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (1.0.1)\n",
            "Requirement already satisfied: PyYAML<7,>=5.1 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (2.23.0)\n",
            "Requirement already satisfied: swagger-ui-bundle<0.1,>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from connexion[swagger-ui]->argminer) (0.0.9)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from clickclick<21,>=1.2->connexion[swagger-ui]->argminer) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask<3,>=1.0.4->connexion[swagger-ui]->argminer) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1->connexion[swagger-ui]->argminer) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1->connexion[swagger-ui]->argminer) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask<3,>=1.0.4->connexion[swagger-ui]->argminer) (2.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.5.1->connexion[swagger-ui]->argminer) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.5.1->connexion[swagger-ui]->argminer) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.5.1->connexion[swagger-ui]->argminer) (5.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20->connexion[swagger-ui]->argminer) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.9.1->connexion[swagger-ui]->argminer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.9.1->connexion[swagger-ui]->argminer) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.9.1->connexion[swagger-ui]->argminer) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.9.1->connexion[swagger-ui]->argminer) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->argminer) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->argminer) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->argminer) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->argminer) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->argminer) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->argminer) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->argminer) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->argminer) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->argminer) (1.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->argminer) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->argminer) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->argminer) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->argminer) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->argminer) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on Adversarial Examples"
      ],
      "metadata": {
        "id": "Gau-TAvBxBEo"
      },
      "id": "Gau-TAvBxBEo"
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "MODEL_NAME = 'ucabqfe/roberta_AAE_io' #   select huggingface model name (this is one of our models, see ucabqfe/ models on HuggingFace)\n",
        "# alternatively, you should be able to specify to a path on the trained models. You can decode the names of the models using argminer.utils.decode_model_name()\n",
        "\n",
        "model_metadata = MODEL_MAP_DICT[MODEL_NAME]\n",
        "dataset = model_metadata['dataset']\n",
        "max_length = model_metadata['max_length']\n",
        "tokenizer_name = model_metadata['hugging_face_model_name']\n",
        "strategy = MODEL_NAME.split('_')[-1]\n",
        "print(f'Running model: {MODEL_NAME}:')\n",
        "print('=======================================================================')\n",
        "PATH_TO_ATTACK = os.path.join(PATH_TO_ADVERSARIAL, strategy)\n",
        "for adversarial_attack in os.listdir(PATH_TO_ATTACK):\n",
        "  model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, add_prefix_space=True)\n",
        "\n",
        "  Processor = getattr(argminer.data, f'{dataset}Processor')\n",
        "  processor = Processor(os.path.join(PATH_TO_ATTACK, adversarial_attack)).from_json(status='postprocessed')\n",
        "  df_text = processor.dataframe[['text', 'labels']]\n",
        "  df_label_map = LABELS_MAP_DICT[dataset][strategy]\n",
        "  dataset = ArgumentMiningDataset(df_label_map, df_text, tokenizer, max_length, f'standard_{strategy}', is_train=False)\n",
        "  dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  df_metrics, df_scores = inference(model, dataloader)\n",
        "  metrics, df_scores_agg = _get_scores_agg(df_scores)\n",
        "  print(f'Completed inference on adversarial attack: {adversarial_attack}. Macro Metrics: {metrics}\\n')\n",
        "  print(df_scores_agg.to_string(), '\\n\\n')\n"
      ],
      "metadata": {
        "id": "8OdVfOQsw254",
        "outputId": "3bbf6b8e-9849-4112-fc40-fe51c1844bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "id": "8OdVfOQsw254",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running model: ucabqfe/roberta_AAE_io:\n",
            "=======================================================================\n",
            "Prediction time: 0.0292\n",
            "Agg to word time: 10.8\n",
            "Get predstring time: 0.778\n",
            "Evaluate time: 3.87\n",
            "Batch 1 complete.\n",
            "Prediction time: 0.0282\n",
            "Agg to word time: 3.02\n",
            "Get predstring time: 0.172\n",
            "Evaluate time: 0.81\n",
            "Batch 2 complete.\n",
            "Completed inference on adversarial attack: spellingError. Macro Metrics: {'macro_f1': 0.5816210181066526, 'macro_recall': 0.6365550482985234, 'macro_precision': 0.5411039776205414}\n",
            "        tp   fn   fp        f1    recall  precision\n",
            "class                                              \n",
            "0      667  620  681  0.506262  0.518260   0.494807\n",
            "1       89   50   63  0.611684  0.640288   0.585526\n",
            "2      173  120  293  0.455863  0.590444   0.371245\n",
            "3      633  161  255  0.752675  0.797229   0.712838 \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c5f00a56e194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mProcessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margminer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{dataset}Processor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_ATTACK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial_attack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'postprocessed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mdf_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'argminer.data' has no attribute '<argminer.data.ArgumentMiningDataset object at 0x7f2660b0c4d0>Processor'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "If you have access to the base data, you can train models with the script below.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1S1EckeGuT9M"
      },
      "id": "1S1EckeGuT9M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59272e27",
      "metadata": {
        "id": "59272e27"
      },
      "outputs": [],
      "source": [
        "# constants (these will be abstracted away by inputs that you give to run)\n",
        "PATH_TO_DATA_DIR = '' # specify this path\n",
        "\n",
        "\n",
        "# -- model specific configurations\n",
        "model_name = 'google/bigbird-roberta-base'\n",
        "max_length = 1024\n",
        "\n",
        "# -- training configurations\n",
        "epochs = 5\n",
        "batch_size = 2\n",
        "verbose = 2\n",
        "save_freq = 2\n",
        "\n",
        "# -- dataset configurations\n",
        "dataset_name = 'Persuade'\n",
        "\n",
        "# -- experiment configurations\n",
        "strategy = 'standard_bieo'\n",
        "strat_name, strat_label = strategy.split('_')\n",
        "\n",
        "# -- inferred configurations\n",
        "df_label_map = LABELS_MAP_DICT[dataset_name][strat_label]\n",
        "num_labels = len(set(df_label_map.label))\n",
        "Processor = eval(f'{dataset_name}Processor')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b81b26",
      "metadata": {
        "id": "f5b81b26"
      },
      "source": [
        "### Tokenizer, Model and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882197f3",
      "metadata": {
        "id": "882197f3",
        "outputId": "1c6866c9-e63f-414a-8218-80d7484a4f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels) \n",
        "optimizer = torch.optim.Adam(params=model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3e225a",
      "metadata": {
        "id": "cb3e225a"
      },
      "source": [
        "### Dataset \n",
        "Note this will change as the Processor develops. On the cluster you will need to use different options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc67877",
      "metadata": {
        "id": "ccc67877",
        "outputId": "22ba1471-aed5-42ca-aebc-6bbc9fdcf1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'argminer.data.PersuadeProcessor'>\n"
          ]
        }
      ],
      "source": [
        "processor = Processor(PATH_TO_DATA_DIR)\n",
        "processor = processor.from_json()\n",
        "df_total = processor.dataframe\n",
        "\n",
        "df_dict = processor.get_tts(test_size=0.3, val_size=0.1)\n",
        "df_train = df_dict.get('train')[['text', 'labels']]\n",
        "df_test = df_dict.get('test')[['text', 'labels']]\n",
        "df_val = df_dict.get('val')[['text', 'labels']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf36a48",
      "metadata": {
        "id": "ccf36a48"
      },
      "outputs": [],
      "source": [
        "train_set = ArgumentMiningDataset(df_label_map, df_train, tokenizer, max_length, strategy)\n",
        "test_set = ArgumentMiningDataset(df_label_map, df_test, tokenizer, max_length, strategy, is_train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2aa673e",
      "metadata": {
        "id": "d2aa673e",
        "outputId": "ad9cf94d-5991-4a83-8443-af62a83b3a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model pushed to device: cuda\n",
            "EPOCH 1 STARTED\n",
            "---------------\n",
            "GPU Utilisation at batch 1 after data loading: 2739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py:978: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Utilisation at batch 1 after training: 5763\n",
            "Batch 1 complete. Time taken: load(0.0248), train(2.19), total(2.22). \n",
            "GPU Utilisation at batch 2 after data loading: 2797\n",
            "GPU Utilisation at batch 2 after training: 7251\n",
            "Batch 2 complete. Time taken: load(0.0156), train(2.04), total(2.05). \n",
            "GPU Utilisation at batch 3 after data loading: 2797\n",
            "GPU Utilisation at batch 3 after training: 7251\n",
            "Batch 3 complete. Time taken: load(0.0133), train(2.03), total(2.04). \n",
            "GPU Utilisation at batch 4 after data loading: 2797\n",
            "GPU Utilisation at batch 4 after training: 7251\n",
            "Batch 4 complete. Time taken: load(0.0162), train(2.04), total(2.06). \n",
            "GPU Utilisation at batch 5 after data loading: 2797\n",
            "GPU Utilisation at batch 5 after training: 7251\n",
            "Batch 5 complete. Time taken: load(0.0172), train(2.03), total(2.04). \n",
            "GPU Utilisation at batch 6 after data loading: 2797\n",
            "GPU Utilisation at batch 6 after training: 7251\n",
            "Batch 6 complete. Time taken: load(0.024), train(2.02), total(2.05). \n",
            "GPU Utilisation at batch 7 after data loading: 2797\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d4ba2c5492e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2878\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m         )\n\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2150\u001b[0m             \u001b[0mto_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m             \u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m         )\n\u001b[1;32m   2154\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, band_mask, from_mask, to_mask, blocked_encoder_mask, return_dict)\u001b[0m\n\u001b[1;32m   1639\u001b[0m                     \u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m                 )\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0mto_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0mfrom_blocked_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0mto_blocked_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         )\n\u001b[1;32m   1496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask)\u001b[0m\n\u001b[1;32m   1395\u001b[0m             ), \"BigBird cannot be used as a decoder when config.attention_type != 'original_full'\"\n\u001b[1;32m   1396\u001b[0m             self_outputs = self.self(\n\u001b[0;32m-> 1397\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_blocked_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_blocked_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m             )\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mplan_from_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mplan_num_rand_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mbigbird_block_sparse_attention\u001b[0;34m(self, query_layer, key_layer, value_layer, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, n_heads, n_rand_blocks, attention_head_size, from_block_size, to_block_size, batch_size, from_seq_len, to_seq_len, seed, plan_from_length, plan_num_rand_blocks, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mrand_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mrand_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0mrand_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mrand_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_attn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if not os.path.exists('models'):\n",
        "  os.makedirs('models')\n",
        "  print('models directory created!')\n",
        "model.to(DEVICE)\n",
        "print(f'Model pushed to device: {DEVICE}')\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    start_epoch_message = f'EPOCH {epoch + 1} STARTED'\n",
        "    print(start_epoch_message)\n",
        "    print(f'{\"-\" * len(start_epoch_message)}')\n",
        "    start_epoch = time.time()\n",
        "\n",
        "    start_load = time.time()\n",
        "    training_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        start_train = time.time()\n",
        "        inputs = move_to_device(inputs, DEVICE)\n",
        "        targets = move_to_device(targets, DEVICE)\n",
        "        if DEVICE != 'cpu':\n",
        "            print(f'GPU Utilisation at batch {i+1} after data loading: {get_gpu_utilization()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss, outputs = model(\n",
        "            labels=targets,\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            return_dict=False\n",
        "        )\n",
        "        if DEVICE != 'cpu':\n",
        "            print(f'GPU Utilisation at batch {i+1} after training: {get_gpu_utilization()}')\n",
        "\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del targets, inputs, loss, outputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        end_train = time.time()\n",
        "\n",
        "        if verbose > 1:\n",
        "            print(\n",
        "                f'Batch {i + 1} complete. Time taken: load({start_train - start_load:.3g}), '\n",
        "                f'train({end_train - start_train:.3g}), total({end_train - start_load:.3g}). '\n",
        "            )\n",
        "        start_load = time.time()\n",
        "\n",
        "    print_message = f'Epoch {epoch + 1}/{epochs} complete. ' \\\n",
        "                    f'Time taken: {start_load - start_epoch:.3g}. ' \\\n",
        "                    f'Loss: {training_loss/(i+1): .3g}'\n",
        "\n",
        "    if verbose:\n",
        "        print(f'{\"-\" * len(print_message)}')\n",
        "        print(print_message)\n",
        "        print(f'{\"-\" * len(print_message)}')\n",
        "\n",
        "    if epoch % save_freq == 0:\n",
        "        encoded_model_name = encode_model_name(model_name, epoch+1)\n",
        "        save_path = f'models/{encoded_model_name}'\n",
        "        model.save_pretrained(save_path)\n",
        "        print(f'Model saved at epoch {epoch+1} at: {save_path}')\n",
        "\n",
        "encoded_model_name = encode_model_name(model_name, 'final')\n",
        "save_path = f'models/{encoded_model_name}'\n",
        "model.save_pretrained(save_path)\n",
        "print(f'Model saved at epoch {epoch + 1} at: {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12afdc17",
      "metadata": {
        "id": "12afdc17"
      },
      "outputs": [],
      "source": [
        "# load trained model\n",
        "path = ''\n",
        "trained_model = AutoModelForTokenClassification.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics, df_scores = inference(trained_model, test_loader)"
      ],
      "metadata": {
        "id": "VQcpzbhuPPwu"
      },
      "id": "VQcpzbhuPPwu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd86b6d3",
      "metadata": {
        "id": "bd86b6d3"
      },
      "outputs": [],
      "source": [
        "_get_scores_agg(df_scores)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "argument-mining",
      "language": "python",
      "name": "argument-mining"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "End-to-end_no_GPU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}