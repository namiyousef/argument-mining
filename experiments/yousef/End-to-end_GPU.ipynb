{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c2ec16d",
      "metadata": {
        "id": "1c2ec16d"
      },
      "source": [
        "# End-to-end\n",
        "\n",
        "This notebook should form the core skeleton of the 'run' function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Set up"
      ],
      "metadata": {
        "id": "XWQS_TUhKKej"
      },
      "id": "XWQS_TUhKKej"
    },
    {
      "cell_type": "code",
      "source": [
        "# -- env setup\n",
        "import os\n",
        "import gc\n",
        "\n",
        "!python3.7 -m pip install git+https://github.com/namiyousef/colab-utils.git\n",
        "from colabtools.utils import get_gpu_utilization, mount_drive, install_private_library\n",
        "\n",
        "drive_path = mount_drive()\n",
        "project_path = os.path.join(drive_path, 'argument-mining')\n",
        "development_dir = os.path.join(drive_path, 'argument-mining/argminer')\n",
        "\n",
        "install_private_library(os.path.join(project_path, 'data/github_config.json'), 'argument-mining')"
      ],
      "metadata": {
        "id": "X7OwoLnBKHQB",
        "outputId": "1a8516e4-4d63-4fde-b736-717f6fe3357e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X7OwoLnBKHQB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/namiyousef/colab-utils.git\n",
            "  Cloning https://github.com/namiyousef/colab-utils.git to /tmp/pip-req-build-5i7ydzp6\n",
            "  Running command git clone -q https://github.com/namiyousef/colab-utils.git /tmp/pip-req-build-5i7ydzp6\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from colabtools==0.0.5) (7.352.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from colabtools==0.0.5) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->colabtools==0.0.5) (3.10.0.2)\n",
            "Building wheels for collected packages: colabtools\n",
            "  Building wheel for colabtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabtools: filename=colabtools-0.0.5-py3-none-any.whl size=3585 sha256=b0808ec10b9aba31db65c105148d19c652618702710623944c7f38fcac29e020\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t4irkesa/wheels/1c/35/c0/364531e4ff0f0fe0f3296c80f1ee668b03ae6c6c378c5a44bf\n",
            "Successfully built colabtools\n",
            "Installing collected packages: colabtools\n",
            "Successfully installed colabtools-0.0.5\n",
            "Google Drive import successful.\n",
            "CUDA device detected. Using GPU...\n",
            "Mounted at /content/drive\n",
            "Google Drive mount successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1302e4f",
      "metadata": {
        "id": "c1302e4f"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7d8ed729",
      "metadata": {
        "id": "7d8ed729",
        "outputId": "efb3ba2f-ec41-4a60-d4fb-7a4fdd2b51dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import of ArgMiner successful\n"
          ]
        }
      ],
      "source": [
        "# -- public imports\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from pandas.testing import assert_frame_equal\n",
        "import time\n",
        "\n",
        "# -- private imports\n",
        "from colabtools.utils import move_to_device\n",
        "from colabtools.config import DEVICE\n",
        "\n",
        "# -- dev imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from argminer.data import ArgumentMiningDataset, TUDarmstadtProcessor, PersuadeProcessor\n",
        "from argminer.evaluation import inference\n",
        "from argminer.utils import encode_model_name\n",
        "from argminer.config import LABELS_MAP_DICT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "59272e27",
      "metadata": {
        "id": "59272e27"
      },
      "outputs": [],
      "source": [
        "# constants (these will be abstracted away by inputs that you give to run)\n",
        "\n",
        "# -- model specific configurations\n",
        "model_name = 'google/bigbird-roberta-base'\n",
        "max_length = 1024\n",
        "\n",
        "# -- training configurations\n",
        "epochs = 5\n",
        "batch_size = 2\n",
        "verbose = 2\n",
        "save_freq = 2\n",
        "\n",
        "# -- dataset configurations\n",
        "dataset_name = 'Persuade'\n",
        "\n",
        "# -- experiment configurations\n",
        "strategy = 'standard_bieo'\n",
        "strat_name, strat_label = strategy.split('_')\n",
        "\n",
        "# -- inferred configurations\n",
        "df_label_map = LABELS_MAP_DICT[dataset_name][strat_label]\n",
        "num_labels = len(set(df_label_map.label))\n",
        "Processor = eval(f'{dataset_name}Processor')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b81b26",
      "metadata": {
        "id": "f5b81b26"
      },
      "source": [
        "### Tokenizer, Model and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "882197f3",
      "metadata": {
        "id": "882197f3",
        "outputId": "1c6866c9-e63f-414a-8218-80d7484a4f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels) \n",
        "# TODO force_download\n",
        "# TODO add option for optimizer\n",
        "optimizer = torch.optim.Adam(params=model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3e225a",
      "metadata": {
        "id": "cb3e225a"
      },
      "source": [
        "### Dataset \n",
        "Note this will change as the Processor develops. On the cluster you will need to use different options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ccc67877",
      "metadata": {
        "id": "ccc67877",
        "outputId": "22ba1471-aed5-42ca-aebc-6bbc9fdcf1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'argminer.data.PersuadeProcessor'>\n"
          ]
        }
      ],
      "source": [
        "processor = Processor(os.path.join(project_path, f'data/{strat_label}'))\n",
        "processor = processor.from_json()\n",
        "df_total = processor.dataframe\n",
        "\n",
        "df_dict = processor.get_tts(test_size=0.3, val_size=0.1)\n",
        "df_train = df_dict.get('train')[['text', 'labels']]\n",
        "df_test = df_dict.get('test')[['text', 'labels']]\n",
        "df_val = df_dict.get('val')[['text', 'labels']]\n",
        "\n",
        "#df_train = df_total[['text', 'labels']].head(10) \n",
        "#df_test = df_total[['text', 'labels']].tail(201)\n",
        "\n",
        "\n",
        "#assert_frame_equal(df_total[['text', 'labels']], pd.concat([df_train, df_test]))\n",
        "\n",
        "# todo this changes NOTE FIXED BT STRATEGY!!\n",
        "# todo this needs to get updated as well.....\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ccf36a48",
      "metadata": {
        "id": "ccf36a48"
      },
      "outputs": [],
      "source": [
        "train_set = ArgumentMiningDataset(df_label_map, df_train, tokenizer, max_length, strategy)\n",
        "test_set = ArgumentMiningDataset(df_label_map, df_test, tokenizer, max_length, strategy, is_train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d2aa673e",
      "metadata": {
        "id": "d2aa673e",
        "outputId": "ad9cf94d-5991-4a83-8443-af62a83b3a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model pushed to device: cuda\n",
            "EPOCH 1 STARTED\n",
            "---------------\n",
            "GPU Utilisation at batch 1 after data loading: 2739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py:978: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  * num_indices_to_pick_from\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Utilisation at batch 1 after training: 5763\n",
            "Batch 1 complete. Time taken: load(0.0248), train(2.19), total(2.22). \n",
            "GPU Utilisation at batch 2 after data loading: 2797\n",
            "GPU Utilisation at batch 2 after training: 7251\n",
            "Batch 2 complete. Time taken: load(0.0156), train(2.04), total(2.05). \n",
            "GPU Utilisation at batch 3 after data loading: 2797\n",
            "GPU Utilisation at batch 3 after training: 7251\n",
            "Batch 3 complete. Time taken: load(0.0133), train(2.03), total(2.04). \n",
            "GPU Utilisation at batch 4 after data loading: 2797\n",
            "GPU Utilisation at batch 4 after training: 7251\n",
            "Batch 4 complete. Time taken: load(0.0162), train(2.04), total(2.06). \n",
            "GPU Utilisation at batch 5 after data loading: 2797\n",
            "GPU Utilisation at batch 5 after training: 7251\n",
            "Batch 5 complete. Time taken: load(0.0172), train(2.03), total(2.04). \n",
            "GPU Utilisation at batch 6 after data loading: 2797\n",
            "GPU Utilisation at batch 6 after training: 7251\n",
            "Batch 6 complete. Time taken: load(0.024), train(2.02), total(2.05). \n",
            "GPU Utilisation at batch 7 after data loading: 2797\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d4ba2c5492e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2878\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m         )\n\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2150\u001b[0m             \u001b[0mto_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m             \u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m         )\n\u001b[1;32m   2154\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, band_mask, from_mask, to_mask, blocked_encoder_mask, return_dict)\u001b[0m\n\u001b[1;32m   1639\u001b[0m                     \u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m                 )\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0mto_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0mfrom_blocked_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0mto_blocked_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocked_encoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         )\n\u001b[1;32m   1496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask)\u001b[0m\n\u001b[1;32m   1395\u001b[0m             ), \"BigBird cannot be used as a decoder when config.attention_type != 'original_full'\"\n\u001b[1;32m   1396\u001b[0m             self_outputs = self.self(\n\u001b[0;32m-> 1397\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_blocked_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_blocked_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m             )\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mplan_from_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mplan_num_rand_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mbigbird_block_sparse_attention\u001b[0;34m(self, query_layer, key_layer, value_layer, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, n_heads, n_rand_blocks, attention_head_size, from_block_size, to_block_size, batch_size, from_seq_len, to_seq_len, seed, plan_from_length, plan_num_rand_blocks, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mrand_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mrand_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0mrand_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mrand_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_attn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if not os.path.exists('models'):\n",
        "  os.makedirs('models')\n",
        "  print('models directory created!')\n",
        "model.to(DEVICE)\n",
        "print(f'Model pushed to device: {DEVICE}')\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    start_epoch_message = f'EPOCH {epoch + 1} STARTED'\n",
        "    print(start_epoch_message)\n",
        "    print(f'{\"-\" * len(start_epoch_message)}')\n",
        "    start_epoch = time.time()\n",
        "\n",
        "    start_load = time.time()\n",
        "    training_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        start_train = time.time()\n",
        "        inputs = move_to_device(inputs, DEVICE)\n",
        "        targets = move_to_device(targets, DEVICE)\n",
        "        if DEVICE != 'cpu':\n",
        "            print(f'GPU Utilisation at batch {i+1} after data loading: {get_gpu_utilization()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss, outputs = model(\n",
        "            labels=targets,\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            return_dict=False\n",
        "        )\n",
        "        if DEVICE != 'cpu':\n",
        "            print(f'GPU Utilisation at batch {i+1} after training: {get_gpu_utilization()}')\n",
        "\n",
        "\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del targets, inputs, loss, outputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        end_train = time.time()\n",
        "\n",
        "        if verbose > 1:\n",
        "            print(\n",
        "                f'Batch {i + 1} complete. Time taken: load({start_train - start_load:.3g}), '\n",
        "                f'train({end_train - start_train:.3g}), total({end_train - start_load:.3g}). '\n",
        "            )\n",
        "        start_load = time.time()\n",
        "\n",
        "    print_message = f'Epoch {epoch + 1}/{epochs} complete. ' \\\n",
        "                    f'Time taken: {start_load - start_epoch:.3g}. ' \\\n",
        "                    f'Loss: {training_loss/(i+1): .3g}'\n",
        "\n",
        "    if verbose:\n",
        "        print(f'{\"-\" * len(print_message)}')\n",
        "        print(print_message)\n",
        "        print(f'{\"-\" * len(print_message)}')\n",
        "\n",
        "    if epoch % save_freq == 0:\n",
        "        encoded_model_name = encode_model_name(model_name, epoch+1)\n",
        "        save_path = f'models/{encoded_model_name}'\n",
        "        model.save_pretrained(save_path)\n",
        "        print(f'Model saved at epoch {epoch+1} at: {save_path}')\n",
        "\n",
        "encoded_model_name = encode_model_name(model_name, 'final')\n",
        "save_path = f'models/{encoded_model_name}'\n",
        "model.save_pretrained(save_path)\n",
        "print(f'Model saved at epoch {epoch + 1} at: {save_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12afdc17",
      "metadata": {
        "id": "12afdc17"
      },
      "outputs": [],
      "source": [
        "# load trained model\n",
        "path = ''\n",
        "trained_model = AutoModelForTokenClassification.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics, df_scores = inference(trained_model, test_loader)"
      ],
      "metadata": {
        "id": "VQcpzbhuPPwu"
      },
      "id": "VQcpzbhuPPwu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd86b6d3",
      "metadata": {
        "id": "bd86b6d3"
      },
      "outputs": [],
      "source": [
        "df_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KSc1Z5UFQJgU"
      },
      "id": "KSc1Z5UFQJgU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WoOfJ70gQeAk"
      },
      "id": "WoOfJ70gQeAk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "argument-mining",
      "language": "python",
      "name": "argument-mining"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "End-to-end_no_GPU.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}