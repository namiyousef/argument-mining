{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c88b4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import warnings\n",
    "from argminer.data import ArgumentMiningDataset, PersuadeProcessor, TUDarmstadtProcessor\n",
    "from argminer.utils import decode_model_name\n",
    "from argminer.evaluation import inference\n",
    "from argminer.config import LABELS_MAP_DICT\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from mlutils.torchtools.metrics import FScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce3f0c",
   "metadata": {},
   "source": [
    "## PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c74cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to have a relative load of the models\n",
    "import os\n",
    "os.chdir('/Users/yousefnami/Desktop')\n",
    "\n",
    "# path to models\n",
    "JOB_DIR = 'tmpdir/job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fec9a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO double check config MAP!\n",
    "CONFIG_MAP = {\n",
    "    '822594.undefined': dict(\n",
    "        processor=PersuadeProcessor,\n",
    "        strat='bieo'\n",
    "    ),\n",
    "    '822595.undefined': dict(\n",
    "        processor=PersuadeProcessor,\n",
    "        strat='bio'\n",
    "    ),\n",
    "    '822596.undefined': dict(\n",
    "        processor=PersuadeProcessor,\n",
    "        strat='io'\n",
    "    ),\n",
    "    '820966.undefined': dict(\n",
    "        processor=PersuadeProcessor,\n",
    "        strat='bieo'\n",
    "    ),\n",
    "    '820965.undefined': dict(\n",
    "       processor=PersuadeProcessor,\n",
    "        strat='bio' \n",
    "    ),\n",
    "    '820962.undefined': dict(\n",
    "        processor=PersuadeProcessor,\n",
    "        strat='io'\n",
    "    ),\n",
    "    '820985.undefined': dict(\n",
    "        processor=TUDarmstadtProcessor,\n",
    "        strat='bieo'\n",
    "    ),\n",
    "    '820986.undefined': dict(\n",
    "        processor=TUDarmstadtProcessor,\n",
    "        strat='bio'\n",
    "    ),\n",
    "    '820987.undefined': dict(\n",
    "        processor=TUDarmstadtProcessor,\n",
    "        strat='io'\n",
    "    )\n",
    "}\n",
    "\n",
    "MAX_LENGTH_DICT = {\n",
    "    'google/bigbird-roberta-base': 1024,\n",
    "    'roberta-base': 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9512f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448f6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics = [FScore(average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b8fec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_loader(df_label_map, df_test, batch_size):\n",
    "    testset = ArgumentMiningDataset(\n",
    "        df_label_map, df_test, tokenizer, max_length, f'standard_{strategy}', is_train=False\n",
    "    )\n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    return testloader\n",
    "\n",
    "def _get_data(path, Processor, strategy, batch_size=32, limit=None):\n",
    "    processor = Processor(path).from_json()\n",
    "    if 'test' in path:\n",
    "        df_test = processor.dataframe[['text', 'labels']]\n",
    "        # TODO here might need to do a label_map\n",
    "    else:\n",
    "        df_dict = processor.get_tts(test_size=0.3)\n",
    "        df_test = df_dict['test'][['text', 'labels']]\n",
    "    if limit is not None:\n",
    "        warnings.warn('LOADING LIMITED DATA')\n",
    "        df_test = df_test.head(limit)\n",
    "    \n",
    "    df_label_map = LABELS_MAP_DICT[processor.__class__.__name__.split('Processor')[0]][strategy]\n",
    "    return df_test, df_label_map\n",
    "\n",
    "def _get_core_data(path, Processor, strategy, batch_size=32, limit=None):\n",
    "    #processor = Processor(path).from_json()\n",
    "    #if 'test' in path:\n",
    "    #    print( processor.dataframe.head())\n",
    "    #    df_test = processor.dataframe[['text', 'labels']]\n",
    "    #else:\n",
    "    #    df_dict = processor.get_tts(test_size=0.3)\n",
    "    #    df_test = df_dict['test'][['text', 'labels']]\n",
    "    #if limit is not None:\n",
    "    #    warnings.warn('LOADING LIMITED DATA')\n",
    "    #   df_test = df_test.head(limit)\n",
    "    \n",
    "    #df_label_map = LABELS_MAP_DICT[processor.__class__.__name__.split('Processor')[0]][strategy]\n",
    "    \n",
    "    #testset = ArgumentMiningDataset(\n",
    "    #    df_label_map, df_test, tokenizer, max_length, f'standard_{strategy}', is_train=False\n",
    "    #)\n",
    "    #testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    df_test, df_label_map = _get_data(path, Processor, strategy, batch_size, limit)\n",
    "    testloader = _get_loader(df_label_map, df_test, batch_size)\n",
    "    \n",
    "    return testloader\n",
    "\n",
    "\n",
    "def _get_other_data(path, Processor, strategy, batch_size=32, limit=None):\n",
    "    if Processor == PersuadeProcessor:\n",
    "        # we are using a Persuade Model but would like to test on TUDarmstadt\n",
    "        df_test, _ = _get_data(path, TUDarmstadtProcessor, strategy, batch_size, limit)\n",
    "        # HARD RESET DF-LABEL-MAP\n",
    "        df_label_map = LABELS_MAP_DICT['Persuade'][strategy]\n",
    "        \n",
    "        label_map_dict = {\n",
    "            'Claim': 'Claim',\n",
    "            'MajorClaim': 'Position',\n",
    "            'Premise': 'O',\n",
    "            'O': 'O'\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        df_test, _ = _get_data(path, PersuadeProcessor, strategy, batch_size, limit)\n",
    "        df_label_map = LABELS_MAP_DICT['TUDarmstadt'][strategy]\n",
    "\n",
    "\n",
    "        label_map_dict = {\n",
    "            'Lead': 'O',\n",
    "            'Rebuttal': 'O',\n",
    "            'Concluding Statement': 'O',\n",
    "            'Position': 'MajorClaim',\n",
    "            'Evidence': 'O',\n",
    "            'Claim': 'Claim',\n",
    "            'Counterclaim': 'O',\n",
    "            'O': 'O'\n",
    "        }\n",
    "    df_test.labels = df_test.labels.apply(\n",
    "        lambda x: [\n",
    "            'O' if label_map_dict[text.split('-')[-1]] =='O' else text.replace(\n",
    "                text.split('-')[-1],\n",
    "                label_map_dict[text.split('-')[-1]]\n",
    "            ) for text in x]\n",
    "    )\n",
    "\n",
    "\n",
    "    testloader = _get_loader(df_label_map, df_test, batch_size)\n",
    "    return testloader\n",
    "\n",
    "def _get_scores_agg(df):\n",
    "    df = df.groupby('class').sum()\n",
    "    df['f1'] = df.tp / (df.tp + 1/2*(df.fp + df.fn))\n",
    "    macro_f1 = df['f1'].mean()\n",
    "    return macro_f1, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14354111",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LIMIT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b331ab21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR MODEL 0: roberta-base at PATH: tmpdir/job/822594.undefined/models/cm9iZXJ0YS1iYXNlX2ZpbmFs\n",
      "PARAMETERS: bieo. <class 'argminer.data.PersuadeProcessor'>\n",
      "=================================================================================\n",
      "Loaded and data loaded. Time:  8.91\n",
      "Prediction time: 468\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 2.21\n",
      "Get predstring time: 0.226\n",
      "Evaluate time: 1.77\n",
      "Batch 1 complete.\n",
      "Prediction time: 555\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 1.51\n",
      "Get predstring time: 0.163\n",
      "Evaluate time: 0.411\n",
      "Batch 2 complete.\n",
      "Prediction time: 420\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 1.66\n",
      "Get predstring time: 0.153\n",
      "Evaluate time: 0.475\n",
      "Batch 3 complete.\n",
      "Prediction time: 433\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 1.31\n",
      "Get predstring time: 0.12\n",
      "Evaluate time: 0.406\n",
      "Batch 4 complete.\n",
      "Prediction time: 315\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 6.5\n",
      "Get predstring time: 0.069\n",
      "Evaluate time: 0.349\n",
      "Batch 5 complete.\n",
      "Prediction time: 406\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 1.52\n",
      "Get predstring time: 0.211\n",
      "Evaluate time: 0.519\n",
      "Batch 6 complete.\n",
      "Prediction time: 606\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 2.9\n",
      "Get predstring time: 0.431\n",
      "Evaluate time: 0.848\n",
      "Batch 7 complete.\n",
      "Prediction time: 574\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 1.69\n",
      "Get predstring time: 0.231\n",
      "Evaluate time: 0.527\n",
      "Batch 8 complete.\n",
      "Prediction time: 585\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 2.05\n",
      "Get predstring time: 0.192\n",
      "Evaluate time: 0.66\n",
      "Batch 9 complete.\n",
      "Prediction time: 538\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
      "Agg to word time: 2.85\n",
      "Get predstring time: 0.157\n",
      "Evaluate time: 0.739\n",
      "Batch 10 complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m SELF \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     51\u001b[0m s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 52\u001b[0m df_metrics, df_scores \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m macro_f1, df_scores_agg \u001b[38;5;241m=\u001b[39m _get_scores_agg(df_scores)\n\u001b[1;32m     54\u001b[0m RESULTS[job][SELF][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: macro_f1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m: df_scores_agg}\n",
      "File \u001b[0;32m~/Desktop/Main/0.Education/2.UCL/Courses/NLP/argument-mining/argminer/evaluation.py:253\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, testloader, metrics)\u001b[0m\n\u001b[1;32m    250\u001b[0m targets \u001b[38;5;241m=\u001b[39m move_to_device(targets, DEVICE)\n\u001b[1;32m    252\u001b[0m s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# TODO add verbose statement\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m s\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    261\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1397\u001b[0m, in \u001b[0;36mRobertaForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1411\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:850\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    841\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    843\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    844\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    845\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    848\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    849\u001b[0m )\n\u001b[0;32m--> 850\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    863\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    518\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    519\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    450\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    451\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 453\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/modeling_utils.py:2438\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 465\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:364\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 364\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/argument-mining/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for MODEL_ID, job in enumerate(os.listdir(JOB_DIR)):\n",
    "    if job != '.DS_Store':\n",
    "        job_path = os.path.join(JOB_DIR, job)\n",
    "        model_dir = os.path.join(job_path, 'models')\n",
    "        model_name = [item for item in os.listdir(model_dir) if item != '.DS_Store'][0]\n",
    "        model_name_decoded = decode_model_name(model_name).split('_')[0] # get base model name\n",
    "        \n",
    "        max_length = MAX_LENGTH_DICT[model_name_decoded]\n",
    "        # define tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_decoded, add_prefix_space=True)\n",
    "        model_path = os.path.join(model_dir, model_name)\n",
    "        \n",
    "        strategy = CONFIG_MAP[job].get('strat')\n",
    "        Processor = CONFIG_MAP[job].get('processor')\n",
    "        print(\n",
    "            f'RUNNING FOR MODEL {MODEL_ID}: {model_name_decoded} at PATH: {model_path}\\n'\n",
    "            f'PARAMETERS: {strategy}. {Processor}'\n",
    "        )\n",
    "        print('=================================================================================')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        s = time.time()\n",
    "        trained_model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "        \n",
    "        RESULTS[job] = {}\n",
    "        \n",
    "        # test the model against itself\n",
    "        RESULTS[job]['self'] = {}\n",
    "        \n",
    "        # specify the path to the json\n",
    "        path = f'Main/0.Education/2.UCL/Courses/NLP/argument-mining/data/core/{strategy}'\n",
    "\n",
    "        #df_test, df_label_map = _get_core_data(path, Processor, strategy)\n",
    "        #processor = Processor(path).from_json()\n",
    "        #df_dict = processor.get_tts(test_size=0.3)\n",
    "        #df_test = df_dict['test'][['text', 'labels']]\n",
    "        #df_label_map = LABELS_MAP_DICT[processor.__class__.__name__.split('Processor')[0]][strategy]\n",
    "        \n",
    "        #testset = ArgumentMiningDataset(\n",
    "        #    df_label_map, df_test, tokenizer, max_length, f'standard_{strategy}', is_train=False\n",
    "        #)\n",
    "        #testloader = DataLoader(testset, batch_size=32)\n",
    "        testloader = _get_core_data(path, Processor, strategy, batch_size=BATCH_SIZE, limit=LIMIT)\n",
    "        print(f'Loaded and data loaded. Time: {time.time() - s: .3g}')\n",
    "\n",
    "\n",
    "        # TODO add metrics support\n",
    "        SELF = 'self'\n",
    "        s = time.time()\n",
    "        df_metrics, df_scores = inference(trained_model, testloader, )\n",
    "        macro_f1, df_scores_agg = _get_scores_agg(df_scores)\n",
    "        RESULTS[job][SELF]['core'] = {'macro_f1': macro_f1, 'scores': df_scores_agg}\n",
    "        \n",
    "        print(f'Took {time.time() -s:.3g} to get scores')\n",
    "        # aggregate the scores        \n",
    "        # TODO save these scores!!!!!\n",
    "        \n",
    "        \n",
    "        \n",
    "        # test models against self adversarial examples\n",
    "\n",
    "\n",
    "        augmented_path = f'test/{strategy}'\n",
    "        for augmentation in os.listdir(augmented_path):\n",
    "            if augmentation != '.DS_Store':\n",
    "                augmentation_path = os.path.join(augmented_path, augmentation)\n",
    "                testloader = _get_core_data(augmentation_path, Processor, strategy,\n",
    "                                                       batch_size=BATCH_SIZE, limit=LIMIT)\n",
    "                #processor = Processor(augmentation).from_json()\n",
    "                #df_test = processor.dataframe[['text', 'labels']]\n",
    "                #testset = ArgumentMiningDataset(\n",
    "                #    df_label_map, df_test, tokenizer, max_length, f'standard_{strategy}', is_train=False\n",
    "                #)\n",
    "                #testloader = DataLoader(testset, batch_size=32)\n",
    "                df_metrics, df_scores = inference(trained_model, testloader, )\n",
    "                macro_f1, df_scores_agg = _get_scores_agg(df_scores)\n",
    "                RESULTS[job][SELF][augmentation] = {'macro_f1': macro_f1, 'scores': df_scores_agg}\n",
    "                # aggregate the scores\n",
    "        \n",
    "        \n",
    "        OTHER = 'other'\n",
    "        RESULTS[job][OTHER] = {}\n",
    "        \n",
    "        # get other processor\n",
    "        testloader = _get_other_data(path, Processor, strategy, batch_size=BATCH_SIZE, limit=LIMIT)\n",
    "        df_metrics, df_scores = inference(trained_model, testloader)\n",
    "        macro_f1, df_scores_agg = _get_scores_agg(df_scores)\n",
    "        RESULTS[job][OTHER]['core'] = {'macro_f1': macro_f1, 'scores': df_scores_agg}\n",
    "        \n",
    "        \n",
    "        for augmentation in os.listdir(augmented_path):\n",
    "            if augmentation != '.DS_Store':\n",
    "                augmentation_path = os.path.join(augmented_path, augmentation)\n",
    "                testloader = _get_other_data(augmentation_path, Processor, strategy,\n",
    "                                             batch_size=BATCH_SIZE, limit=LIMIT)\n",
    "                df_metrics, df_scores = inference(trained_model, testloader)\n",
    "                macro_f1, df_scores_agg = _get_scores_agg(df_scores)\n",
    "                RESULTS[job][OTHER][augmentation] = {'macro_f1': macro_f1, 'scores': df_scores_agg}\n",
    "        \n",
    "        #RESULTS[model_name_decoded]['self']['core'] # the score\n",
    "        \n",
    "        \n",
    "        # test model against other datasets, need to convert datasets\n",
    "        #RESULTS[model_name_decoded]['transfer'] = {}\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2142cf8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'822594.undefined': {'self': {'core': {'macro_f1': 0.46401515151515155,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    0      0.0  2.0  4.0  0.000000\n",
       "    1      1.0  1.0  1.0  0.500000\n",
       "    2      1.0  1.0  1.0  0.500000\n",
       "    3      3.0  4.0  6.0  0.375000\n",
       "    6      5.0  0.0  1.0  0.909091\n",
       "    7      1.0  1.0  1.0  0.500000},\n",
       "   'custom_fillers': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   2  0.0\n",
       "    3      0.0   0   2  0.0\n",
       "    5      0.0   1   1  0.0\n",
       "    6      0.0   1   2  0.0\n",
       "    7      0.0   0   1  0.0},\n",
       "   'synonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   1  0.0\n",
       "    2      0.0   1   2  0.0\n",
       "    3      0.0   0   4  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   4  0.0},\n",
       "   'keywordChange': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   4  0.0\n",
       "    3      0.0   0   5  0.0\n",
       "    5      0.0   1   1  0.0\n",
       "    6      0.0   1   3  0.0},\n",
       "   'spellingError': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   3  0.0\n",
       "    3      0.0   0   2  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   6  0.0},\n",
       "   'antonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   3  0.0\n",
       "    2      0.0   1   3  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    4      0.0   0   1  0.0\n",
       "    5      0.0   1   2  0.0\n",
       "    6      0.0   1   9  0.0}},\n",
       "  'other': {'core': {'macro_f1': 0.09294871794871795,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  8.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  3.0  0.250000\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  3.0  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.08465608465608465,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  7.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      2.0   5  9.0  0.222222\n",
       "    6      0.0   0  6.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'synonym': {'macro_f1': 0.16666666666666666,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  6.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  0.0  0.666667\n",
       "    3      2.0   5  3.0  0.333333\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.09294871794871795,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  4.0  0.000000\n",
       "    2      1.0   3  3.0  0.250000\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.1414141414141414,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  6.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  0.0  0.666667\n",
       "    3      1.0   6  3.0  0.181818\n",
       "    6      0.0   0  3.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'antonym': {'macro_f1': 0.15584415584415584,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      2.0   2  1.0  0.571429\n",
       "    3      2.0   5  2.0  0.363636\n",
       "    6      0.0   0  6.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000}}},\n",
       " '820966.undefined': {'self': {'core': {'macro_f1': 0.32765151515151514,\n",
       "    'scores':         tp   fn    fp        f1\n",
       "    class                          \n",
       "    0      1.0  1.0   3.0  0.333333\n",
       "    1      1.0  1.0   1.0  0.500000\n",
       "    2      1.0  1.0   1.0  0.500000\n",
       "    3      5.0  2.0  10.0  0.454545\n",
       "    4      0.0  0.0   2.0  0.000000\n",
       "    5      0.0  0.0   3.0  0.000000\n",
       "    6      4.0  1.0   7.0  0.500000\n",
       "    7      1.0  1.0   3.0  0.333333},\n",
       "   'custom_fillers': {'macro_f1': 0.05555555555555555,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      0.0  0.0   1  0.000000\n",
       "    1      0.0  0.0   3  0.000000\n",
       "    2      1.0  0.0   4  0.333333\n",
       "    3      0.0  0.0   5  0.000000\n",
       "    5      0.0  1.0   0  0.000000\n",
       "    6      0.0  1.0   6  0.000000},\n",
       "   'synonym': {'macro_f1': 0.1,\n",
       "    'scores':         tp   fn    fp   f1\n",
       "    class                     \n",
       "    1      0.0  0.0   1.0  0.0\n",
       "    2      1.0  0.0   2.0  0.5\n",
       "    3      0.0  0.0  10.0  0.0\n",
       "    5      0.0  1.0   0.0  0.0\n",
       "    6      0.0  1.0  11.0  0.0},\n",
       "   'keywordChange': {'macro_f1': 0.2,\n",
       "    'scores':         tp   fn   fp   f1\n",
       "    class                    \n",
       "    1      0.0  0.0  2.0  0.0\n",
       "    2      1.0  0.0  0.0  1.0\n",
       "    3      0.0  0.0  8.0  0.0\n",
       "    5      0.0  1.0  0.0  0.0\n",
       "    6      0.0  1.0  9.0  0.0},\n",
       "   'spellingError': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   1  0.0\n",
       "    2      0.0   1   1  0.0\n",
       "    3      0.0   0   5  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   7  0.0},\n",
       "   'antonym': {'macro_f1': 0.13333333333333333,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    1      0.0  0.0  5.0  0.000000\n",
       "    2      1.0  0.0  1.0  0.666667\n",
       "    3      0.0  0.0  2.0  0.000000\n",
       "    5      0.0  1.0  0.0  0.000000\n",
       "    6      0.0  1.0  9.0  0.000000}},\n",
       "  'other': {'core': {'macro_f1': 0.1391941391941392,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  7.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  0.0  0.666667\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    4      0.0   0  1.0  0.000000\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.11538461538461539,\n",
       "    'scores':         tp  fn    fp        f1\n",
       "    class                         \n",
       "    0      0.0  13  12.0  0.000000\n",
       "    1      0.0   0   2.0  0.000000\n",
       "    2      2.0   2   2.0  0.500000\n",
       "    3      2.0   5   4.0  0.307692\n",
       "    5      0.0   0   2.0  0.000000\n",
       "    6      0.0   0   7.0  0.000000\n",
       "    7      0.0   0   2.0  0.000000},\n",
       "   'synonym': {'macro_f1': 0.14718614718614717,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  5.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      2.0   2  0.0  0.666667\n",
       "    3      2.0   5  2.0  0.363636\n",
       "    4      0.0   0  1.0  0.000000\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.11538461538461539,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  8.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  2.0  0.500000\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    4      0.0   0  3.0  0.000000\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.08585858585858586,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  7.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  1.0  0.333333\n",
       "    3      1.0   6  3.0  0.181818\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'antonym': {'macro_f1': 0.12925170068027209,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  1.0  0.571429\n",
       "    3      2.0   5  3.0  0.333333\n",
       "    4      0.0   0  2.0  0.000000\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000}}},\n",
       " '820985.undefined': {'self': {'core': {'macro_f1': 0.8198587127158555,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      23  2.0  1.0  0.938776\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       5  2.0  2.0  0.714286\n",
       "    3      10  2.0  4.0  0.769231},\n",
       "   'custom_fillers': {'macro_f1': 0.5019230769230769,\n",
       "    'scores':          tp  fn  fp        f1\n",
       "    class                        \n",
       "    0      15.0  10  12  0.576923\n",
       "    1       1.0   3   5  0.200000\n",
       "    2       3.0   4   3  0.461538\n",
       "    3      10.0   2   4  0.769231},\n",
       "   'synonym': {'macro_f1': 0.6692857142857143,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      19  6.0  6.0  0.760000\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       4  3.0  5.0  0.500000\n",
       "    3       7  5.0  6.0  0.560000},\n",
       "   'keywordChange': {'macro_f1': 0.8198587127158555,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      23  2.0  1.0  0.938776\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       5  2.0  2.0  0.714286\n",
       "    3      10  2.0  4.0  0.769231},\n",
       "   'spellingError': {'macro_f1': 0.6861878881987578,\n",
       "    'scores':        tp    fn    fp        f1\n",
       "    class                          \n",
       "    0      12  13.0  13.0  0.480000\n",
       "    1       3   1.0   0.0  0.857143\n",
       "    2       5   2.0   4.0  0.625000\n",
       "    3       9   3.0   2.0  0.782609},\n",
       "   'antonym': {'macro_f1': 0.7298188405797101,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      18  7.0  7.0  0.720000\n",
       "    1       3  1.0  1.0  0.750000\n",
       "    2       5  2.0  3.0  0.666667\n",
       "    3       9  3.0  2.0  0.782609}},\n",
       "  'other': {'core': {'macro_f1': 0.05263157894736842,\n",
       "    'scores':         tp  fn  fp        f1\n",
       "    class                       \n",
       "    0      2.0   7   8  0.210526\n",
       "    1      0.0   2   4  0.000000\n",
       "    2      0.0   7   0  0.000000\n",
       "    3      0.0   0   4  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.125,\n",
       "    'scores':         tp   fn  fp   f1\n",
       "    class                   \n",
       "    0      2.0  0.0   4  0.5\n",
       "    1      0.0  1.0   1  0.0\n",
       "    2      0.0  0.0   1  0.0\n",
       "    3      0.0  0.0   2  0.0},\n",
       "   'synonym': {'macro_f1': 0.09523809523809523,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      1.0  1.0   4  0.285714\n",
       "    1      0.0  1.0   0  0.000000\n",
       "    3      0.0  0.0   3  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.0625,\n",
       "    'scores':         tp   fn  fp    f1\n",
       "    class                    \n",
       "    0      1.0  1.0   5  0.25\n",
       "    1      0.0  1.0   0  0.00\n",
       "    2      0.0  0.0   1  0.00\n",
       "    3      0.0  0.0   3  0.00},\n",
       "   'spellingError': {'macro_f1': 0.05,\n",
       "    'scores':         tp   fn  fp   f1\n",
       "    class                   \n",
       "    0      1.0  1.0   7  0.2\n",
       "    1      0.0  1.0   0  0.0\n",
       "    2      0.0  0.0   4  0.0\n",
       "    3      0.0  0.0   2  0.0},\n",
       "   'antonym': {'macro_f1': 0.07142857142857142,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      1.0  1.0   4  0.285714\n",
       "    1      0.0  1.0   0  0.000000\n",
       "    2      0.0  0.0   3  0.000000\n",
       "    3      0.0  0.0   2  0.000000}}},\n",
       " '822595.undefined': {'self': {'core': {'macro_f1': 0.38316624895572265,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    0      1.0  1.0  5.0  0.250000\n",
       "    1      1.0  1.0  1.0  0.500000\n",
       "    2      1.0  1.0  0.0  0.666667\n",
       "    3      4.0  3.0  8.0  0.421053\n",
       "    4      0.0  0.0  4.0  0.000000\n",
       "    6      4.0  1.0  9.0  0.444444\n",
       "    7      1.0  1.0  2.0  0.400000},\n",
       "   'custom_fillers': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   0   3  0.0\n",
       "    1      0.0   0   7  0.0\n",
       "    2      0.0   1   7  0.0\n",
       "    3      0.0   0   4  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   7  0.0},\n",
       "   'synonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   2  0.0\n",
       "    3      0.0   0   1  0.0\n",
       "    5      0.0   1   1  0.0\n",
       "    6      0.0   1   5  0.0},\n",
       "   'keywordChange': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   6  0.0\n",
       "    2      0.0   1   3  0.0\n",
       "    3      0.0   0   1  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   6  0.0},\n",
       "   'spellingError': {'macro_f1': 0.1,\n",
       "    'scores':         tp   fn  fp   f1\n",
       "    class                   \n",
       "    1      0.0  0.0   1  0.0\n",
       "    2      0.0  1.0   2  0.0\n",
       "    3      0.0  0.0   3  0.0\n",
       "    5      0.0  1.0   0  0.0\n",
       "    6      1.0  0.0   2  0.5},\n",
       "   'antonym': {'macro_f1': 0.26666666666666666,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    1      0.0  0.0  2.0  0.000000\n",
       "    2      1.0  0.0  1.0  0.666667\n",
       "    3      0.0  0.0  1.0  0.000000\n",
       "    5      0.0  1.0  0.0  0.000000\n",
       "    6      1.0  0.0  1.0  0.666667}},\n",
       "  'other': {'core': {'macro_f1': 0.10822510822510822,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      2.0   5  2.0  0.363636\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.08971153846153847,\n",
       "    'scores':         tp  fn  fp        f1\n",
       "    class                       \n",
       "    0      2.0  11  10  0.160000\n",
       "    1      0.0   0   2  0.000000\n",
       "    2      1.0   3   3  0.250000\n",
       "    3      2.0   5   4  0.307692\n",
       "    4      0.0   0   1  0.000000\n",
       "    5      0.0   0   1  0.000000\n",
       "    6      0.0   0   5  0.000000\n",
       "    7      0.0   0   2  0.000000},\n",
       "   'synonym': {'macro_f1': 0.08095238095238096,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  5.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      1.0   6  2.0  0.200000\n",
       "    6      0.0   0  3.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.10822510822510822,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      2.0   5  2.0  0.363636\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.11599511599511599,\n",
       "    'scores':         tp  fn    fp        f1\n",
       "    class                         \n",
       "    0      1.0  12  12.0  0.076923\n",
       "    1      0.0   0   2.0  0.000000\n",
       "    2      1.0   3   2.0  0.285714\n",
       "    3      2.0   5   3.0  0.333333\n",
       "    6      0.0   0   4.0  0.000000\n",
       "    7      0.0   0   2.0  0.000000},\n",
       "   'antonym': {'macro_f1': 0.1717171717171717,\n",
       "    'scores':         tp  fn    fp        f1\n",
       "    class                         \n",
       "    0      0.0  13  10.0  0.000000\n",
       "    1      0.0   0   2.0  0.000000\n",
       "    2      2.0   2   0.0  0.666667\n",
       "    3      2.0   5   2.0  0.363636\n",
       "    6      0.0   0   4.0  0.000000\n",
       "    7      0.0   0   2.0  0.000000}}},\n",
       " '822596.undefined': {'self': {'core': {'macro_f1': 0.4952380952380952,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    0      0.0  2.0  8.0  0.000000\n",
       "    1      2.0  0.0  0.0  1.000000\n",
       "    2      2.0  0.0  0.0  1.000000\n",
       "    3      4.0  3.0  4.0  0.533333\n",
       "    4      0.0  0.0  1.0  0.000000\n",
       "    6      4.0  1.0  6.0  0.533333\n",
       "    7      1.0  1.0  2.0  0.400000},\n",
       "   'custom_fillers': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   0   1  0.0\n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   2  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   3  0.0},\n",
       "   'synonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   1  0.0\n",
       "    3      0.0   0   2  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   3  0.0},\n",
       "   'keywordChange': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   5  0.0\n",
       "    2      0.0   1   3  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   3  0.0},\n",
       "   'spellingError': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   1  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   3  0.0},\n",
       "   'antonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   1  0.0\n",
       "    2      0.0   1   2  0.0\n",
       "    3      0.0   0   8  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   7  0.0}},\n",
       "  'other': {'core': {'macro_f1': 0.1111111111111111,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  5.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  1.0  0.333333\n",
       "    3      2.0   5  3.0  0.333333\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.12198912198912198,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      1.0  12  8.0  0.090909\n",
       "    1      0.0   0  1.0  0.000000\n",
       "    2      1.0   3  1.0  0.333333\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'synonym': {'macro_f1': 0.14285714285714285,\n",
       "    'scores':         tp  fn    fp        f1\n",
       "    class                         \n",
       "    0      0.0  13  10.0  0.000000\n",
       "    1      0.0   0   2.0  0.000000\n",
       "    2      2.0   2   0.0  0.666667\n",
       "    3      2.0   5   3.0  0.333333\n",
       "    4      0.0   0   1.0  0.000000\n",
       "    6      0.0   0   6.0  0.000000\n",
       "    7      0.0   0   2.0  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.15079365079365079,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  5.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      2.0   2  1.0  0.571429\n",
       "    3      2.0   5  3.0  0.333333\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.11352126134734832,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      1.0  12  9.0  0.086957\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  4.0  0.400000\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    4      0.0   0  1.0  0.000000\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  4.0  0.000000},\n",
       "   'antonym': {'macro_f1': 0.16666666666666666,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  6.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  0.0  0.666667\n",
       "    3      2.0   5  3.0  0.333333\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000}}},\n",
       " '820987.undefined': {'self': {'core': {'macro_f1': 0.7949423247559894,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      16  9.0  8.0  0.653061\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       6  1.0  2.0  0.800000\n",
       "    3      10  2.0  1.0  0.869565},\n",
       "   'custom_fillers': {'macro_f1': 0.5666214177978883,\n",
       "    'scores':        tp    fn    fp        f1\n",
       "    class                          \n",
       "    0       9  16.0  16.0  0.360000\n",
       "    1       3   1.0   2.0  0.666667\n",
       "    2       4   3.0   6.0  0.470588\n",
       "    3      10   2.0   4.0  0.769231},\n",
       "   'synonym': {'macro_f1': 0.6559658283603297,\n",
       "    'scores':        tp    fn    fp        f1\n",
       "    class                          \n",
       "    0      15  10.0  11.0  0.588235\n",
       "    1       4   0.0   1.0  0.888889\n",
       "    2       5   2.0   4.0  0.625000\n",
       "    3       6   6.0   5.0  0.521739},\n",
       "   'keywordChange': {'macro_f1': 0.7949423247559894,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      16  9.0  8.0  0.653061\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       6  1.0  2.0  0.800000\n",
       "    3      10  2.0  1.0  0.869565},\n",
       "   'spellingError': {'macro_f1': 0.636356100795756,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      19  6.0  8.0  0.730769\n",
       "    1       3  1.0  1.0  0.750000\n",
       "    2       3  4.0  6.0  0.375000\n",
       "    3      10  2.0  7.0  0.689655},\n",
       "   'antonym': {'macro_f1': 0.6669501133786848,\n",
       "    'scores':        tp    fn   fp        f1\n",
       "    class                         \n",
       "    0      15  10.0  9.0  0.612245\n",
       "    1       3   1.0  1.0  0.750000\n",
       "    2       5   2.0  6.0  0.555556\n",
       "    3       9   3.0  3.0  0.750000}},\n",
       "  'other': {'core': {'macro_f1': 0.03225806451612903,\n",
       "    'scores':         tp  fn  fp        f1\n",
       "    class                       \n",
       "    0      2.0   7  20  0.129032\n",
       "    1      0.0   2   1  0.000000\n",
       "    2      0.0   7   3  0.000000\n",
       "    3      0.0   0  17  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.05,\n",
       "    'scores':         tp   fn  fp   f1\n",
       "    class                   \n",
       "    0      1.0  1.0   7  0.2\n",
       "    1      0.0  1.0   1  0.0\n",
       "    2      0.0  0.0   2  0.0\n",
       "    3      0.0  0.0   4  0.0},\n",
       "   'synonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   2  13  0.0\n",
       "    1      0.0   1   5  0.0\n",
       "    2      0.0   0   4  0.0\n",
       "    3      0.0   0   2  0.0},\n",
       "   'keywordChange': {'macro_f1': 0.041666666666666664,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      1.0  1.0   9  0.166667\n",
       "    1      0.0  1.0   0  0.000000\n",
       "    2      0.0  0.0   5  0.000000\n",
       "    3      0.0  0.0   4  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.0625,\n",
       "    'scores':         tp   fn  fp    f1\n",
       "    class                    \n",
       "    0      1.0  1.0   5  0.25\n",
       "    1      0.0  1.0   0  0.00\n",
       "    2      0.0  0.0   3  0.00\n",
       "    3      0.0  0.0   2  0.00},\n",
       "   'antonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   2   7  0.0\n",
       "    1      0.0   1   1  0.0\n",
       "    2      0.0   0   3  0.0\n",
       "    3      0.0   0   1  0.0}}},\n",
       " '820962.undefined': {'self': {'core': {'macro_f1': 0.33511904761904765,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    0      0.0  2.0  4.0  0.000000\n",
       "    1      1.0  1.0  2.0  0.400000\n",
       "    2      1.0  1.0  1.0  0.500000\n",
       "    3      5.0  2.0  2.0  0.714286\n",
       "    4      0.0  0.0  1.0  0.000000\n",
       "    5      0.0  0.0  1.0  0.000000\n",
       "    6      4.0  1.0  3.0  0.666667\n",
       "    7      1.0  1.0  2.0  0.400000},\n",
       "   'custom_fillers': {'macro_f1': 0.16666666666666666,\n",
       "    'scores':         tp   fn   fp   f1\n",
       "    class                    \n",
       "    0      0.0  0.0  2.0  0.0\n",
       "    1      0.0  0.0  2.0  0.0\n",
       "    2      1.0  0.0  0.0  1.0\n",
       "    3      0.0  0.0  4.0  0.0\n",
       "    5      0.0  1.0  0.0  0.0\n",
       "    6      0.0  1.0  3.0  0.0},\n",
       "   'synonym': {'macro_f1': 0.12380952380952381,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    1      0.0  0.0   4  0.000000\n",
       "    2      1.0  0.0   5  0.285714\n",
       "    3      0.0  0.0   5  0.000000\n",
       "    5      0.0  1.0   0  0.000000\n",
       "    6      1.0  0.0   4  0.333333},\n",
       "   'keywordChange': {'macro_f1': 0.13333333333333333,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    1      0.0  0.0  2.0  0.000000\n",
       "    2      1.0  0.0  1.0  0.666667\n",
       "    3      0.0  0.0  4.0  0.000000\n",
       "    5      0.0  1.0  0.0  0.000000\n",
       "    6      0.0  1.0  5.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.21333333333333332,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    1      0.0  0.0  1.0  0.000000\n",
       "    2      1.0  0.0  1.0  0.666667\n",
       "    3      0.0  0.0  2.0  0.000000\n",
       "    5      0.0  1.0  0.0  0.000000\n",
       "    6      1.0  0.0  3.0  0.400000},\n",
       "   'antonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   2  0.0\n",
       "    2      0.0   1   3  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   6  0.0\n",
       "    7      0.0   0   2  0.0}},\n",
       "  'other': {'core': {'macro_f1': 0.0989010989010989,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.05833333333333333,\n",
       "    'scores':         tp  fn  fp        f1\n",
       "    class                       \n",
       "    0      1.0  12  10  0.083333\n",
       "    1      0.0   0   2  0.000000\n",
       "    2      0.0   4   3  0.000000\n",
       "    3      2.0   5   6  0.266667\n",
       "    6      0.0   0   6  0.000000\n",
       "    7      0.0   0   2  0.000000},\n",
       "   'synonym': {'macro_f1': 0.0861111111111111,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  5.0  0.000000\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      1.0   3  3.0  0.250000\n",
       "    3      2.0   5  6.0  0.266667\n",
       "    6      0.0   0  5.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.0989010989010989,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      2.0   5  4.0  0.307692\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.13766233766233765,\n",
       "    'scores':         tp    fn    fp        f1\n",
       "    class                           \n",
       "    0      0.0  13.0  10.0  0.000000\n",
       "    1      0.0   0.0   2.0  0.000000\n",
       "    2      3.0   1.0   3.0  0.600000\n",
       "    3      2.0   5.0   2.0  0.363636\n",
       "    4      0.0   0.0   3.0  0.000000\n",
       "    6      0.0   0.0   4.0  0.000000\n",
       "    7      0.0   0.0   2.0  0.000000},\n",
       "   'antonym': {'macro_f1': 0.14393939393939395,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  9.0  0.000000\n",
       "    1      0.0   0  4.0  0.000000\n",
       "    2      2.0   2  2.0  0.500000\n",
       "    3      2.0   5  2.0  0.363636\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000}}},\n",
       " '820965.undefined': {'self': {'core': {'macro_f1': 0.37576312576312576,\n",
       "    'scores':         tp   fn   fp        f1\n",
       "    class                         \n",
       "    0      0.0  2.0  3.0  0.000000\n",
       "    1      1.0  1.0  5.0  0.250000\n",
       "    2      1.0  1.0  0.0  0.666667\n",
       "    3      4.0  3.0  7.0  0.444444\n",
       "    4      0.0  0.0  2.0  0.000000\n",
       "    6      5.0  0.0  3.0  0.769231\n",
       "    7      1.0  1.0  1.0  0.500000},\n",
       "   'custom_fillers': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   0   7  0.0\n",
       "    1      0.0   0   1  0.0\n",
       "    2      0.0   1   5  0.0\n",
       "    3      0.0   0   9  0.0\n",
       "    5      0.0   1   2  0.0\n",
       "    6      0.0   1   9  0.0\n",
       "    7      0.0   0   2  0.0},\n",
       "   'synonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   0   1  0.0\n",
       "    1      0.0   0   4  0.0\n",
       "    2      0.0   1   3  0.0\n",
       "    3      0.0   0   6  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   9  0.0},\n",
       "   'keywordChange': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    1      0.0   0   3  0.0\n",
       "    2      0.0   1   2  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    5      0.0   1   0  0.0\n",
       "    6      0.0   1   6  0.0\n",
       "    7      0.0   0   1  0.0},\n",
       "   'spellingError': {'macro_f1': 0.05555555555555555,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    1      0.0  0.0   3  0.000000\n",
       "    2      0.0  1.0   7  0.000000\n",
       "    3      0.0  0.0   3  0.000000\n",
       "    5      0.0  1.0   0  0.000000\n",
       "    6      1.0  0.0   4  0.333333\n",
       "    7      0.0  0.0   4  0.000000},\n",
       "   'antonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   0   1  0.0\n",
       "    1      0.0   0   9  0.0\n",
       "    2      0.0   1   4  0.0\n",
       "    3      0.0   0   3  0.0\n",
       "    5      0.0   1   1  0.0\n",
       "    6      0.0   1   3  0.0\n",
       "    7      0.0   0   2  0.0}},\n",
       "  'other': {'core': {'macro_f1': 0.10317460317460318,\n",
       "    'scores':         tp  fn    fp        f1\n",
       "    class                         \n",
       "    0      0.0  13  10.0  0.000000\n",
       "    1      0.0   0   2.0  0.000000\n",
       "    2      1.0   3   1.0  0.333333\n",
       "    3      2.0   5   5.0  0.285714\n",
       "    6      0.0   0   6.0  0.000000\n",
       "    7      0.0   0   2.0  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.06043956043956044,\n",
       "    'scores':         tp  fn  fp        f1\n",
       "    class                       \n",
       "    0      1.0  12  12  0.076923\n",
       "    1      0.0   0   1  0.000000\n",
       "    2      0.0   4   2  0.000000\n",
       "    3      2.0   5   5  0.285714\n",
       "    6      0.0   0   7  0.000000\n",
       "    7      0.0   0   2  0.000000},\n",
       "   'synonym': {'macro_f1': 0.07539682539682539,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      0.0  13  7.0  0.000000\n",
       "    1      0.0   0  3.0  0.000000\n",
       "    2      1.0   3  2.0  0.285714\n",
       "    3      1.0   6  4.0  0.166667\n",
       "    6      0.0   0  6.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.09999999999999999,\n",
       "    'scores':         tp  fn    fp        f1\n",
       "    class                         \n",
       "    0      0.0  13  11.0  0.000000\n",
       "    1      0.0   0   2.0  0.000000\n",
       "    2      1.0   3   1.0  0.333333\n",
       "    3      2.0   5   6.0  0.266667\n",
       "    6      0.0   0   6.0  0.000000\n",
       "    7      0.0   0   2.0  0.000000},\n",
       "   'spellingError': {'macro_f1': 0.18686868686868685,\n",
       "    'scores':         tp  fn   fp        f1\n",
       "    class                        \n",
       "    0      1.0  12  8.0  0.090909\n",
       "    1      0.0   0  2.0  0.000000\n",
       "    2      2.0   2  0.0  0.666667\n",
       "    3      2.0   5  2.0  0.363636\n",
       "    6      0.0   0  4.0  0.000000\n",
       "    7      0.0   0  2.0  0.000000},\n",
       "   'antonym': {'macro_f1': 0.08333333333333333,\n",
       "    'scores':         tp  fn    fp    f1\n",
       "    class                     \n",
       "    0      0.0  13  14.0  0.00\n",
       "    1      0.0   0   2.0  0.00\n",
       "    2      1.0   3   3.0  0.25\n",
       "    3      2.0   5   7.0  0.25\n",
       "    6      0.0   0   5.0  0.00\n",
       "    7      0.0   0   2.0  0.00}}},\n",
       " '820986.undefined': {'self': {'core': {'macro_f1': 0.8851208961845608,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      23  2.0  1.0  0.938776\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       7  0.0  2.0  0.875000\n",
       "    3      10  2.0  1.0  0.869565},\n",
       "   'custom_fillers': {'macro_f1': 0.7161239495798319,\n",
       "    'scores':        tp    fn    fp        f1\n",
       "    class                          \n",
       "    0      14  11.0  12.0  0.549020\n",
       "    1       3   1.0   0.0  0.857143\n",
       "    2       5   2.0   4.0  0.625000\n",
       "    3      10   2.0   2.0  0.833333},\n",
       "   'synonym': {'macro_f1': 0.6855602240896359,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      19  6.0  7.0  0.745098\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       4  3.0  5.0  0.500000\n",
       "    3       8  4.0  5.0  0.640000},\n",
       "   'keywordChange': {'macro_f1': 0.8427871148459384,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      21  4.0  3.0  0.857143\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       7  0.0  3.0  0.823529\n",
       "    3      10  2.0  2.0  0.833333},\n",
       "   'spellingError': {'macro_f1': 0.5,\n",
       "    'scores':          tp  fn    fp        f1\n",
       "    class                          \n",
       "    0      17.0   8   9.0  0.666667\n",
       "    1       1.0   3   2.0  0.285714\n",
       "    2       4.0   3  10.0  0.380952\n",
       "    3       9.0   3   6.0  0.666667},\n",
       "   'antonym': {'macro_f1': 0.7674378881987578,\n",
       "    'scores':        tp   fn   fp        f1\n",
       "    class                        \n",
       "    0      17  8.0  8.0  0.680000\n",
       "    1       3  1.0  0.0  0.857143\n",
       "    2       6  1.0  3.0  0.750000\n",
       "    3       9  3.0  2.0  0.782609}},\n",
       "  'other': {'core': {'macro_f1': 0.027777777777777776,\n",
       "    'scores':         tp  fn  fp        f1\n",
       "    class                       \n",
       "    0      1.0   8   8  0.111111\n",
       "    1      0.0   2   1  0.000000\n",
       "    2      0.0   7   0  0.000000\n",
       "    3      0.0   0   6  0.000000},\n",
       "   'custom_fillers': {'macro_f1': 0.1111111111111111,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      1.0  1.0   3  0.333333\n",
       "    1      0.0  1.0   0  0.000000\n",
       "    2      0.0  0.0   2  0.000000},\n",
       "   'synonym': {'macro_f1': 0.1111111111111111,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      1.0  1.0   3  0.333333\n",
       "    1      0.0  1.0   0  0.000000\n",
       "    2      0.0  0.0   2  0.000000},\n",
       "   'keywordChange': {'macro_f1': 0.13333333333333333,\n",
       "    'scores':         tp   fn  fp   f1\n",
       "    class                   \n",
       "    0      1.0  1.0   2  0.4\n",
       "    1      0.0  1.0   0  0.0\n",
       "    2      0.0  0.0   1  0.0},\n",
       "   'spellingError': {'macro_f1': 0.19047619047619047,\n",
       "    'scores':         tp   fn  fp        f1\n",
       "    class                        \n",
       "    0      2.0  0.0   3  0.571429\n",
       "    1      0.0  1.0   0  0.000000\n",
       "    2      0.0  0.0   3  0.000000},\n",
       "   'antonym': {'macro_f1': 0.0,\n",
       "    'scores':         tp  fn  fp   f1\n",
       "    class                  \n",
       "    0      0.0   2   4  0.0\n",
       "    1      0.0   1   0  0.0\n",
       "    2      0.0   0   2  0.0}}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac81730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argument-mining",
   "language": "python",
   "name": "argument-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
