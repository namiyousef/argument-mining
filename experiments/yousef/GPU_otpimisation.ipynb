{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU_otpimisation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvFUc4LzQTj4P9dJez/Oby",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namiyousef/argument-mining/blob/develop/experiments/yousef/GPU_otpimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72gEKBK2Ck4V",
        "outputId": "2ad06bf4-6282-4efc-e81b-5f1e1ece5ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/namiyousef/colab-utils.git\n",
            "  Cloning https://github.com/namiyousef/colab-utils.git to /tmp/pip-req-build-zdd9jwpb\n",
            "  Running command git clone -q https://github.com/namiyousef/colab-utils.git /tmp/pip-req-build-zdd9jwpb\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from colabtools==0.0.1) (7.352.0)\n",
            "Google Drive import successful.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mount successful.\n",
            "Import of ArgMiner successful\n",
            "CUDA device detected. Using GPU...\n"
          ]
        }
      ],
      "source": [
        "# -- env setup\n",
        "import os\n",
        "import gc\n",
        "\n",
        "!python3.7 -m pip install git+https://github.com/namiyousef/colab-utils.git\n",
        "from colabtools.utils import get_gpu_utilization, mount_drive, install_private_library\n",
        "\n",
        "drive_path = mount_drive()\n",
        "project_path = os.path.join(drive_path, 'argument-mining')\n",
        "development_dir = os.path.join(drive_path, 'argument-mining/experiments/yousef')\n",
        "\n",
        "install_private_library(os.path.join(project_path, 'data/github_config.json'), 'argument-mining')\n",
        "\n",
        "# -- public imports\n",
        "import sys\n",
        "from transformers import BigBirdTokenizer, AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# -- private imports\n",
        "from argminer.data import KaggleDataset\n",
        "\n",
        "# -- dev imports\n",
        "# add working package to system path\n",
        "sys.path.append(development_dir)\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from python_files.data import create_labels_doc_level\n",
        "from python_files.run import train_longformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'allenai/longformer-base-4096'\n",
        "print(f'GPU usage: {get_gpu_utilization()}')\n",
        "try:\n",
        "  ma = torch.cuda.memory_allocated()\n",
        "  print(ma)\n",
        "\n",
        "  mc = torch.cuda.memory_reserved()\n",
        "  print(mc)\n",
        "\n",
        "  del tokenizer, model, optimizer, dataset, train_loader, df_kaggle, config_model\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  ma = torch.cuda.memory_allocated()\n",
        "  print(ma)\n",
        "\n",
        "  mc = torch.cuda.memory_cached()\n",
        "  print(mc)\n",
        "  print(f'GPU usage after deletion: {get_gpu_utilization()}')\n",
        "except:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
        "\n",
        "  config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
        "  config_model.num_labels = 15\n",
        "\n",
        "  model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config_model)\n",
        "  optimizer = torch.optim.Adam(params=model.parameters())\n",
        "\n",
        "  df_train = pd.read_csv(os.path.join(project_path, 'data/kaggle/test.csv'))\n",
        "  df_kaggle = df_train[['text', 'labels']].copy(); del df_train\n",
        "  df_kaggle.labels = df_kaggle.labels.apply(lambda x: [int(num) for num in x[1:-1].split(', ')])\n",
        "  df_kaggle.head(), df_kaggle.info()\n",
        "  \n",
        "  dataset = KaggleDataset(df_kaggle, tokenizer, 512)\n",
        "  train_loader = DataLoader(dataset, shuffle=True, batch_size=1)\n",
        "\n",
        "  print(f'GPU usage after creation: {get_gpu_utilization()}')\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  torch.backends.cudnn.enabled = True\n",
        "  print(f'GPU mem: {get_gpu_utilization()}')\n",
        "  model.to('cuda:0')\n",
        "  print(f'GPU mem after model sent: {get_gpu_utilization()}')\n",
        "  for i, (inputs, targets) in enumerate(train_loader):\n",
        "      inputs = {key: val.to('cuda:0') for key, val in inputs.items()}\n",
        "      targets = targets.to('cuda:0')\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss, outputs = model(labels=targets, **inputs, return_dict=False)\n",
        "      print(f'GPU mem modelling batch {i+1}: {get_gpu_utilization()}')\n",
        "      del targets, inputs, loss, outputs\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f'GPU mem batch {i+1}: {get_gpu_utilization()}')\n",
        "      break\n",
        "  model.to('cpu')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXbK-lQACvlQ",
        "outputId": "c78af27e-c9ca-4cc5-8a21-da26e95ca8a4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU usage: 1330\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15594 entries, 0 to 15593\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    15594 non-null  object\n",
            " 1   labels  15594 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 243.8+ KB\n",
            "GPU usage after creation: 1330\n",
            "GPU mem: 1330\n",
            "GPU mem after model sent: 1954\n",
            "GPU mem modelling batch 1: 2806\n",
            "GPU mem batch 1: 1954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hIxVRnrykku7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wu9LFIm_l3sW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --gpu-reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItvdqTBWmiaP",
        "outputId": "43be8554-1171-40bf-ff63-2d0f360c0012"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 00000000:00:04.0 is currently in use by another process.\n",
            "\n",
            "1 device is currently being used by one or more other processes (e.g., Fabric Manager, CUDA application, graphics application such as an X server, or a monitoring application such as another instance of nvidia-smi). Please first kill all processes using this device and all compute applications running in the system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo fuser -v /dev/nvidia*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a-cPS9EnaFJ",
        "outputId": "cb973220-62ff-4e34-a14f-cdc55b7b3bc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     USER        PID ACCESS COMMAND\n",
            "/dev/nvidia0:        root      14034 F...m python3\n",
            "/dev/nvidiactl:      root      14034 F...m python3\n",
            "/dev/nvidia-uvm:     root      14034 F...m python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo fuser -v /dev/nvidia*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjny6jbNnjFk",
        "outputId": "0f4258f9-cfcc-4852-9562-ce02ed3ef8b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     USER        PID ACCESS COMMAND\n",
            "/dev/nvidia0:        root      14034 F...m python3\n",
            "/dev/nvidiactl:      root      14034 F...m python3\n",
            "/dev/nvidia-uvm:     root      14034 F...m python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toeQ8EkPnnVT",
        "outputId": "3d729f08-4119-48ef-c3a3-782d1704a6c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar  7 23:34:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    27W /  70W |   1954MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argminer"
      ],
      "metadata": {
        "id": "9ZO5cVFgnptY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9MDNujKznrCm"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}