{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zIVWaSRUxuA"
      },
      "source": [
        "# Foreword:\n",
        "To run in Colab, we need to have the files in Drive. \n",
        "To do this easily, we have to follow a couple of steps:\n",
        "1. Go to Federico's `NLP_project` folder in Drive [here](https://drive.google.com/drive/folders/16Gm33Ckb_YoX_z_x9xVITt2afa-aAPLX?usp=sharing), and Add a shortcut to your drive.\n",
        "2. Mount google drive on Colab by running the code cells that will follow.\n",
        "3. Done, the directory structure will look like this:\n",
        "```\n",
        "YOUR_GOOGLE_DRIVE/\n",
        "└── COMP0087/\n",
        "    ├── data/\n",
        "    │   ├── test\n",
        "    │   ├── train\n",
        "    │   ├── train.csv\n",
        "    │   └── sample_submission.csv\n",
        "    ├── model\n",
        "    └── output\n",
        "```\n",
        "4. Make sure you change the directory you are using in the `HyperParameters` class defined below to `/content/drive/MyDrive/NLP_project`\n",
        "I have already done this automatically by setting a cd to that folder if we are on colab, I am writing this just so that you are aware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ifzzOtkEUxuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be4b55c-738a-4f60-c694-6b3c68fc8843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "ON_COLAB = True\n",
        "if ON_COLAB:\n",
        "  # Mount drive:\n",
        "  from google.colab import drive, files\n",
        "  # mount Google Drive\n",
        "  drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5K5Cr2fUxuE"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eriNZqVPVW9a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# if on Colab, we need to install missing stuff!\n",
        "if ON_COLAB:\n",
        "  !pip install transformers\n",
        "  !pip install iterative-stratification\n",
        "  !pip install nvidia-ml-py3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OtO0xcw1UxuF"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gpu_utilization():\n",
        "    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\n",
        "\n",
        "def get_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    return info.used//1024**2\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()\n",
        "\n",
        "print_gpu_utilization()"
      ],
      "metadata": {
        "id": "4EE28HBXpjrJ",
        "outputId": "5429cf18-4ca2-4f2b-81c3-639fadde27d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 0 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i9N0xf4RXlVL"
      },
      "outputs": [],
      "source": [
        "if ON_COLAB:\n",
        "  !cd /content/drive/MyDrive/NLP_project\n",
        "\n",
        "\n",
        "# DATA DIR ---- TO CHANGE\n",
        "DATA_DIR = 'drive/MyDrive/NLP_project/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lgNlqCIUxuF"
      },
      "source": [
        "Config class containing all necessary hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "COkK9LFUUxuG"
      },
      "outputs": [],
      "source": [
        "class HyperParameters:\n",
        "    \n",
        "    # Here we choose model type. Can be changed for others\n",
        "    name = 'longformer'\n",
        "    model_savename = 'longformer'\n",
        "    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\n",
        "    \n",
        "    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\n",
        "    base_dir = DATA_DIR\n",
        "    data_dir = os.path.join(base_dir, 'data')\n",
        "    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\n",
        "    model_dir = os.path.join(base_dir, f'model/{name}')\n",
        "    output_dir = os.path.join(base_dir, f'output/{name}')\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    is_debug = False\n",
        "    n_epoch = 2 # not to exceed runtime limit\n",
        "    n_fold = 5\n",
        "    verbose_steps = 500\n",
        "    random_seed = 42\n",
        "\n",
        "    # Model specific hyperparameters\n",
        "    max_length = 1024\n",
        "    inference_max_length = 4096\n",
        "    train_batch_size = 4\n",
        "    valid_batch_size = 4\n",
        "    lr = 4e-5\n",
        "\n",
        "    # Task hyperparameters\n",
        "    num_labels = 15\n",
        "    label_subtokens = True\n",
        "    output_hidden_states = True\n",
        "    hidden_dropout_prob = 0.1\n",
        "    layer_norm_eps = 1e-7\n",
        "    add_pooling_layer = False\n",
        "    verbose_steps = 500\n",
        "    if is_debug:\n",
        "        debug_sample = 1000\n",
        "        verbose_steps = 16\n",
        "        n_epoch = 1\n",
        "        n_fold = 2\n",
        "\n",
        "if not os.path.exists(HyperParameters.model_dir):\n",
        "    !mkdir $HyperParameters.model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF5bW6aVUxuG"
      },
      "source": [
        "Constant for the task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "08glsD47UxuH"
      },
      "outputs": [],
      "source": [
        "IGNORE_INDEX = -100\n",
        "NON_LABEL = -1\n",
        "OUTPUT_LABELS = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
        "                 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
        "LABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\n",
        "IDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\n",
        "\n",
        "MIN_THRESH = {\n",
        "    \"I-Lead\": 9,\n",
        "    \"I-Position\": 5,\n",
        "    \"I-Evidence\": 14,\n",
        "    \"I-Claim\": 3,\n",
        "    \"I-Concluding Statement\": 11,\n",
        "    \"I-Counterclaim\": 6,\n",
        "    \"I-Rebuttal\": 4,\n",
        "}\n",
        "\n",
        "PROB_THRESH = {\n",
        "    \"I-Lead\": 0.7,\n",
        "    \"I-Position\": 0.55,\n",
        "    \"I-Evidence\": 0.65,\n",
        "    \"I-Claim\": 0.55,\n",
        "    \"I-Concluding Statement\": 0.7,\n",
        "    \"I-Counterclaim\": 0.5,\n",
        "    \"I-Rebuttal\": 0.55,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WJbWh4GUxuI"
      },
      "source": [
        "Taming randomness and setting device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2u_8GvKvUxuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862f4ecd-b342-482b-f278-b81d02cbf9c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed=HyperParameters.random_seed):\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    random.seed(seed)\n",
        "    \n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.deterministic =True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Set proper device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXW7QdTPUxuK"
      },
      "source": [
        "# Data importing and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh1Bdo7UUxuK"
      },
      "source": [
        "Importing corrected data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cDhwkU7MUxuK"
      },
      "outputs": [],
      "source": [
        "df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O7hy7b6uUxuL"
      },
      "outputs": [],
      "source": [
        "def agg_essays(train_flg):\n",
        "    \"\"\"\n",
        "    Splits every word in an essay and adds the text of each essay to a dataframe.\n",
        "    \"\"\"\n",
        "    folder = 'train' if train_flg else 'test'\n",
        "    names, texts =[], []\n",
        "    for f in tqdm(list(os.listdir(f'{HyperParameters.data_dir}/{folder}'))):\n",
        "        names.append(f.replace('.txt', ''))\n",
        "        texts.append(open(f'{HyperParameters.data_dir}/{folder}/' + f, 'r').read())\n",
        "        df_texts = pd.DataFrame({'id': names, 'text': texts})\n",
        "\n",
        "    df_texts['text_split'] = df_texts.text.str.split()\n",
        "    print('Completed tokenizing texts.')\n",
        "    return df_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "av_kvU8eUxuL"
      },
      "outputs": [],
      "source": [
        "def ner(df_texts, df_train):\n",
        "    \"\"\"\n",
        "    Maps discourse type to each word of the text, according to the train.csv file.\n",
        "    \"\"\"\n",
        "    all_entities = []\n",
        "    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\n",
        "        total = len(row['text_split'])\n",
        "        entities = ['O'] * total\n",
        "\n",
        "        for _, row2 in df_train[df_train['id'] == row['id']].iterrows():\n",
        "            discourse = row2['discourse_type']\n",
        "            list_ix = [int(x) for x in row2['predictionstring'].split(' ')]\n",
        "            entities[list_ix[0]] = f'B-{discourse}'\n",
        "            for k in list_ix[1:]: entities[k] = f'I-{discourse}'\n",
        "        all_entities.append(entities)\n",
        "\n",
        "    df_texts['entities'] = all_entities\n",
        "    print('Completed mapping discourse to each token.')\n",
        "    return df_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Sq2Rco3eUxuL"
      },
      "outputs": [],
      "source": [
        "def preprocess(df_train = None):\n",
        "    \"\"\"\n",
        "    Generates the dataframe we will use for training.\n",
        "    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\n",
        "    \"\"\"\n",
        "    if df_train is None:\n",
        "        train_flg = False\n",
        "    else:\n",
        "        train_flg = True\n",
        "    \n",
        "    df_texts = agg_essays(train_flg)\n",
        "\n",
        "    if train_flg:\n",
        "        df_texts = ner(df_texts, df_train)\n",
        "    return df_texts\n",
        "\n",
        "# Make sure we only run pre-processing if we did not do it in the past:\n",
        "\n",
        "if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \n",
        "    alltrain_texts = preprocess(df_alltrain)\n",
        "    test_texts = preprocess()\n",
        "else:\n",
        "    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZLa0fkXbUxuM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0d563dfc-8361-42b3-a647-29c2bd7d3caa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-153d834b-f0ca-4e15-8e6b-718f54baba79\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>text_split</th>\n",
              "      <th>entities</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3321A3E87AD3</td>\n",
              "      <td>I do agree that some students would benefit fr...</td>\n",
              "      <td>[I, do, agree, that, some, students, would, be...</td>\n",
              "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DFEAEC512BAB</td>\n",
              "      <td>Should students design a summer project for sc...</td>\n",
              "      <td>[Should, students, design, a, summer, project,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-Position, I-Positio...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2E4AFCD3987F</td>\n",
              "      <td>Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...</td>\n",
              "      <td>[Dear, State, Senator, ,, In, the, ruels, of, ...</td>\n",
              "      <td>[O, O, O, O, B-Position, I-Position, I-Positio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EB6C2AF20BFE</td>\n",
              "      <td>People sometimes have a different opinion than...</td>\n",
              "      <td>[People, sometimes, have, a, different, opinio...</td>\n",
              "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A91A08E523D5</td>\n",
              "      <td>Dear senator,\\n\\nAs you know the Electoral Col...</td>\n",
              "      <td>[Dear, senator,, As, you, know, the, Electoral...</td>\n",
              "      <td>[O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-153d834b-f0ca-4e15-8e6b-718f54baba79')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-153d834b-f0ca-4e15-8e6b-718f54baba79 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-153d834b-f0ca-4e15-8e6b-718f54baba79');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             id                                               text  \\\n",
              "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
              "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
              "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
              "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
              "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
              "\n",
              "                                          text_split  \\\n",
              "0  [I, do, agree, that, some, students, would, be...   \n",
              "1  [Should, students, design, a, summer, project,...   \n",
              "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
              "3  [People, sometimes, have, a, different, opinio...   \n",
              "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
              "\n",
              "                                            entities  kfold  \n",
              "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
              "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
              "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
              "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
              "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Visualize preprocessing result:\n",
        "parse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\n",
        "alltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\n",
        "alltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\n",
        "\n",
        "alltrain_texts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRFI_KLMUxuN"
      },
      "source": [
        "# Preparing cross validation\n",
        "\n",
        "Generate proper folds so that the essays we use in each fold have roughly the same number of discourse types overall.\n",
        "Only compute if we don't have the file in directory already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IOkXJ5UTUxuN"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \n",
        "    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \n",
        "    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\n",
        "\n",
        "    # Generate name for the dummy columns\n",
        "    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\n",
        "    # dfx is now only the dataset with dummy columns selected: don't need to pass the data to do the splits\n",
        "    dfx = dfx[dummy_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a1AuTst8UxuN"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \n",
        "    # Generate cross validation object\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Extract labels\n",
        "    labels = [c for c in dfx.columns if c != \"id\"]\n",
        "    dfx_labels = dfx[labels]\n",
        "\n",
        "    # Dummy kfold assignment\n",
        "    dfx[\"kfold\"] = -1\n",
        "\n",
        "    # Split\n",
        "    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\n",
        "        print(len(trn_), len(val_))\n",
        "        \n",
        "        # Change the value of the kfold column at the validation index to the value of the fold\n",
        "        # This will tell us when to use the current entry in the validation set\n",
        "        dfx.loc[val_, \"kfold\"] = fold\n",
        "\n",
        "    # merge back to original dataframe\n",
        "    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\n",
        "    print(alltrain_texts.kfold.value_counts())\n",
        "\n",
        "    # Save so next time we import it directly\n",
        "    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7fcRANPUxuO"
      },
      "source": [
        "# Model and Dataset classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gDGkTBtUxuO"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "APIkM7YwUxuO"
      },
      "outputs": [],
      "source": [
        "# need help with this\n",
        "class FeedbackPrizeDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len, has_labels):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_labels = has_labels\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.text[index]\n",
        "        encoding = self.tokenizer(\n",
        "            text.split(),\n",
        "            is_split_into_words = True,\n",
        "            padding = 'max_length',\n",
        "            truncation = True,\n",
        "            max_length = self.max_len\n",
        "        )\n",
        "        word_ids = encoding.word_ids()\n",
        "\n",
        "        # targets\n",
        "        if self.has_labels:\n",
        "            word_labels = self.data.entities[index]\n",
        "            prev_word_idx = None\n",
        "            labels_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    labels_ids.append(IGNORE_INDEX)\n",
        "                elif word_idx != prev_word_idx:\n",
        "                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n",
        "                else:\n",
        "                    if HyperParameters.label_subtokens:\n",
        "                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n",
        "                    else:\n",
        "                        labels_ids.append(IGNORE_INDEX)\n",
        "                prev_word_idx = word_idx\n",
        "            encoding['labels'] = labels_ids\n",
        "        # convert to torch.tensor\n",
        "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
        "        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\n",
        "        item['word_ids'] = torch.as_tensor(word_ids2)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE6pC1LdUxuP"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "X9UQQRt2UxuP"
      },
      "outputs": [],
      "source": [
        "class FeedbackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeedbackModel, self).__init__()\n",
        "        \n",
        "        # init config of transformer model of choice:\n",
        "        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\n",
        "        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\n",
        "        print(model_config)\n",
        "        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\n",
        "        \n",
        "        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\n",
        "        self.model_config = model_config\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.dropout4 = nn.Dropout(0.4)\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\n",
        "    \n",
        "    def forward(self, input_ids, mask):\n",
        "        x = self.backbone(input_ids, mask)\n",
        "        logits1 = self.head(self.dropout1(x[0]))\n",
        "        logits2 = self.head(self.dropout2(x[0]))\n",
        "        logits3 = self.head(self.dropout3(x[0]))\n",
        "        logits4 = self.head(self.dropout4(x[0]))\n",
        "        logits5 = self.head(self.dropout5(x[0]))\n",
        "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RslNIsyMUxuP"
      },
      "outputs": [],
      "source": [
        "def build_model_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\n",
        "    model = FeedbackModel()\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PriptyGpUxuP"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmRQM_TPUxuQ"
      },
      "source": [
        "What does this do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_CJQ0r-yUxuQ"
      },
      "outputs": [],
      "source": [
        "# Need help with this: used in training to transform raw logits to labels needed\n",
        "def active_logits(raw_logits, word_ids):\n",
        "    word_ids = word_ids.view(-1)\n",
        "    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\n",
        "    active_mask = active_mask != NON_LABEL\n",
        "    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\n",
        "    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\n",
        "    active_logits = active_logits.view(-1, HyperParameters.num_labels) \n",
        "    return active_logits\n",
        "\n",
        "def active_labels(labels):\n",
        "    active_mask = labels.view(-1) != IGNORE_INDEX\n",
        "    active_labels = torch.masked_select(labels.view(-1), active_mask)\n",
        "    return active_labels\n",
        "\n",
        "def active_preds_prob(active_logits):\n",
        "    active_preds = torch.argmax(active_logits, axis = 1)\n",
        "    active_preds_prob, _ = torch.max(active_logits, axis = 1)\n",
        "    return active_preds, active_preds_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkokQcdsUxuQ"
      },
      "source": [
        "F1 scoring functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SfhEUmMcUxuQ"
      },
      "outputs": [],
      "source": [
        "def calculate_overlap(set_pred, set_gt):\n",
        "    \"\"\"\n",
        "    Calculates if the overlap between prediction and\n",
        "    ground truth is enough fora potential True positive\n",
        "    \"\"\"\n",
        "    # Length of each and intersection\n",
        "    try:\n",
        "        len_gt = len(set_gt)\n",
        "        len_pred = len(set_pred)\n",
        "        inter = len(set_gt & set_pred)\n",
        "        overlap_1 = inter / len_gt\n",
        "        overlap_2 = inter/ len_pred\n",
        "        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n",
        "    except:  # at least one of the input is NaN\n",
        "        return False\n",
        "\n",
        "def score_feedback_comp_micro(pred_df, gt_df, discourse_type):\n",
        "    \"\"\"\n",
        "    A function that scores for the kaggle\n",
        "        Student Writing Competition\n",
        "        \n",
        "    Uses the steps in the evaluation page here:\n",
        "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
        "    \"\"\"\n",
        "    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n",
        "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
        "    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n",
        "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
        "    pred_df['pred_id'] = pred_df.index\n",
        "    gt_df['gt_id'] = gt_df.index\n",
        "    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n",
        "    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n",
        "    \n",
        "    # Step 1. all ground truths and predictions for a given class are compared.\n",
        "    joined = pred_df.merge(gt_df,\n",
        "                           left_on='id',\n",
        "                           right_on='id',\n",
        "                           how='outer',\n",
        "                           suffixes=('_pred','_gt')\n",
        "                          )\n",
        "    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \n",
        "                                                     joined.predictionstring_gt)]\n",
        "    \n",
        "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
        "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
        "    # the prediction is a match and considered a true positive.\n",
        "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
        "    # we don't need to compute the match to compute the score\n",
        "    TP = joined.loc[overlaps]['gt_id'].nunique()\n",
        "\n",
        "    # 3. Any unmatched ground truths are false negatives\n",
        "    # and any unmatched predictions are false positives.\n",
        "    TPandFP = len(pred_df)\n",
        "    TPandFN = len(gt_df)\n",
        "    \n",
        "    #calc microf1\n",
        "    my_f1_score = 2*TP / (TPandFP + TPandFN)\n",
        "    return my_f1_score\n",
        "\n",
        "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
        "    \"\"\"\n",
        "    Final helper function for model evaluation.\n",
        "    \n",
        "    Args:\n",
        "    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: ['id','class','predictionstring']\n",
        "    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\n",
        "    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\n",
        "    \n",
        "    Returns:\n",
        "    f1                      (float): F1 score of the model\n",
        "    (optional) class_scores  (dict): Dictionary of per-class F1 score\n",
        "    \"\"\"\n",
        "    class_scores = {}\n",
        "    for discourse_type in gt_df.discourse_type.unique():\n",
        "        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\n",
        "        class_scores[discourse_type] = class_score\n",
        "    f1 = np.mean([v for v in class_scores.values()])\n",
        "    if return_class_scores:\n",
        "        return f1, class_scores\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIQdqtzoUxuR"
      },
      "source": [
        "# Training and validation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GrUCjcbUxuR"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "WSeGDvufUxuR"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, train_data_loader, optimizer, epoch, criterion):\n",
        "    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    stream = tqdm(train_data_loader)\n",
        "    # Init gradscaler to ensure everything works smoothly on cuda\n",
        "    scaler = GradScaler()\n",
        "    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\n",
        "    for batch_idx, batch in enumerate(stream, start = 1):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        raw_labels = batch['labels'].to(device, dtype = torch.long)\n",
        "        word_ids = batch['word_ids'].to(device, dtype = torch.long)\n",
        "        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\n",
        "\n",
        "        # Calculate output with autocast for cuda support\n",
        "        with autocast():\n",
        "            raw_logits = model(input_ids = ids, mask = mask)\n",
        "        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\n",
        "\n",
        "        #logits = active_logits(raw_logits, word_ids)\n",
        "        #labels = active_labels(raw_labels)\n",
        "\n",
        "        logits = raw_logits\n",
        "        labels = raw_labels\n",
        "        sf_logits = torch.softmax(logits, dim=-1)\n",
        "        preds, preds_prob = active_preds_prob(sf_logits)\n",
        "        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        train_loss += loss.item()\n",
        "        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\n",
        "\n",
        "        \n",
        "        if batch_idx % HyperParameters.verbose_steps == 0:\n",
        "            loss_step = train_loss / batch_idx\n",
        "            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\n",
        "\n",
        "        if batch_idx == 3:\n",
        "          raise Exception()\n",
        "            \n",
        "    epoch_loss = train_loss / batch_idx\n",
        "    epoch_accuracy = train_accuracy / batch_idx    \n",
        "    print_gpu_utilization()\n",
        "    # Cleanup\n",
        "    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\n",
        "    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRR3l8npUxuR"
      },
      "source": [
        "### Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mj0dSxx0UxuR"
      },
      "outputs": [],
      "source": [
        "def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\n",
        "    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\n",
        "    f1score =[]\n",
        "    # classes = oof['class'].unique()\n",
        "    classes = ['Lead', 'Position', 'Claim','Counterclaim', 'Rebuttal','Evidence','Concluding Statement']\n",
        "    print(f\"Validation F1 scores\")\n",
        "\n",
        "    for c in classes:\n",
        "        pred_df = oof.loc[oof['class'] == c].copy()\n",
        "        gt_df = df_val_eval.loc[df_val_eval['discourse_type'] == c].copy()\n",
        "        f1 = score_feedback_comp(pred_df, gt_df)\n",
        "        print(f' * {c:<10}: {f1:4f}')\n",
        "        f1score.append(f1)\n",
        "    f1avg = np.mean(f1score)\n",
        "    print(f'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}')\n",
        "    return valid_loss, oof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSZ3iq8dUxuS"
      },
      "source": [
        "### Infer on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3qtgxPTlUxuS"
      },
      "outputs": [],
      "source": [
        "def inference(model, data_loader, criterion, valid_flg):\n",
        "    stream = tqdm(data_loader)\n",
        "    model.eval()\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "    all_logits = None\n",
        "    for batch_idx, batch in enumerate(stream, start = 1):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        with torch.no_grad():\n",
        "            raw_logits = model(input_ids=ids, mask = mask)\n",
        "        del ids, mask\n",
        "        \n",
        "        word_ids = batch['word_ids'].to(device, dtype = torch.long)\n",
        "        logits = active_logits(raw_logits, word_ids)\n",
        "        sf_logits = torch.softmax(logits, dim= -1)\n",
        "        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\n",
        "        if valid_flg:    \n",
        "            raw_labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            labels = active_labels(raw_labels)\n",
        "            preds, preds_prob = active_preds_prob(sf_logits)\n",
        "            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "            loss = criterion(logits, labels)\n",
        "            valid_loss += loss.item()\n",
        "        \n",
        "        if batch_idx == 1:\n",
        "            all_logits = sf_raw_logits.cpu().numpy()\n",
        "        else:\n",
        "            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\n",
        "\n",
        "    \n",
        "    if valid_flg:        \n",
        "        epoch_loss = valid_loss / batch_idx\n",
        "        epoch_accuracy = valid_accuracy / batch_idx\n",
        "    else:\n",
        "        epoch_loss, epoch_accuracy = 0, 0\n",
        "    return all_logits, epoch_loss, epoch_accuracy\n",
        "\n",
        "\n",
        "def preds_class_prob(all_logits, data_loader):\n",
        "    print(\"predict target class and its probabilty\")\n",
        "    final_predictions = []\n",
        "    final_predictions_score = []\n",
        "    stream = tqdm(data_loader)\n",
        "    len_sample = all_logits.shape[0]\n",
        "\n",
        "    for batch_idx, batch in enumerate(stream, start=0):\n",
        "        for minibatch_idx in range(HyperParameters.valid_batch_size):\n",
        "            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\n",
        "            if sample_idx > len_sample - 1 : break\n",
        "            word_ids = batch['word_ids'][minibatch_idx].numpy()\n",
        "            predictions =[]\n",
        "            predictions_prob = []\n",
        "            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\n",
        "            pred_score = np.max(all_logits[sample_idx], axis=1)\n",
        "            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\n",
        "            prev_word_idx = -1\n",
        "            for idx, word_idx in enumerate(word_ids):\n",
        "                if word_idx == -1:\n",
        "                    pass\n",
        "                elif word_idx != prev_word_idx:\n",
        "                    predictions.append(pred_class_labels[idx])\n",
        "                    predictions_prob.append(pred_score[idx])\n",
        "                    prev_word_idx = word_idx\n",
        "            final_predictions.append(predictions)\n",
        "            final_predictions_score.append(predictions_prob)\n",
        "    return final_predictions, final_predictions_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "E5dGJEzDUxuS"
      },
      "outputs": [],
      "source": [
        "def get_preds_onefold(model, df, dl, criterion, valid_flg):\n",
        "    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\n",
        "    all_preds, all_preds_prob = preds_class_prob(logits, dl)\n",
        "    df_pred = post_process_pred(df, all_preds, all_preds_prob)\n",
        "    return df_pred, valid_loss, valid_acc\n",
        "\n",
        "def get_preds_folds(model, df, dl, criterion, valid_flg=False):\n",
        "    for i_fold in range(HyperParameters.n_fold):\n",
        "        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\n",
        "        print(f\"{model_filename} inference\")\n",
        "        model = model.to(device)\n",
        "        model.load_state_dict(torch.load(model_filename))\n",
        "        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\n",
        "        if i_fold == 0:\n",
        "            avg_pred_logits = logits\n",
        "        else:\n",
        "            avg_pred_logits += logits\n",
        "    avg_pred_logits /= HyperParameters.n_fold\n",
        "    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\n",
        "    df_pred = post_process_pred(df, all_preds, all_preds_prob)\n",
        "    return df_pred\n",
        "\n",
        "def post_process_pred(df, all_preds, all_preds_prob):\n",
        "    final_preds = []\n",
        "    for i in range(len(df)):\n",
        "        idx = df.id.values[i]\n",
        "        pred = all_preds[i]\n",
        "        pred_prob = all_preds_prob[i]\n",
        "        j = 0\n",
        "        while j < len(pred):\n",
        "            cls = pred[j]\n",
        "            if cls == 'O': j += 1\n",
        "            else: cls = cls.replace('B', 'I')\n",
        "            end = j + 1\n",
        "            while end < len(pred) and pred[end] == cls:\n",
        "                end += 1\n",
        "            if cls != 'O' and cls !='':\n",
        "                avg_score = np.mean(pred_prob[j:end])\n",
        "                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\n",
        "                    final_preds.append((idx, cls.replace('I-', ''), ' '.join(map(str, list(range(j, end))))))\n",
        "            j = end\n",
        "    df_pred = pd.DataFrame(final_preds)\n",
        "    df_pred.columns = ['id', 'class', 'new_predictionstring']\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAmJyKnHUxuS"
      },
      "source": [
        "# Finally getting some action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_gpu_utilization()"
      ],
      "metadata": {
        "id": "eNRB8mFsufoL",
        "outputId": "52736f74-53aa-4d0f-f5e8-881d96bcc99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 3 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_size(size):\n",
        "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
        "\tassert(isinstance(size, torch.Size))\n",
        "\treturn \" × \".join(map(str, size))\n",
        "\n",
        "def dump_tensors(gpu_only=True):\n",
        "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
        "\timport gc\n",
        "\ttotal_size = 0\n",
        "\tfor obj in gc.get_objects():\n",
        "\t\ttry:\n",
        "\t\t\tif torch.is_tensor(obj):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
        "\t\t\t\t\ttotal_size += obj.numel()\n",
        "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
        "\t\t\t\t\ttotal_size += obj.data.numel()\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tpass        \n",
        "\tprint(\"Total size:\", total_size)"
      ],
      "metadata": {
        "id": "WVHmI_NzvTPy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_tensors()"
      ],
      "metadata": {
        "id": "U-gD0UN7vVMu",
        "outputId": "02a1068e-c816-487d-bcd3-a47b02e4f188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: astor.all_symbols is deprecated.  Please use astor.symbol_data.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: astor.treewalk is deprecated.  Please use astor.tree_walk.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: astor.codegen is deprecated.  Please use astor.code_gen.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py:171: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  \"torch.distributed.reduce_op is deprecated, please use \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KaggleDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class for loading data in batches after it has been processed\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # -- prepare data\n",
        "        assert sorted(dataframe.columns) == ['labels', 'text'], f\"Please make sure input dataframe has the columns (text, labels)\"\n",
        "        # data must be in the correct format\n",
        "        self.inputs = dataframe.text.values\n",
        "        self.targets = dataframe.labels.values\n",
        "        #if not is_string_dtype(self.inputs): raise TypeError('Text data must be string type')\n",
        "        # TODO assertion below is bug; not deleting so remember to add correct assertions\n",
        "        #if not is_integer_dtype(self.targets): raise TypeError('Label data must be integer type')\n",
        "\n",
        "        # -- prepare tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # self.inputs anf self.targets must be of a type that is indexible as shown\n",
        "        inputs = self.inputs[index]\n",
        "        targets = self.targets[index]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            # consider parametrising these\n",
        "            inputs.split(),\n",
        "            is_split_into_words=True, # this means that extra \\n should be ignored\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        word_ids = inputs.word_ids()\n",
        "        word_id_mask = [word_id is not None for word_id in word_ids]\n",
        "        word_ids = [word_id for word_id in word_ids if word_id is not None]\n",
        "\n",
        "        inputs = {\n",
        "            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\n",
        "        }\n",
        "        targets = torch.as_tensor(targets, dtype=torch.long)\n",
        "        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\n",
        "        expanded_targets[word_id_mask] = targets[word_ids]\n",
        "        inputs['labels'] = expanded_targets\n",
        "        inputs['word_ids'] = torch.tensor(word_ids, dtype=torch.long)\n",
        "        return inputs\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "-bNSOZ2t01cS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7dHGvjpPUxuS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eef7a90afeb44c448532e10c8ef5a20e",
            "ca3198d71a3f4887874da99cc3e78882",
            "c4d33c2a02644f85a146f390d6aa0736",
            "9d549d2487dd4197a978761bd72be2ad",
            "ec2dd4649f114c6fad2430c245fe68aa",
            "81cfe2eb173e40fea93d20dc2f171f03",
            "433b7e11f87442198edef448a8c16074",
            "22c91851758d425383c6653f877dbc1b",
            "25ffd2bde996468a9c003a1883191813",
            "45645e8eb5af4fca8c6a84fa3b99dab3",
            "d81a205322c84e7e8830374196e00a36"
          ]
        },
        "outputId": "c2bc7157-8d6d-44fd-f2c4-5bb29bf72079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<frame at 0x557474b03c20, file '<ipython-input-45-be461fdb2349>', line 32, code train_fn>, {'model': FeedbackModel(\n",
            "  (backbone): LongformerModel(\n",
            "    (embeddings): LongformerEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): LongformerEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): LongformerPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.2, inplace=False)\n",
            "  (dropout3): Dropout(p=0.3, inplace=False)\n",
            "  (dropout4): Dropout(p=0.4, inplace=False)\n",
            "  (dropout5): Dropout(p=0.5, inplace=False)\n",
            "  (head): Linear(in_features=768, out_features=15, bias=True)\n",
            "), 'train_data_loader': <torch.utils.data.dataloader.DataLoader object at 0x7faf5a5c8ad0>, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 4e-05\n",
            "    weight_decay: 0\n",
            "), 'epoch': 1, 'criterion': CrossEntropyLoss(), 'train_loss': 0, 'train_accuracy': 0, 'stream': <tqdm.notebook.tqdm_notebook object at 0x7faf5a5c8810>, 'scaler': <torch.cuda.amp.grad_scaler.GradScaler object at 0x7faf5a4bc850>, 'batch_idx': 1, 'batch': {'input_ids': tensor([[    0,    38,   109,  2854,    14,   103,   521,    74,  1796,    31,\n",
            "           804,  4050,     6,    53,    45,    70,     9,   106,    74,     4,\n",
            "           345,    18,    82,    14,  1532,   357,   114,    51,    32,    11,\n",
            "            10,  8171,    19,    10,  3254,  2934,    11,   760,     9,   106,\n",
            "             4,   345,    18,    67,    82,    54,  3860,     7,  4161, 12461,\n",
            "            11,     5,  8171,     4,  2246,    32,     5,    82,    14,    74,\n",
            "          4008,  1796,    31,    41,   804,   768,     4,   404,     9,    49,\n",
            "         26434,    32,  9820,     8,    51,    64,  1004,     5,  3149,    62,\n",
            "            15,    49,  9972,   114,    51,  3860,     7,  4161,     4,   345,\n",
            "            32,  1795,    31,   258,   145,    11,    10,  8171,     8,   145,\n",
            "            11,    41,   804,  8171,     4,  1773,    38,  2854,    14,   103,\n",
            "            74,  1796,     6,   905,    18,  1067,    59,   596,    24,    18,\n",
            "           129,    22, 12465,   845,   318,    47,    32,   101,   162,     6,\n",
            "            47,    74,  1195,    28,    11,    10,  8171,    19,    97,   521,\n",
            "            54,    32,  2239,     5,   276,   383,    25,    47,     4,  9068,\n",
            "             6,    47,    64,  1394,   106,    13,   244,   114,    47,  2039,\n",
            "           402,    50,    95,    33,    10,  1607,    15,     5,  5674,     4,\n",
            "          1801,  1686,     7,   643,    59,    99,    47,   214,  2239,    40,\n",
            "          1733,   110,  2655,    15,     5,  2087,     8,    47,   429,   190,\n",
            "          6396,   106,    10,   631,    50,    80,     4,   152,    40,    67,\n",
            "           492,    47,    10,   778,     7,  1067,    19,     5,  2721,  1972,\n",
            "            11,   110,  1380,   328,   407,     6,   596,    32,    51,   258,\n",
            "         10142,   116,    38,   348,   393,   551,    10,  1380,   804,     6,\n",
            "            53,    24,   531,    28,  2579,     7,   836,     5,  1380, 11263,\n",
            "            47,   236,     4,   407,     6,   114,    47,   214,    41,    15,\n",
            "            12,   627,    12,  2977,  1294,     6,    38,    74,   356,    88,\n",
            "           804,  7484,     4, 42027,    16,    10,   380,   631,   209,   360,\n",
            "             8,    38,   348,  1317,     9,   171,    82,    19,    22,  4684,\n",
            "          4294,  6882,   845,   252,    32,   888,  6023,     7,   213,     7,\n",
            "           334,     8,    38,  1395,   206,     9,    10,   357, 13306,    13,\n",
            "            14,    87,   804,  4050,     4,   374,     5,    97,   865,     6,\n",
            "            82,    11,     5,  8171,    64,  1994,    15,     5,  2087,     8,\n",
            "          6396,   349,    97,    92,   383,    25,    38,    26,   137,     4,\n",
            "           370,    33,     5,  4808,     9,  1996,     5,  3254,    13,  3485,\n",
            "             4,  8374,    11,    10,  8171,    67, 15296,   110,    82,  2417,\n",
            "             4,   166,    70,   216,    89,    16,   205,     8,  1099,    14,\n",
            "           283,    31,   804,  7484,     4,  1216,    32,   103,  2188,   596,\n",
            "            82,   206,    89,    16,   117,   205,   567,    31,    24,     4,\n",
            "           370,   189,    45,   120,     5,   455,  2239,   676,    31,   546,\n",
            "            23,   110,  9972,  1195,    87,   519,   951,    19,    70,     5,\n",
            "          5274,  2686,     7,    47,     8,   442,    47,  3116,   106,   159,\n",
            "             4,   370,    33,   117,   592,   301,   150,    47,   214,  2239,\n",
            "             8,    47,    64,    75,  1994,    19, 18295,     7,  1532,    55,\n",
            "            50,   120,  5274,    13,    10, 19122,     4,   318,    47,   214,\n",
            "           804,  2239,     6,   960,    16,    15,    47,   328,   345,    16,\n",
            "           410,   244,    14,   606,    19,   804,  4050,  1195,    87,    10,\n",
            "           588,  8171,     4,   404,    11,    70,     6,    38,  2854,    14,\n",
            "           103,   521,    74,  1796,    31,   804,  4050,     4,  2246,    54,\n",
            "            32,   182,  2222,     8, 15295,    74,   657,   804,  4050,     4,\n",
            "            38,   619,    14,    89,    32,    55,   521,    54,    74,  1195,\n",
            "          1095,    11,     5,  8171,     8,  2764, 15189,     7,  1532,    55,\n",
            "             4,    85,    18,    70,    59,    99,   349,   621, 20618,     4,\n",
            "          5855,  7484,    32,    45,    13,   961,    53,    32,  2299,    10,\n",
            "           372,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]]), 'labels': tensor([[ 0,  1,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  2,\n",
            "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  4, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  3, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  4,  4,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  3, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10,  6, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "         13, 13,  3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  5, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12,  0]]), 'word_ids': tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  10,  11,  12,\n",
            "          13,  14,  15,  16,  16,  17,  17,  18,  19,  20,  21,  22,  23,  24,\n",
            "          25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  35,  36,  36,\n",
            "          37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  46,  47,  48,  49,\n",
            "          50,  51,  52,  53,  54,  55,  56,  57,  58,  58,  59,  60,  61,  62,\n",
            "          63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
            "          77,  78,  79,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
            "          90,  91,  92,  93,  94,  94,  95,  96,  97,  98,  99, 100, 101, 101,\n",
            "         102, 102, 103, 104, 105, 106, 106, 107, 108, 108, 108, 109, 110, 111,\n",
            "         112, 113, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
            "         125, 126, 127, 128, 129, 130, 131, 131, 132, 132, 133, 134, 135, 136,\n",
            "         137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
            "         150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 161, 162,\n",
            "         163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 175,\n",
            "         176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
            "         190, 191, 191, 192, 192, 193, 194, 195, 196, 197, 197, 198, 198, 199,\n",
            "         200, 201, 202, 203, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
            "         213, 214, 215, 215, 216, 216, 217, 218, 218, 219, 220, 220, 220, 220,\n",
            "         220, 221, 221, 222, 223, 224, 225, 226, 227, 227, 228, 229, 230, 231,\n",
            "         232, 233, 234, 235, 236, 236, 237, 238, 239, 240, 241, 242, 242, 242,\n",
            "         243, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
            "         256, 257, 258, 259, 260, 261, 262, 263, 264, 264, 265, 266, 267, 268,\n",
            "         268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
            "         282, 283, 284, 285, 286, 287, 287, 288, 289, 290, 291, 292, 293, 294,\n",
            "         295, 296, 297, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 306,\n",
            "         307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 319,\n",
            "         320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
            "         333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n",
            "         347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360,\n",
            "         361, 362, 363, 363, 364, 365, 366, 367, 368, 369, 370, 370, 371, 372,\n",
            "         373, 374, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
            "         386, 386, 387, 388, 388, 389, 390, 390, 391, 392, 393, 394, 394, 395,\n",
            "         396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 408,\n",
            "         409, 410, 411, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
            "         421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 432, 433,\n",
            "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "         448, 448, 449, 450, 451, 451, 452, 452, 453, 454, 455, 456, 457, 458,\n",
            "         458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469]])}, 'ids': tensor([[    0,    38,   109,  2854,    14,   103,   521,    74,  1796,    31,\n",
            "           804,  4050,     6,    53,    45,    70,     9,   106,    74,     4,\n",
            "           345,    18,    82,    14,  1532,   357,   114,    51,    32,    11,\n",
            "            10,  8171,    19,    10,  3254,  2934,    11,   760,     9,   106,\n",
            "             4,   345,    18,    67,    82,    54,  3860,     7,  4161, 12461,\n",
            "            11,     5,  8171,     4,  2246,    32,     5,    82,    14,    74,\n",
            "          4008,  1796,    31,    41,   804,   768,     4,   404,     9,    49,\n",
            "         26434,    32,  9820,     8,    51,    64,  1004,     5,  3149,    62,\n",
            "            15,    49,  9972,   114,    51,  3860,     7,  4161,     4,   345,\n",
            "            32,  1795,    31,   258,   145,    11,    10,  8171,     8,   145,\n",
            "            11,    41,   804,  8171,     4,  1773,    38,  2854,    14,   103,\n",
            "            74,  1796,     6,   905,    18,  1067,    59,   596,    24,    18,\n",
            "           129,    22, 12465,   845,   318,    47,    32,   101,   162,     6,\n",
            "            47,    74,  1195,    28,    11,    10,  8171,    19,    97,   521,\n",
            "            54,    32,  2239,     5,   276,   383,    25,    47,     4,  9068,\n",
            "             6,    47,    64,  1394,   106,    13,   244,   114,    47,  2039,\n",
            "           402,    50,    95,    33,    10,  1607,    15,     5,  5674,     4,\n",
            "          1801,  1686,     7,   643,    59,    99,    47,   214,  2239,    40,\n",
            "          1733,   110,  2655,    15,     5,  2087,     8,    47,   429,   190,\n",
            "          6396,   106,    10,   631,    50,    80,     4,   152,    40,    67,\n",
            "           492,    47,    10,   778,     7,  1067,    19,     5,  2721,  1972,\n",
            "            11,   110,  1380,   328,   407,     6,   596,    32,    51,   258,\n",
            "         10142,   116,    38,   348,   393,   551,    10,  1380,   804,     6,\n",
            "            53,    24,   531,    28,  2579,     7,   836,     5,  1380, 11263,\n",
            "            47,   236,     4,   407,     6,   114,    47,   214,    41,    15,\n",
            "            12,   627,    12,  2977,  1294,     6,    38,    74,   356,    88,\n",
            "           804,  7484,     4, 42027,    16,    10,   380,   631,   209,   360,\n",
            "             8,    38,   348,  1317,     9,   171,    82,    19,    22,  4684,\n",
            "          4294,  6882,   845,   252,    32,   888,  6023,     7,   213,     7,\n",
            "           334,     8,    38,  1395,   206,     9,    10,   357, 13306,    13,\n",
            "            14,    87,   804,  4050,     4,   374,     5,    97,   865,     6,\n",
            "            82,    11,     5,  8171,    64,  1994,    15,     5,  2087,     8,\n",
            "          6396,   349,    97,    92,   383,    25,    38,    26,   137,     4,\n",
            "           370,    33,     5,  4808,     9,  1996,     5,  3254,    13,  3485,\n",
            "             4,  8374,    11,    10,  8171,    67, 15296,   110,    82,  2417,\n",
            "             4,   166,    70,   216,    89,    16,   205,     8,  1099,    14,\n",
            "           283,    31,   804,  7484,     4,  1216,    32,   103,  2188,   596,\n",
            "            82,   206,    89,    16,   117,   205,   567,    31,    24,     4,\n",
            "           370,   189,    45,   120,     5,   455,  2239,   676,    31,   546,\n",
            "            23,   110,  9972,  1195,    87,   519,   951,    19,    70,     5,\n",
            "          5274,  2686,     7,    47,     8,   442,    47,  3116,   106,   159,\n",
            "             4,   370,    33,   117,   592,   301,   150,    47,   214,  2239,\n",
            "             8,    47,    64,    75,  1994,    19, 18295,     7,  1532,    55,\n",
            "            50,   120,  5274,    13,    10, 19122,     4,   318,    47,   214,\n",
            "           804,  2239,     6,   960,    16,    15,    47,   328,   345,    16,\n",
            "           410,   244,    14,   606,    19,   804,  4050,  1195,    87,    10,\n",
            "           588,  8171,     4,   404,    11,    70,     6,    38,  2854,    14,\n",
            "           103,   521,    74,  1796,    31,   804,  4050,     4,  2246,    54,\n",
            "            32,   182,  2222,     8, 15295,    74,   657,   804,  4050,     4,\n",
            "            38,   619,    14,    89,    32,    55,   521,    54,    74,  1195,\n",
            "          1095,    11,     5,  8171,     8,  2764, 15189,     7,  1532,    55,\n",
            "             4,    85,    18,    70,    59,    99,   349,   621, 20618,     4,\n",
            "          5855,  7484,    32,    45,    13,   961,    53,    32,  2299,    10,\n",
            "           372,     2]], device='cuda:0'), 'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]], device='cuda:0'), 'raw_labels': tensor([[ 0,  1,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  2,\n",
            "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  4, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  3, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  4,  4,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  3, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10,  6, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "         13, 13,  3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  5, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12,  0]], device='cuda:0'), 'word_ids': tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  10,  11,  12,\n",
            "          13,  14,  15,  16,  16,  17,  17,  18,  19,  20,  21,  22,  23,  24,\n",
            "          25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  35,  36,  36,\n",
            "          37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  46,  47,  48,  49,\n",
            "          50,  51,  52,  53,  54,  55,  56,  57,  58,  58,  59,  60,  61,  62,\n",
            "          63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
            "          77,  78,  79,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
            "          90,  91,  92,  93,  94,  94,  95,  96,  97,  98,  99, 100, 101, 101,\n",
            "         102, 102, 103, 104, 105, 106, 106, 107, 108, 108, 108, 109, 110, 111,\n",
            "         112, 113, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
            "         125, 126, 127, 128, 129, 130, 131, 131, 132, 132, 133, 134, 135, 136,\n",
            "         137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
            "         150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 161, 162,\n",
            "         163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 175,\n",
            "         176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
            "         190, 191, 191, 192, 192, 193, 194, 195, 196, 197, 197, 198, 198, 199,\n",
            "         200, 201, 202, 203, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
            "         213, 214, 215, 215, 216, 216, 217, 218, 218, 219, 220, 220, 220, 220,\n",
            "         220, 221, 221, 222, 223, 224, 225, 226, 227, 227, 228, 229, 230, 231,\n",
            "         232, 233, 234, 235, 236, 236, 237, 238, 239, 240, 241, 242, 242, 242,\n",
            "         243, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
            "         256, 257, 258, 259, 260, 261, 262, 263, 264, 264, 265, 266, 267, 268,\n",
            "         268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
            "         282, 283, 284, 285, 286, 287, 287, 288, 289, 290, 291, 292, 293, 294,\n",
            "         295, 296, 297, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 306,\n",
            "         307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 319,\n",
            "         320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
            "         333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n",
            "         347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360,\n",
            "         361, 362, 363, 363, 364, 365, 366, 367, 368, 369, 370, 370, 371, 372,\n",
            "         373, 374, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
            "         386, 386, 387, 388, 388, 389, 390, 390, 391, 392, 393, 394, 394, 395,\n",
            "         396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 408,\n",
            "         409, 410, 411, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
            "         421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 432, 433,\n",
            "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "         448, 448, 449, 450, 451, 451, 452, 452, 453, 454, 455, 456, 457, 458,\n",
            "         458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469]],\n",
            "       device='cuda:0'), 'raw_logits': tensor([[[ 0.0022, -0.0614,  0.2769,  ..., -0.2312, -0.6885, -0.5127],\n",
            "         [-0.1190, -0.4741,  0.3333,  ..., -0.7251, -0.8467, -0.0442],\n",
            "         [ 0.0159, -0.3042, -0.0317,  ..., -0.4985, -0.5952, -0.0503],\n",
            "         ...,\n",
            "         [ 0.2512, -0.3347, -0.0521,  ..., -0.2754, -0.3628,  0.0062],\n",
            "         [ 0.0347, -0.1438,  0.1104,  ..., -0.2168, -0.0638, -0.1055],\n",
            "         [ 0.1670, -0.3379,  0.0158,  ..., -0.3630, -0.3218, -0.1176]]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>), 'logits': tensor([[[ 0.0022, -0.0614,  0.2769,  ..., -0.2312, -0.6885, -0.5127],\n",
            "         [-0.1190, -0.4741,  0.3333,  ..., -0.7251, -0.8467, -0.0442],\n",
            "         [ 0.0159, -0.3042, -0.0317,  ..., -0.4985, -0.5952, -0.0503],\n",
            "         ...,\n",
            "         [ 0.2512, -0.3347, -0.0521,  ..., -0.2754, -0.3628,  0.0062],\n",
            "         [ 0.0347, -0.1438,  0.1104,  ..., -0.2168, -0.0638, -0.1055],\n",
            "         [ 0.1670, -0.3379,  0.0158,  ..., -0.3630, -0.3218, -0.1176]]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>), 'labels': tensor([[ 0,  1,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  2,\n",
            "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  4, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  3, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  4,  4,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
            "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,  3, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10,  6, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "         13, 13,  3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
            "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  5, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
            "         12, 12, 12, 12, 12, 12, 12, 12, 12,  0]], device='cuda:0'), 'sf_logits': tensor([[[0.0702, 0.0659, 0.0923,  ..., 0.0556, 0.0352, 0.0419],\n",
            "         [0.0630, 0.0442, 0.0991,  ..., 0.0344, 0.0305, 0.0679],\n",
            "         [0.0731, 0.0531, 0.0697,  ..., 0.0437, 0.0397, 0.0684],\n",
            "         ...,\n",
            "         [0.0869, 0.0483, 0.0641,  ..., 0.0513, 0.0470, 0.0680],\n",
            "         [0.0660, 0.0553, 0.0712,  ..., 0.0514, 0.0598, 0.0574],\n",
            "         [0.0861, 0.0520, 0.0740,  ..., 0.0507, 0.0528, 0.0648]]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SoftmaxBackward0>), 'preds': tensor([[195, 493, 254, 270,   8,  46, 140,  17,  28, 477, 191, 132, 277, 358,\n",
            "         233]], device='cuda:0'), 'preds_prob': tensor([[0.1466, 0.0923, 0.1648, 0.1434, 0.2612, 0.1029, 0.0749, 0.1711, 0.2017,\n",
            "         0.1486, 0.1722, 0.1117, 0.1091, 0.0998, 0.1423]], device='cuda:0',\n",
            "       dtype=torch.float16, grad_fn=<MaxBackward0>)}, {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'ON_COLAB = True\\nif ON_COLAB:\\n  # Mount drive:\\n  from google.colab import drive, files\\n  # mount Google Drive\\n  drive.mount(\"/content/drive\")', \"get_ipython().run_cell_magic('capture', '', '# if on Colab, we need to install missing stuff!\\\\nif ON_COLAB:\\\\n  !pip install transformers\\\\n  !pip install iterative-stratification\\\\n  !pip install nvidia-ml-py3')\", 'import gc\\nimport os\\nimport torch\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport torch.nn as nn\\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\\n\\nfrom tqdm.notebook import tqdm\\nfrom sklearn.metrics import accuracy_score\\nfrom torch.cuda.amp import autocast, GradScaler\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold', 'def print_gpu_utilization():\\n    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\\n\\ndef get_gpu_utilization():\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    info = nvmlDeviceGetMemoryInfo(handle)\\n    return info.used//1024**2\\n\\ndef print_summary(result):\\n    print(f\"Time: {result.metrics[\\'train_runtime\\']:.2f}\")\\n    print(f\"Samples/second: {result.metrics[\\'train_samples_per_second\\']:.2f}\")\\n    print_gpu_utilization()\\n\\nprint_gpu_utilization()', \"if ON_COLAB:\\n  get_ipython().system('cd /content/drive/MyDrive/NLP_project')\\n\\n\\n# DATA DIR ---- TO CHANGE\\nDATA_DIR = 'drive/MyDrive/NLP_project/'\", \"class HyperParameters:\\n    \\n    # Here we choose model type. Can be changed for others\\n    name = 'longformer'\\n    model_savename = 'longformer'\\n    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\\n    \\n    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\\n    base_dir = DATA_DIR\\n    data_dir = os.path.join(base_dir, 'data')\\n    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\\n    model_dir = os.path.join(base_dir, f'model/{name}')\\n    output_dir = os.path.join(base_dir, f'output/{name}')\\n    \\n    # Training hyperparameters\\n    is_debug = False\\n    n_epoch = 2 # not to exceed runtime limit\\n    n_fold = 5\\n    verbose_steps = 500\\n    random_seed = 42\\n\\n    # Model specific hyperparameters\\n    max_length = 1024\\n    inference_max_length = 4096\\n    train_batch_size = 4\\n    valid_batch_size = 4\\n    lr = 4e-5\\n\\n    # Task hyperparameters\\n    num_labels = 15\\n    label_subtokens = True\\n    output_hidden_states = True\\n    hidden_dropout_prob = 0.1\\n    layer_norm_eps = 1e-7\\n    add_pooling_layer = False\\n    verbose_steps = 500\\n    if is_debug:\\n        debug_sample = 1000\\n        verbose_steps = 16\\n        n_epoch = 1\\n        n_fold = 2\\n\\nif not os.path.exists(HyperParameters.model_dir):\\n    get_ipython().system('mkdir $HyperParameters.model_dir')\", 'IGNORE_INDEX = -100\\nNON_LABEL = -1\\nOUTPUT_LABELS = [\\'O\\', \\'B-Lead\\', \\'I-Lead\\', \\'B-Position\\', \\'I-Position\\', \\'B-Claim\\', \\'I-Claim\\', \\'B-Counterclaim\\', \\'I-Counterclaim\\', \\n                 \\'B-Rebuttal\\', \\'I-Rebuttal\\', \\'B-Evidence\\', \\'I-Evidence\\', \\'B-Concluding Statement\\', \\'I-Concluding Statement\\']\\nLABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\\nIDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\\n\\nMIN_THRESH = {\\n    \"I-Lead\": 9,\\n    \"I-Position\": 5,\\n    \"I-Evidence\": 14,\\n    \"I-Claim\": 3,\\n    \"I-Concluding Statement\": 11,\\n    \"I-Counterclaim\": 6,\\n    \"I-Rebuttal\": 4,\\n}\\n\\nPROB_THRESH = {\\n    \"I-Lead\": 0.7,\\n    \"I-Position\": 0.55,\\n    \"I-Evidence\": 0.65,\\n    \"I-Claim\": 0.55,\\n    \"I-Concluding Statement\": 0.7,\\n    \"I-Counterclaim\": 0.5,\\n    \"I-Rebuttal\": 0.55,\\n}', \"def set_seed(seed=HyperParameters.random_seed):\\n    np.random.seed(seed)\\n    \\n    random.seed(seed)\\n    \\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    \\n    torch.backends.cudnn.deterministic =True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed()\\n\\n# Set proper device\\nif torch.cuda.is_available():\\n    device = torch.device('cuda')\\nelse:\\n    device = torch.device('cpu')\\n\\nprint(f'Using device: {device}')\", \"df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')\", 'def agg_essays(train_flg):\\n    \"\"\"\\n    Splits every word in an essay and adds the text of each essay to a dataframe.\\n    \"\"\"\\n    folder = \\'train\\' if train_flg else \\'test\\'\\n    names, texts =[], []\\n    for f in tqdm(list(os.listdir(f\\'{HyperParameters.data_dir}/{folder}\\'))):\\n        names.append(f.replace(\\'.txt\\', \\'\\'))\\n        texts.append(open(f\\'{HyperParameters.data_dir}/{folder}/\\' + f, \\'r\\').read())\\n        df_texts = pd.DataFrame({\\'id\\': names, \\'text\\': texts})\\n\\n    df_texts[\\'text_split\\'] = df_texts.text.str.split()\\n    print(\\'Completed tokenizing texts.\\')\\n    return df_texts', 'def ner(df_texts, df_train):\\n    \"\"\"\\n    Maps discourse type to each word of the text, according to the train.csv file.\\n    \"\"\"\\n    all_entities = []\\n    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\\n        total = len(row[\\'text_split\\'])\\n        entities = [\\'O\\'] * total\\n\\n        for _, row2 in df_train[df_train[\\'id\\'] == row[\\'id\\']].iterrows():\\n            discourse = row2[\\'discourse_type\\']\\n            list_ix = [int(x) for x in row2[\\'predictionstring\\'].split(\\' \\')]\\n            entities[list_ix[0]] = f\\'B-{discourse}\\'\\n            for k in list_ix[1:]: entities[k] = f\\'I-{discourse}\\'\\n        all_entities.append(entities)\\n\\n    df_texts[\\'entities\\'] = all_entities\\n    print(\\'Completed mapping discourse to each token.\\')\\n    return df_texts', 'def preprocess(df_train = None):\\n    \"\"\"\\n    Generates the dataframe we will use for training.\\n    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\\n    \"\"\"\\n    if df_train is None:\\n        train_flg = False\\n    else:\\n        train_flg = True\\n    \\n    df_texts = agg_essays(train_flg)\\n\\n    if train_flg:\\n        df_texts = ner(df_texts, df_train)\\n    return df_texts\\n\\n# Make sure we only run pre-processing if we did not do it in the past:\\n\\nif not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    alltrain_texts = preprocess(df_alltrain)\\n    test_texts = preprocess()\\nelse:\\n    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")', \"# Visualize preprocessing result:\\nparse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\\nalltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\\nalltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\\n\\nalltrain_texts.head()\", 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \\n    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\\n\\n    # Generate name for the dummy columns\\n    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\\n    # dfx is now only the dataset with dummy columns selected: don\\'t need to pass the data to do the splits\\n    dfx = dfx[dummy_cols]', 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Generate cross validation object\\n    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n    # Extract labels\\n    labels = [c for c in dfx.columns if c != \"id\"]\\n    dfx_labels = dfx[labels]\\n\\n    # Dummy kfold assignment\\n    dfx[\"kfold\"] = -1\\n\\n    # Split\\n    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\\n        print(len(trn_), len(val_))\\n        \\n        # Change the value of the kfold column at the validation index to the value of the fold\\n        # This will tell us when to use the current entry in the validation set\\n        dfx.loc[val_, \"kfold\"] = fold\\n\\n    # merge back to original dataframe\\n    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\\n    print(alltrain_texts.kfold.value_counts())\\n\\n    # Save so next time we import it directly\\n    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)', \"# need help with this\\nclass FeedbackPrizeDataset(Dataset):\\n    def __init__(self, dataframe, tokenizer, max_len, has_labels):\\n        self.len = len(dataframe)\\n        self.data = dataframe\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.has_labels = has_labels\\n    \\n    def __getitem__(self, index):\\n        text = self.data.text[index]\\n        encoding = self.tokenizer(\\n            text.split(),\\n            is_split_into_words = True,\\n            padding = 'max_length',\\n            truncation = True,\\n            max_length = self.max_len\\n        )\\n        word_ids = encoding.word_ids()\\n\\n        # targets\\n        if self.has_labels:\\n            word_labels = self.data.entities[index]\\n            prev_word_idx = None\\n            labels_ids = []\\n            for word_idx in word_ids:\\n                if word_idx is None:\\n                    labels_ids.append(IGNORE_INDEX)\\n                elif word_idx != prev_word_idx:\\n                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                else:\\n                    if HyperParameters.label_subtokens:\\n                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                    else:\\n                        labels_ids.append(IGNORE_INDEX)\\n                prev_word_idx = word_idx\\n            encoding['labels'] = labels_ids\\n        # convert to torch.tensor\\n        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\\n        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\\n        item['word_ids'] = torch.as_tensor(word_ids2)\\n        return item\\n\\n    def __len__(self):\\n        return self.len\", \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'def build_model_tokenizer():\\n    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\\n    model = FeedbackModel()\\n    return model, tokenizer', '# Need help with this: used in training to transform raw logits to labels needed\\ndef active_logits(raw_logits, word_ids):\\n    word_ids = word_ids.view(-1)\\n    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\\n    active_mask = active_mask != NON_LABEL\\n    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\\n    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\\n    active_logits = active_logits.view(-1, HyperParameters.num_labels) \\n    return active_logits\\n\\ndef active_labels(labels):\\n    active_mask = labels.view(-1) != IGNORE_INDEX\\n    active_labels = torch.masked_select(labels.view(-1), active_mask)\\n    return active_labels\\n\\ndef active_preds_prob(active_logits):\\n    active_preds = torch.argmax(active_logits, axis = 1)\\n    active_preds_prob, _ = torch.max(active_logits, axis = 1)\\n    return active_preds, active_preds_prob', 'def calculate_overlap(set_pred, set_gt):\\n    \"\"\"\\n    Calculates if the overlap between prediction and\\n    ground truth is enough fora potential True positive\\n    \"\"\"\\n    # Length of each and intersection\\n    try:\\n        len_gt = len(set_gt)\\n        len_pred = len(set_pred)\\n        inter = len(set_gt & set_pred)\\n        overlap_1 = inter / len_gt\\n        overlap_2 = inter/ len_pred\\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\\n    except:  # at least one of the input is NaN\\n        return False\\n\\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\\n    \"\"\"\\n    A function that scores for the kaggle\\n        Student Writing Competition\\n        \\n    Uses the steps in the evaluation page here:\\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\\n    \"\"\"\\n    gt_df = gt_df.loc[gt_df[\\'discourse_type\\'] == discourse_type, \\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df = pred_df.loc[pred_df[\\'class\\'] == discourse_type,\\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df[\\'pred_id\\'] = pred_df.index\\n    gt_df[\\'gt_id\\'] = gt_df.index\\n    pred_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in pred_df[\\'predictionstring\\']]\\n    gt_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in gt_df[\\'predictionstring\\']]\\n    \\n    # Step 1. all ground truths and predictions for a given class are compared.\\n    joined = pred_df.merge(gt_df,\\n                           left_on=\\'id\\',\\n                           right_on=\\'id\\',\\n                           how=\\'outer\\',\\n                           suffixes=(\\'_pred\\',\\'_gt\\')\\n                          )\\n    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \\n                                                     joined.predictionstring_gt)]\\n    \\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \\n    # and the overlap between the prediction and the ground truth >= 0.5,\\n    # the prediction is a match and considered a true positive.\\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\\n    # we don\\'t need to compute the match to compute the score\\n    TP = joined.loc[overlaps][\\'gt_id\\'].nunique()\\n\\n    # 3. Any unmatched ground truths are false negatives\\n    # and any unmatched predictions are false positives.\\n    TPandFP = len(pred_df)\\n    TPandFN = len(gt_df)\\n    \\n    #calc microf1\\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\\n    return my_f1_score\\n\\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\\n    \"\"\"\\n    Final helper function for model evaluation.\\n    \\n    Args:\\n    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: [\\'id\\',\\'class\\',\\'predictionstring\\']\\n    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\\n    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\\n    \\n    Returns:\\n    f1                      (float): F1 score of the model\\n    (optional) class_scores  (dict): Dictionary of per-class F1 score\\n    \"\"\"\\n    class_scores = {}\\n    for discourse_type in gt_df.discourse_type.unique():\\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\\n        class_scores[discourse_type] = class_score\\n    f1 = np.mean([v for v in class_scores.values()])\\n    if return_class_scores:\\n        return f1, class_scores\\n    return f1', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n            \\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\\n    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\\n    f1score =[]\\n    # classes = oof[\\'class\\'].unique()\\n    classes = [\\'Lead\\', \\'Position\\', \\'Claim\\',\\'Counterclaim\\', \\'Rebuttal\\',\\'Evidence\\',\\'Concluding Statement\\']\\n    print(f\"Validation F1 scores\")\\n\\n    for c in classes:\\n        pred_df = oof.loc[oof[\\'class\\'] == c].copy()\\n        gt_df = df_val_eval.loc[df_val_eval[\\'discourse_type\\'] == c].copy()\\n        f1 = score_feedback_comp(pred_df, gt_df)\\n        print(f\\' * {c:<10}: {f1:4f}\\')\\n        f1score.append(f1)\\n    f1avg = np.mean(f1score)\\n    print(f\\'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}\\')\\n    return valid_loss, oof', 'def inference(model, data_loader, criterion, valid_flg):\\n    stream = tqdm(data_loader)\\n    model.eval()\\n    \\n    valid_loss = 0\\n    valid_accuracy = 0\\n    all_logits = None\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch[\\'input_ids\\'].to(device, dtype = torch.long)\\n        mask = batch[\\'attention_mask\\'].to(device, dtype = torch.long)\\n        with torch.no_grad():\\n            raw_logits = model(input_ids=ids, mask = mask)\\n        del ids, mask\\n        \\n        word_ids = batch[\\'word_ids\\'].to(device, dtype = torch.long)\\n        logits = active_logits(raw_logits, word_ids)\\n        sf_logits = torch.softmax(logits, dim= -1)\\n        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\\n        if valid_flg:    \\n            raw_labels = batch[\\'labels\\'].to(device, dtype = torch.long)\\n            labels = active_labels(raw_labels)\\n            preds, preds_prob = active_preds_prob(sf_logits)\\n            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n            loss = criterion(logits, labels)\\n            valid_loss += loss.item()\\n        \\n        if batch_idx == 1:\\n            all_logits = sf_raw_logits.cpu().numpy()\\n        else:\\n            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\\n\\n    \\n    if valid_flg:        \\n        epoch_loss = valid_loss / batch_idx\\n        epoch_accuracy = valid_accuracy / batch_idx\\n    else:\\n        epoch_loss, epoch_accuracy = 0, 0\\n    return all_logits, epoch_loss, epoch_accuracy\\n\\n\\ndef preds_class_prob(all_logits, data_loader):\\n    print(\"predict target class and its probabilty\")\\n    final_predictions = []\\n    final_predictions_score = []\\n    stream = tqdm(data_loader)\\n    len_sample = all_logits.shape[0]\\n\\n    for batch_idx, batch in enumerate(stream, start=0):\\n        for minibatch_idx in range(HyperParameters.valid_batch_size):\\n            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\\n            if sample_idx > len_sample - 1 : break\\n            word_ids = batch[\\'word_ids\\'][minibatch_idx].numpy()\\n            predictions =[]\\n            predictions_prob = []\\n            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\\n            pred_score = np.max(all_logits[sample_idx], axis=1)\\n            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\\n            prev_word_idx = -1\\n            for idx, word_idx in enumerate(word_ids):\\n                if word_idx == -1:\\n                    pass\\n                elif word_idx != prev_word_idx:\\n                    predictions.append(pred_class_labels[idx])\\n                    predictions_prob.append(pred_score[idx])\\n                    prev_word_idx = word_idx\\n            final_predictions.append(predictions)\\n            final_predictions_score.append(predictions_prob)\\n    return final_predictions, final_predictions_score', 'def get_preds_onefold(model, df, dl, criterion, valid_flg):\\n    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n    all_preds, all_preds_prob = preds_class_prob(logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred, valid_loss, valid_acc\\n\\ndef get_preds_folds(model, df, dl, criterion, valid_flg=False):\\n    for i_fold in range(HyperParameters.n_fold):\\n        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\\n        print(f\"{model_filename} inference\")\\n        model = model.to(device)\\n        model.load_state_dict(torch.load(model_filename))\\n        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n        if i_fold == 0:\\n            avg_pred_logits = logits\\n        else:\\n            avg_pred_logits += logits\\n    avg_pred_logits /= HyperParameters.n_fold\\n    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred\\n\\ndef post_process_pred(df, all_preds, all_preds_prob):\\n    final_preds = []\\n    for i in range(len(df)):\\n        idx = df.id.values[i]\\n        pred = all_preds[i]\\n        pred_prob = all_preds_prob[i]\\n        j = 0\\n        while j < len(pred):\\n            cls = pred[j]\\n            if cls == \\'O\\': j += 1\\n            else: cls = cls.replace(\\'B\\', \\'I\\')\\n            end = j + 1\\n            while end < len(pred) and pred[end] == cls:\\n                end += 1\\n            if cls != \\'O\\' and cls !=\\'\\':\\n                avg_score = np.mean(pred_prob[j:end])\\n                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\\n                    final_preds.append((idx, cls.replace(\\'I-\\', \\'\\'), \\' \\'.join(map(str, list(range(j, end))))))\\n            j = end\\n    df_pred = pd.DataFrame(final_preds)\\n    df_pred.columns = [\\'id\\', \\'class\\', \\'new_predictionstring\\']\\n    return df_pred', 'print_gpu_utilization()', 'def pretty_size(size):\\n\\t\"\"\"Pretty prints a torch.Size object\"\"\"\\n\\tassert(isinstance(size, torch.Size))\\n\\treturn \" × \".join(map(str, size))\\n\\ndef dump_tensors(gpu_only=True):\\n\\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\\n\\timport gc\\n\\ttotal_size = 0\\n\\tfor obj in gc.get_objects():\\n\\t\\ttry:\\n\\t\\t\\tif torch.is_tensor(obj):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" pinned\" if obj.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  pretty_size(obj.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.numel()\\n\\t\\t\\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   type(obj.data).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" pinned\" if obj.data.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" grad\" if obj.requires_grad else \"\", \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" volatile\" if obj.volatile else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   pretty_size(obj.data.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.data.numel()\\n\\t\\texcept Exception as e:\\n\\t\\t\\tpass        \\n\\tprint(\"Total size:\", total_size)', 'dump_tensors()', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        #if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        print(model_config)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n            print(raw_logits.shape)\\n            print(word_ids.shape)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  raise Exception()\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])'], '_oh': {13:              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  }, '_dh': ['/content'], '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>, 'In': ['', 'ON_COLAB = True\\nif ON_COLAB:\\n  # Mount drive:\\n  from google.colab import drive, files\\n  # mount Google Drive\\n  drive.mount(\"/content/drive\")', \"get_ipython().run_cell_magic('capture', '', '# if on Colab, we need to install missing stuff!\\\\nif ON_COLAB:\\\\n  !pip install transformers\\\\n  !pip install iterative-stratification\\\\n  !pip install nvidia-ml-py3')\", 'import gc\\nimport os\\nimport torch\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport torch.nn as nn\\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\\n\\nfrom tqdm.notebook import tqdm\\nfrom sklearn.metrics import accuracy_score\\nfrom torch.cuda.amp import autocast, GradScaler\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold', 'def print_gpu_utilization():\\n    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\\n\\ndef get_gpu_utilization():\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    info = nvmlDeviceGetMemoryInfo(handle)\\n    return info.used//1024**2\\n\\ndef print_summary(result):\\n    print(f\"Time: {result.metrics[\\'train_runtime\\']:.2f}\")\\n    print(f\"Samples/second: {result.metrics[\\'train_samples_per_second\\']:.2f}\")\\n    print_gpu_utilization()\\n\\nprint_gpu_utilization()', \"if ON_COLAB:\\n  get_ipython().system('cd /content/drive/MyDrive/NLP_project')\\n\\n\\n# DATA DIR ---- TO CHANGE\\nDATA_DIR = 'drive/MyDrive/NLP_project/'\", \"class HyperParameters:\\n    \\n    # Here we choose model type. Can be changed for others\\n    name = 'longformer'\\n    model_savename = 'longformer'\\n    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\\n    \\n    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\\n    base_dir = DATA_DIR\\n    data_dir = os.path.join(base_dir, 'data')\\n    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\\n    model_dir = os.path.join(base_dir, f'model/{name}')\\n    output_dir = os.path.join(base_dir, f'output/{name}')\\n    \\n    # Training hyperparameters\\n    is_debug = False\\n    n_epoch = 2 # not to exceed runtime limit\\n    n_fold = 5\\n    verbose_steps = 500\\n    random_seed = 42\\n\\n    # Model specific hyperparameters\\n    max_length = 1024\\n    inference_max_length = 4096\\n    train_batch_size = 4\\n    valid_batch_size = 4\\n    lr = 4e-5\\n\\n    # Task hyperparameters\\n    num_labels = 15\\n    label_subtokens = True\\n    output_hidden_states = True\\n    hidden_dropout_prob = 0.1\\n    layer_norm_eps = 1e-7\\n    add_pooling_layer = False\\n    verbose_steps = 500\\n    if is_debug:\\n        debug_sample = 1000\\n        verbose_steps = 16\\n        n_epoch = 1\\n        n_fold = 2\\n\\nif not os.path.exists(HyperParameters.model_dir):\\n    get_ipython().system('mkdir $HyperParameters.model_dir')\", 'IGNORE_INDEX = -100\\nNON_LABEL = -1\\nOUTPUT_LABELS = [\\'O\\', \\'B-Lead\\', \\'I-Lead\\', \\'B-Position\\', \\'I-Position\\', \\'B-Claim\\', \\'I-Claim\\', \\'B-Counterclaim\\', \\'I-Counterclaim\\', \\n                 \\'B-Rebuttal\\', \\'I-Rebuttal\\', \\'B-Evidence\\', \\'I-Evidence\\', \\'B-Concluding Statement\\', \\'I-Concluding Statement\\']\\nLABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\\nIDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\\n\\nMIN_THRESH = {\\n    \"I-Lead\": 9,\\n    \"I-Position\": 5,\\n    \"I-Evidence\": 14,\\n    \"I-Claim\": 3,\\n    \"I-Concluding Statement\": 11,\\n    \"I-Counterclaim\": 6,\\n    \"I-Rebuttal\": 4,\\n}\\n\\nPROB_THRESH = {\\n    \"I-Lead\": 0.7,\\n    \"I-Position\": 0.55,\\n    \"I-Evidence\": 0.65,\\n    \"I-Claim\": 0.55,\\n    \"I-Concluding Statement\": 0.7,\\n    \"I-Counterclaim\": 0.5,\\n    \"I-Rebuttal\": 0.55,\\n}', \"def set_seed(seed=HyperParameters.random_seed):\\n    np.random.seed(seed)\\n    \\n    random.seed(seed)\\n    \\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    \\n    torch.backends.cudnn.deterministic =True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed()\\n\\n# Set proper device\\nif torch.cuda.is_available():\\n    device = torch.device('cuda')\\nelse:\\n    device = torch.device('cpu')\\n\\nprint(f'Using device: {device}')\", \"df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')\", 'def agg_essays(train_flg):\\n    \"\"\"\\n    Splits every word in an essay and adds the text of each essay to a dataframe.\\n    \"\"\"\\n    folder = \\'train\\' if train_flg else \\'test\\'\\n    names, texts =[], []\\n    for f in tqdm(list(os.listdir(f\\'{HyperParameters.data_dir}/{folder}\\'))):\\n        names.append(f.replace(\\'.txt\\', \\'\\'))\\n        texts.append(open(f\\'{HyperParameters.data_dir}/{folder}/\\' + f, \\'r\\').read())\\n        df_texts = pd.DataFrame({\\'id\\': names, \\'text\\': texts})\\n\\n    df_texts[\\'text_split\\'] = df_texts.text.str.split()\\n    print(\\'Completed tokenizing texts.\\')\\n    return df_texts', 'def ner(df_texts, df_train):\\n    \"\"\"\\n    Maps discourse type to each word of the text, according to the train.csv file.\\n    \"\"\"\\n    all_entities = []\\n    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\\n        total = len(row[\\'text_split\\'])\\n        entities = [\\'O\\'] * total\\n\\n        for _, row2 in df_train[df_train[\\'id\\'] == row[\\'id\\']].iterrows():\\n            discourse = row2[\\'discourse_type\\']\\n            list_ix = [int(x) for x in row2[\\'predictionstring\\'].split(\\' \\')]\\n            entities[list_ix[0]] = f\\'B-{discourse}\\'\\n            for k in list_ix[1:]: entities[k] = f\\'I-{discourse}\\'\\n        all_entities.append(entities)\\n\\n    df_texts[\\'entities\\'] = all_entities\\n    print(\\'Completed mapping discourse to each token.\\')\\n    return df_texts', 'def preprocess(df_train = None):\\n    \"\"\"\\n    Generates the dataframe we will use for training.\\n    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\\n    \"\"\"\\n    if df_train is None:\\n        train_flg = False\\n    else:\\n        train_flg = True\\n    \\n    df_texts = agg_essays(train_flg)\\n\\n    if train_flg:\\n        df_texts = ner(df_texts, df_train)\\n    return df_texts\\n\\n# Make sure we only run pre-processing if we did not do it in the past:\\n\\nif not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    alltrain_texts = preprocess(df_alltrain)\\n    test_texts = preprocess()\\nelse:\\n    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")', \"# Visualize preprocessing result:\\nparse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\\nalltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\\nalltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\\n\\nalltrain_texts.head()\", 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \\n    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\\n\\n    # Generate name for the dummy columns\\n    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\\n    # dfx is now only the dataset with dummy columns selected: don\\'t need to pass the data to do the splits\\n    dfx = dfx[dummy_cols]', 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Generate cross validation object\\n    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n    # Extract labels\\n    labels = [c for c in dfx.columns if c != \"id\"]\\n    dfx_labels = dfx[labels]\\n\\n    # Dummy kfold assignment\\n    dfx[\"kfold\"] = -1\\n\\n    # Split\\n    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\\n        print(len(trn_), len(val_))\\n        \\n        # Change the value of the kfold column at the validation index to the value of the fold\\n        # This will tell us when to use the current entry in the validation set\\n        dfx.loc[val_, \"kfold\"] = fold\\n\\n    # merge back to original dataframe\\n    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\\n    print(alltrain_texts.kfold.value_counts())\\n\\n    # Save so next time we import it directly\\n    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)', \"# need help with this\\nclass FeedbackPrizeDataset(Dataset):\\n    def __init__(self, dataframe, tokenizer, max_len, has_labels):\\n        self.len = len(dataframe)\\n        self.data = dataframe\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.has_labels = has_labels\\n    \\n    def __getitem__(self, index):\\n        text = self.data.text[index]\\n        encoding = self.tokenizer(\\n            text.split(),\\n            is_split_into_words = True,\\n            padding = 'max_length',\\n            truncation = True,\\n            max_length = self.max_len\\n        )\\n        word_ids = encoding.word_ids()\\n\\n        # targets\\n        if self.has_labels:\\n            word_labels = self.data.entities[index]\\n            prev_word_idx = None\\n            labels_ids = []\\n            for word_idx in word_ids:\\n                if word_idx is None:\\n                    labels_ids.append(IGNORE_INDEX)\\n                elif word_idx != prev_word_idx:\\n                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                else:\\n                    if HyperParameters.label_subtokens:\\n                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                    else:\\n                        labels_ids.append(IGNORE_INDEX)\\n                prev_word_idx = word_idx\\n            encoding['labels'] = labels_ids\\n        # convert to torch.tensor\\n        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\\n        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\\n        item['word_ids'] = torch.as_tensor(word_ids2)\\n        return item\\n\\n    def __len__(self):\\n        return self.len\", \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'def build_model_tokenizer():\\n    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\\n    model = FeedbackModel()\\n    return model, tokenizer', '# Need help with this: used in training to transform raw logits to labels needed\\ndef active_logits(raw_logits, word_ids):\\n    word_ids = word_ids.view(-1)\\n    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\\n    active_mask = active_mask != NON_LABEL\\n    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\\n    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\\n    active_logits = active_logits.view(-1, HyperParameters.num_labels) \\n    return active_logits\\n\\ndef active_labels(labels):\\n    active_mask = labels.view(-1) != IGNORE_INDEX\\n    active_labels = torch.masked_select(labels.view(-1), active_mask)\\n    return active_labels\\n\\ndef active_preds_prob(active_logits):\\n    active_preds = torch.argmax(active_logits, axis = 1)\\n    active_preds_prob, _ = torch.max(active_logits, axis = 1)\\n    return active_preds, active_preds_prob', 'def calculate_overlap(set_pred, set_gt):\\n    \"\"\"\\n    Calculates if the overlap between prediction and\\n    ground truth is enough fora potential True positive\\n    \"\"\"\\n    # Length of each and intersection\\n    try:\\n        len_gt = len(set_gt)\\n        len_pred = len(set_pred)\\n        inter = len(set_gt & set_pred)\\n        overlap_1 = inter / len_gt\\n        overlap_2 = inter/ len_pred\\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\\n    except:  # at least one of the input is NaN\\n        return False\\n\\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\\n    \"\"\"\\n    A function that scores for the kaggle\\n        Student Writing Competition\\n        \\n    Uses the steps in the evaluation page here:\\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\\n    \"\"\"\\n    gt_df = gt_df.loc[gt_df[\\'discourse_type\\'] == discourse_type, \\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df = pred_df.loc[pred_df[\\'class\\'] == discourse_type,\\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df[\\'pred_id\\'] = pred_df.index\\n    gt_df[\\'gt_id\\'] = gt_df.index\\n    pred_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in pred_df[\\'predictionstring\\']]\\n    gt_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in gt_df[\\'predictionstring\\']]\\n    \\n    # Step 1. all ground truths and predictions for a given class are compared.\\n    joined = pred_df.merge(gt_df,\\n                           left_on=\\'id\\',\\n                           right_on=\\'id\\',\\n                           how=\\'outer\\',\\n                           suffixes=(\\'_pred\\',\\'_gt\\')\\n                          )\\n    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \\n                                                     joined.predictionstring_gt)]\\n    \\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \\n    # and the overlap between the prediction and the ground truth >= 0.5,\\n    # the prediction is a match and considered a true positive.\\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\\n    # we don\\'t need to compute the match to compute the score\\n    TP = joined.loc[overlaps][\\'gt_id\\'].nunique()\\n\\n    # 3. Any unmatched ground truths are false negatives\\n    # and any unmatched predictions are false positives.\\n    TPandFP = len(pred_df)\\n    TPandFN = len(gt_df)\\n    \\n    #calc microf1\\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\\n    return my_f1_score\\n\\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\\n    \"\"\"\\n    Final helper function for model evaluation.\\n    \\n    Args:\\n    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: [\\'id\\',\\'class\\',\\'predictionstring\\']\\n    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\\n    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\\n    \\n    Returns:\\n    f1                      (float): F1 score of the model\\n    (optional) class_scores  (dict): Dictionary of per-class F1 score\\n    \"\"\"\\n    class_scores = {}\\n    for discourse_type in gt_df.discourse_type.unique():\\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\\n        class_scores[discourse_type] = class_score\\n    f1 = np.mean([v for v in class_scores.values()])\\n    if return_class_scores:\\n        return f1, class_scores\\n    return f1', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n            \\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\\n    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\\n    f1score =[]\\n    # classes = oof[\\'class\\'].unique()\\n    classes = [\\'Lead\\', \\'Position\\', \\'Claim\\',\\'Counterclaim\\', \\'Rebuttal\\',\\'Evidence\\',\\'Concluding Statement\\']\\n    print(f\"Validation F1 scores\")\\n\\n    for c in classes:\\n        pred_df = oof.loc[oof[\\'class\\'] == c].copy()\\n        gt_df = df_val_eval.loc[df_val_eval[\\'discourse_type\\'] == c].copy()\\n        f1 = score_feedback_comp(pred_df, gt_df)\\n        print(f\\' * {c:<10}: {f1:4f}\\')\\n        f1score.append(f1)\\n    f1avg = np.mean(f1score)\\n    print(f\\'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}\\')\\n    return valid_loss, oof', 'def inference(model, data_loader, criterion, valid_flg):\\n    stream = tqdm(data_loader)\\n    model.eval()\\n    \\n    valid_loss = 0\\n    valid_accuracy = 0\\n    all_logits = None\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch[\\'input_ids\\'].to(device, dtype = torch.long)\\n        mask = batch[\\'attention_mask\\'].to(device, dtype = torch.long)\\n        with torch.no_grad():\\n            raw_logits = model(input_ids=ids, mask = mask)\\n        del ids, mask\\n        \\n        word_ids = batch[\\'word_ids\\'].to(device, dtype = torch.long)\\n        logits = active_logits(raw_logits, word_ids)\\n        sf_logits = torch.softmax(logits, dim= -1)\\n        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\\n        if valid_flg:    \\n            raw_labels = batch[\\'labels\\'].to(device, dtype = torch.long)\\n            labels = active_labels(raw_labels)\\n            preds, preds_prob = active_preds_prob(sf_logits)\\n            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n            loss = criterion(logits, labels)\\n            valid_loss += loss.item()\\n        \\n        if batch_idx == 1:\\n            all_logits = sf_raw_logits.cpu().numpy()\\n        else:\\n            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\\n\\n    \\n    if valid_flg:        \\n        epoch_loss = valid_loss / batch_idx\\n        epoch_accuracy = valid_accuracy / batch_idx\\n    else:\\n        epoch_loss, epoch_accuracy = 0, 0\\n    return all_logits, epoch_loss, epoch_accuracy\\n\\n\\ndef preds_class_prob(all_logits, data_loader):\\n    print(\"predict target class and its probabilty\")\\n    final_predictions = []\\n    final_predictions_score = []\\n    stream = tqdm(data_loader)\\n    len_sample = all_logits.shape[0]\\n\\n    for batch_idx, batch in enumerate(stream, start=0):\\n        for minibatch_idx in range(HyperParameters.valid_batch_size):\\n            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\\n            if sample_idx > len_sample - 1 : break\\n            word_ids = batch[\\'word_ids\\'][minibatch_idx].numpy()\\n            predictions =[]\\n            predictions_prob = []\\n            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\\n            pred_score = np.max(all_logits[sample_idx], axis=1)\\n            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\\n            prev_word_idx = -1\\n            for idx, word_idx in enumerate(word_ids):\\n                if word_idx == -1:\\n                    pass\\n                elif word_idx != prev_word_idx:\\n                    predictions.append(pred_class_labels[idx])\\n                    predictions_prob.append(pred_score[idx])\\n                    prev_word_idx = word_idx\\n            final_predictions.append(predictions)\\n            final_predictions_score.append(predictions_prob)\\n    return final_predictions, final_predictions_score', 'def get_preds_onefold(model, df, dl, criterion, valid_flg):\\n    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n    all_preds, all_preds_prob = preds_class_prob(logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred, valid_loss, valid_acc\\n\\ndef get_preds_folds(model, df, dl, criterion, valid_flg=False):\\n    for i_fold in range(HyperParameters.n_fold):\\n        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\\n        print(f\"{model_filename} inference\")\\n        model = model.to(device)\\n        model.load_state_dict(torch.load(model_filename))\\n        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n        if i_fold == 0:\\n            avg_pred_logits = logits\\n        else:\\n            avg_pred_logits += logits\\n    avg_pred_logits /= HyperParameters.n_fold\\n    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred\\n\\ndef post_process_pred(df, all_preds, all_preds_prob):\\n    final_preds = []\\n    for i in range(len(df)):\\n        idx = df.id.values[i]\\n        pred = all_preds[i]\\n        pred_prob = all_preds_prob[i]\\n        j = 0\\n        while j < len(pred):\\n            cls = pred[j]\\n            if cls == \\'O\\': j += 1\\n            else: cls = cls.replace(\\'B\\', \\'I\\')\\n            end = j + 1\\n            while end < len(pred) and pred[end] == cls:\\n                end += 1\\n            if cls != \\'O\\' and cls !=\\'\\':\\n                avg_score = np.mean(pred_prob[j:end])\\n                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\\n                    final_preds.append((idx, cls.replace(\\'I-\\', \\'\\'), \\' \\'.join(map(str, list(range(j, end))))))\\n            j = end\\n    df_pred = pd.DataFrame(final_preds)\\n    df_pred.columns = [\\'id\\', \\'class\\', \\'new_predictionstring\\']\\n    return df_pred', 'print_gpu_utilization()', 'def pretty_size(size):\\n\\t\"\"\"Pretty prints a torch.Size object\"\"\"\\n\\tassert(isinstance(size, torch.Size))\\n\\treturn \" × \".join(map(str, size))\\n\\ndef dump_tensors(gpu_only=True):\\n\\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\\n\\timport gc\\n\\ttotal_size = 0\\n\\tfor obj in gc.get_objects():\\n\\t\\ttry:\\n\\t\\t\\tif torch.is_tensor(obj):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" pinned\" if obj.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  pretty_size(obj.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.numel()\\n\\t\\t\\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   type(obj.data).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" pinned\" if obj.data.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" grad\" if obj.requires_grad else \"\", \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" volatile\" if obj.volatile else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   pretty_size(obj.data.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.data.numel()\\n\\t\\texcept Exception as e:\\n\\t\\t\\tpass        \\n\\tprint(\"Total size:\", total_size)', 'dump_tensors()', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        #if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        print(model_config)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n            print(raw_logits.shape)\\n            print(word_ids.shape)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  raise Exception()\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])'], 'Out': {13:              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  }, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fb1405eccd0>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fb13d36efd0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fb13d36efd0>, '_':              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  , '__': '', '___': '', '_i': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_ii': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_iii': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i1': 'ON_COLAB = True\\nif ON_COLAB:\\n  # Mount drive:\\n  from google.colab import drive, files\\n  # mount Google Drive\\n  drive.mount(\"/content/drive\")', 'ON_COLAB': True, 'drive': <module 'google.colab.drive' from '/usr/local/lib/python3.7/dist-packages/google/colab/drive.py'>, 'files': <module 'google.colab.files' from '/usr/local/lib/python3.7/dist-packages/google/colab/files.py'>, '_i2': '%%capture\\n# if on Colab, we need to install missing stuff!\\nif ON_COLAB:\\n  !pip install transformers\\n  !pip install iterative-stratification\\n  !pip install nvidia-ml-py3', '_exit_code': 0, '_i3': 'import gc\\nimport os\\nimport torch\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport torch.nn as nn\\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\\n\\nfrom tqdm.notebook import tqdm\\nfrom sklearn.metrics import accuracy_score\\nfrom torch.cuda.amp import autocast, GradScaler\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold', 'gc': <module 'gc' (built-in)>, 'os': <module 'os' from '/usr/lib/python3.7/os.py'>, 'torch': <module 'torch' from '/usr/local/lib/python3.7/dist-packages/torch/__init__.py'>, 'random': <module 'random' from '/usr/lib/python3.7/random.py'>, 'np': <module 'numpy' from '/usr/local/lib/python3.7/dist-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/usr/local/lib/python3.7/dist-packages/pandas/__init__.py'>, 'nn': <module 'torch.nn' from '/usr/local/lib/python3.7/dist-packages/torch/nn/__init__.py'>, 'nvmlDeviceGetHandleByIndex': <function nvmlDeviceGetHandleByIndex at 0x7fb02a320b00>, 'nvmlDeviceGetMemoryInfo': <function nvmlDeviceGetMemoryInfo at 0x7fb02a32f320>, 'nvmlInit': <function nvmlInit at 0x7fb02a399f80>, 'tqdm': <class 'tqdm.notebook.tqdm_notebook'>, 'accuracy_score': <function accuracy_score at 0x7fb01bc59680>, 'autocast': <class 'torch.cuda.amp.autocast_mode.autocast'>, 'GradScaler': <class 'torch.cuda.amp.grad_scaler.GradScaler'>, 'Dataset': <class 'torch.utils.data.dataset.Dataset'>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'AutoConfig': <class 'transformers.models.auto.configuration_auto.AutoConfig'>, 'AutoModel': <class 'transformers.models.auto.modeling_auto.AutoModel'>, 'AutoTokenizer': <class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>, 'MultilabelStratifiedKFold': <class 'iterstrat.ml_stratifiers.MultilabelStratifiedKFold'>, '_i4': 'def print_gpu_utilization():\\n    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\\n\\ndef get_gpu_utilization():\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    info = nvmlDeviceGetMemoryInfo(handle)\\n    return info.used//1024**2\\n\\ndef print_summary(result):\\n    print(f\"Time: {result.metrics[\\'train_runtime\\']:.2f}\")\\n    print(f\"Samples/second: {result.metrics[\\'train_samples_per_second\\']:.2f}\")\\n    print_gpu_utilization()\\n\\nprint_gpu_utilization()', 'print_gpu_utilization': <function print_gpu_utilization at 0x7fb018ee60e0>, 'get_gpu_utilization': <function get_gpu_utilization at 0x7fb018ea1680>, 'print_summary': <function print_summary at 0x7fb02a36a050>, '_i5': \"if ON_COLAB:\\n  !cd /content/drive/MyDrive/NLP_project\\n\\n\\n# DATA DIR ---- TO CHANGE\\nDATA_DIR = 'drive/MyDrive/NLP_project/'\", 'DATA_DIR': 'drive/MyDrive/NLP_project/', '_i6': \"class HyperParameters:\\n    \\n    # Here we choose model type. Can be changed for others\\n    name = 'longformer'\\n    model_savename = 'longformer'\\n    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\\n    \\n    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\\n    base_dir = DATA_DIR\\n    data_dir = os.path.join(base_dir, 'data')\\n    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\\n    model_dir = os.path.join(base_dir, f'model/{name}')\\n    output_dir = os.path.join(base_dir, f'output/{name}')\\n    \\n    # Training hyperparameters\\n    is_debug = False\\n    n_epoch = 2 # not to exceed runtime limit\\n    n_fold = 5\\n    verbose_steps = 500\\n    random_seed = 42\\n\\n    # Model specific hyperparameters\\n    max_length = 1024\\n    inference_max_length = 4096\\n    train_batch_size = 4\\n    valid_batch_size = 4\\n    lr = 4e-5\\n\\n    # Task hyperparameters\\n    num_labels = 15\\n    label_subtokens = True\\n    output_hidden_states = True\\n    hidden_dropout_prob = 0.1\\n    layer_norm_eps = 1e-7\\n    add_pooling_layer = False\\n    verbose_steps = 500\\n    if is_debug:\\n        debug_sample = 1000\\n        verbose_steps = 16\\n        n_epoch = 1\\n        n_fold = 2\\n\\nif not os.path.exists(HyperParameters.model_dir):\\n    !mkdir $HyperParameters.model_dir\", 'HyperParameters': <class '__main__.HyperParameters'>, '_i7': 'IGNORE_INDEX = -100\\nNON_LABEL = -1\\nOUTPUT_LABELS = [\\'O\\', \\'B-Lead\\', \\'I-Lead\\', \\'B-Position\\', \\'I-Position\\', \\'B-Claim\\', \\'I-Claim\\', \\'B-Counterclaim\\', \\'I-Counterclaim\\', \\n                 \\'B-Rebuttal\\', \\'I-Rebuttal\\', \\'B-Evidence\\', \\'I-Evidence\\', \\'B-Concluding Statement\\', \\'I-Concluding Statement\\']\\nLABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\\nIDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\\n\\nMIN_THRESH = {\\n    \"I-Lead\": 9,\\n    \"I-Position\": 5,\\n    \"I-Evidence\": 14,\\n    \"I-Claim\": 3,\\n    \"I-Concluding Statement\": 11,\\n    \"I-Counterclaim\": 6,\\n    \"I-Rebuttal\": 4,\\n}\\n\\nPROB_THRESH = {\\n    \"I-Lead\": 0.7,\\n    \"I-Position\": 0.55,\\n    \"I-Evidence\": 0.65,\\n    \"I-Claim\": 0.55,\\n    \"I-Concluding Statement\": 0.7,\\n    \"I-Counterclaim\": 0.5,\\n    \"I-Rebuttal\": 0.55,\\n}', 'IGNORE_INDEX': -100, 'NON_LABEL': -1, 'OUTPUT_LABELS': ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement'], 'LABELS_TO_IDS': {'O': 0, 'B-Lead': 1, 'I-Lead': 2, 'B-Position': 3, 'I-Position': 4, 'B-Claim': 5, 'I-Claim': 6, 'B-Counterclaim': 7, 'I-Counterclaim': 8, 'B-Rebuttal': 9, 'I-Rebuttal': 10, 'B-Evidence': 11, 'I-Evidence': 12, 'B-Concluding Statement': 13, 'I-Concluding Statement': 14}, 'IDS_TO_LABELS': {0: 'O', 1: 'B-Lead', 2: 'I-Lead', 3: 'B-Position', 4: 'I-Position', 5: 'B-Claim', 6: 'I-Claim', 7: 'B-Counterclaim', 8: 'I-Counterclaim', 9: 'B-Rebuttal', 10: 'I-Rebuttal', 11: 'B-Evidence', 12: 'I-Evidence', 13: 'B-Concluding Statement', 14: 'I-Concluding Statement'}, 'MIN_THRESH': {'I-Lead': 9, 'I-Position': 5, 'I-Evidence': 14, 'I-Claim': 3, 'I-Concluding Statement': 11, 'I-Counterclaim': 6, 'I-Rebuttal': 4}, 'PROB_THRESH': {'I-Lead': 0.7, 'I-Position': 0.55, 'I-Evidence': 0.65, 'I-Claim': 0.55, 'I-Concluding Statement': 0.7, 'I-Counterclaim': 0.5, 'I-Rebuttal': 0.55}, '_i8': \"def set_seed(seed=HyperParameters.random_seed):\\n    np.random.seed(seed)\\n    \\n    random.seed(seed)\\n    \\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    \\n    torch.backends.cudnn.deterministic =True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed()\\n\\n# Set proper device\\nif torch.cuda.is_available():\\n    device = torch.device('cuda')\\nelse:\\n    device = torch.device('cpu')\\n\\nprint(f'Using device: {device}')\", 'set_seed': <function set_seed at 0x7fb018ec60e0>, 'device': device(type='cuda'), '_i9': \"df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')\", 'df_alltrain':                   id  discourse_id  discourse_start  discourse_end  \\\n",
            "0       423A1CA112E2  1.622628e+12              8.0          229.0   \n",
            "1       423A1CA112E2  1.622628e+12            230.0          312.0   \n",
            "2       423A1CA112E2  1.622628e+12            313.0          401.0   \n",
            "3       423A1CA112E2  1.622628e+12            402.0          758.0   \n",
            "4       423A1CA112E2  1.622628e+12            759.0          886.0   \n",
            "...              ...           ...              ...            ...   \n",
            "144288  4C471936CD75  1.618153e+12           2234.0         3203.0   \n",
            "144289  4C471936CD75  1.618153e+12           3221.0         4509.0   \n",
            "144290  4C471936CD75  1.618025e+12           4510.0         4570.0   \n",
            "144291  4C471936CD75  1.618025e+12           4570.0         4922.0   \n",
            "144292  4C471936CD75  1.618025e+12           4935.0         5825.0   \n",
            "\n",
            "                                           discourse_text  \\\n",
            "0       Modern humans today are always on their phone....   \n",
            "1       They are some really bad consequences when stu...   \n",
            "2       Some certain areas in the United States ban ph...   \n",
            "3       When people have phones, they know about certa...   \n",
            "4       Driving is one of the way how to get around. P...   \n",
            "...                                                   ...   \n",
            "144288   if I'm not sure what college I want to attend...   \n",
            "144289   seeking multiple opinions before making a har...   \n",
            "144290  it is better to seek multiple opinions instead...   \n",
            "144291  The impact of asking people to help you make a...   \n",
            "144292  there are many other reasons one might want to...   \n",
            "\n",
            "              discourse_type      discourse_type_num  \\\n",
            "0                       Lead                  Lead 1   \n",
            "1                   Position              Position 1   \n",
            "2                   Evidence              Evidence 1   \n",
            "3                   Evidence              Evidence 2   \n",
            "4                      Claim                 Claim 1   \n",
            "...                      ...                     ...   \n",
            "144288              Evidence              Evidence 2   \n",
            "144289              Evidence              Evidence 3   \n",
            "144290              Position              Position 1   \n",
            "144291              Evidence              Evidence 4   \n",
            "144292  Concluding Statement  Concluding Statement 1   \n",
            "\n",
            "                                         predictionstring  \\\n",
            "0       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
            "1            45 46 47 48 49 50 51 52 53 54 55 56 57 58 59   \n",
            "2         60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75   \n",
            "3       76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...   \n",
            "4       139 140 141 142 143 144 145 146 147 148 149 15...   \n",
            "...                                                   ...   \n",
            "144288  386 387 388 389 390 391 392 393 394 395 396 39...   \n",
            "144289  576 577 578 579 580 581 582 583 584 585 586 58...   \n",
            "144290        828 829 830 831 832 833 834 835 836 837 838   \n",
            "144291  839 840 841 842 843 844 845 846 847 848 849 85...   \n",
            "144292  905 906 907 908 909 910 911 912 913 914 915 91...   \n",
            "\n",
            "                                            text_by_index  new_start  new_end  \\\n",
            "0       Modern humans today are always on their phone....          8      230   \n",
            "1       They are some really bad consequences when stu...        230      313   \n",
            "2       Some certain areas in the United States ban ph...        313      401   \n",
            "3       When people have phones, they know about certa...        402      758   \n",
            "4       Driving is one of the way how to get around. P...        759      887   \n",
            "...                                                   ...        ...      ...   \n",
            "144288   if I'm not sure what college I want to attend...       2235     3203   \n",
            "144289   seeking multiple opinions before making a har...       3222     4510   \n",
            "144290  it is better to seek multiple opinions instead...       4510     4570   \n",
            "144291  The impact of asking people to help you make a...       4570     4923   \n",
            "144292  there are many other reasons one might want to...       4935     5748   \n",
            "\n",
            "                                        text_by_new_index  \\\n",
            "0       Modern humans today are always on their phone....   \n",
            "1       They are some really bad consequences when stu...   \n",
            "2       Some certain areas in the United States ban ph...   \n",
            "3       When people have phones, they know about certa...   \n",
            "4       Driving is one of the way how to get around. P...   \n",
            "...                                                   ...   \n",
            "144288  if I'm not sure what college I want to attend,...   \n",
            "144289  seeking multiple opinions before making a hard...   \n",
            "144290  it is better to seek multiple opinions instead...   \n",
            "144291  The impact of asking people to help you make a...   \n",
            "144292  there are many other reasons one might want to...   \n",
            "\n",
            "                                     new_predictionstring  \n",
            "0       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
            "1            45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
            "2         60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
            "3       76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
            "4       139 140 141 142 143 144 145 146 147 148 149 15...  \n",
            "...                                                   ...  \n",
            "144288  386 387 388 389 390 391 392 393 394 395 396 39...  \n",
            "144289  576 577 578 579 580 581 582 583 584 585 586 58...  \n",
            "144290        828 829 830 831 832 833 834 835 836 837 838  \n",
            "144291  839 840 841 842 843 844 845 846 847 848 849 85...  \n",
            "144292  905 906 907 908 909 910 911 912 913 914 915 91...  \n",
            "\n",
            "[144293 rows x 13 columns], '_i10': 'def agg_essays(train_flg):\\n    \"\"\"\\n    Splits every word in an essay and adds the text of each essay to a dataframe.\\n    \"\"\"\\n    folder = \\'train\\' if train_flg else \\'test\\'\\n    names, texts =[], []\\n    for f in tqdm(list(os.listdir(f\\'{HyperParameters.data_dir}/{folder}\\'))):\\n        names.append(f.replace(\\'.txt\\', \\'\\'))\\n        texts.append(open(f\\'{HyperParameters.data_dir}/{folder}/\\' + f, \\'r\\').read())\\n        df_texts = pd.DataFrame({\\'id\\': names, \\'text\\': texts})\\n\\n    df_texts[\\'text_split\\'] = df_texts.text.str.split()\\n    print(\\'Completed tokenizing texts.\\')\\n    return df_texts', 'agg_essays': <function agg_essays at 0x7fb018ea8440>, '_i11': 'def ner(df_texts, df_train):\\n    \"\"\"\\n    Maps discourse type to each word of the text, according to the train.csv file.\\n    \"\"\"\\n    all_entities = []\\n    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\\n        total = len(row[\\'text_split\\'])\\n        entities = [\\'O\\'] * total\\n\\n        for _, row2 in df_train[df_train[\\'id\\'] == row[\\'id\\']].iterrows():\\n            discourse = row2[\\'discourse_type\\']\\n            list_ix = [int(x) for x in row2[\\'predictionstring\\'].split(\\' \\')]\\n            entities[list_ix[0]] = f\\'B-{discourse}\\'\\n            for k in list_ix[1:]: entities[k] = f\\'I-{discourse}\\'\\n        all_entities.append(entities)\\n\\n    df_texts[\\'entities\\'] = all_entities\\n    print(\\'Completed mapping discourse to each token.\\')\\n    return df_texts', 'ner': <function ner at 0x7fb010272200>, '_i12': 'def preprocess(df_train = None):\\n    \"\"\"\\n    Generates the dataframe we will use for training.\\n    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\\n    \"\"\"\\n    if df_train is None:\\n        train_flg = False\\n    else:\\n        train_flg = True\\n    \\n    df_texts = agg_essays(train_flg)\\n\\n    if train_flg:\\n        df_texts = ner(df_texts, df_train)\\n    return df_texts\\n\\n# Make sure we only run pre-processing if we did not do it in the past:\\n\\nif not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    alltrain_texts = preprocess(df_alltrain)\\n    test_texts = preprocess()\\nelse:\\n    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")', 'preprocess': <function preprocess at 0x7fb010272320>, 'alltrain_texts':                  id                                               text  \\\n",
            "0      3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1      DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2      2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3      EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4      A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "...             ...                                                ...   \n",
            "15589  1C899F124FEB  While some students may think it's a beneficia...   \n",
            "15590  4453444AF383  There has been a strong arguement going on wea...   \n",
            "15591  EF0D75BF48DA  I favor in to changing election by popular vot...   \n",
            "15592  8FFDA5B9D359  Do you think students would benefit from being...   \n",
            "15593  ACAB1FCA0A30  I would like to change the election for the pr...   \n",
            "\n",
            "                                              text_split  \\\n",
            "0      [I, do, agree, that, some, students, would, be...   \n",
            "1      [Should, students, design, a, summer, project,...   \n",
            "2      [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3      [People, sometimes, have, a, different, opinio...   \n",
            "4      [Dear, senator,, As, you, know, the, Electoral...   \n",
            "...                                                  ...   \n",
            "15589  [While, some, students, may, think, it's, a, b...   \n",
            "15590  [There, has, been, a, strong, arguement, going...   \n",
            "15591  [I, favor, in, to, changing, election, by, pop...   \n",
            "15592  [Do, you, think, students, would, benefit, fro...   \n",
            "15593  [I, would, like, to, change, the, election, fo...   \n",
            "\n",
            "                                                entities  kfold  \n",
            "0      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1      [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2      [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4      [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  \n",
            "...                                                  ...    ...  \n",
            "15589  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "15590  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "15591  [B-Position, I-Position, I-Position, I-Positio...      3  \n",
            "15592  [B-Position, I-Position, I-Position, I-Positio...      2  \n",
            "15593  [B-Position, I-Position, I-Position, I-Positio...      4  \n",
            "\n",
            "[15594 rows x 5 columns], '_i13': \"# Visualize preprocessing result:\\nparse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\\nalltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\\nalltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\\n\\nalltrain_texts.head()\", 'parse_string': <function <lambda> at 0x7fb010272830>, '_13':              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  , '_i14': 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \\n    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\\n\\n    # Generate name for the dummy columns\\n    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\\n    # dfx is now only the dataset with dummy columns selected: don\\'t need to pass the data to do the splits\\n    dfx = dfx[dummy_cols]', '_i15': 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Generate cross validation object\\n    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n    # Extract labels\\n    labels = [c for c in dfx.columns if c != \"id\"]\\n    dfx_labels = dfx[labels]\\n\\n    # Dummy kfold assignment\\n    dfx[\"kfold\"] = -1\\n\\n    # Split\\n    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\\n        print(len(trn_), len(val_))\\n        \\n        # Change the value of the kfold column at the validation index to the value of the fold\\n        # This will tell us when to use the current entry in the validation set\\n        dfx.loc[val_, \"kfold\"] = fold\\n\\n    # merge back to original dataframe\\n    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\\n    print(alltrain_texts.kfold.value_counts())\\n\\n    # Save so next time we import it directly\\n    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)', '_i16': \"# need help with this\\nclass FeedbackPrizeDataset(Dataset):\\n    def __init__(self, dataframe, tokenizer, max_len, has_labels):\\n        self.len = len(dataframe)\\n        self.data = dataframe\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.has_labels = has_labels\\n    \\n    def __getitem__(self, index):\\n        text = self.data.text[index]\\n        encoding = self.tokenizer(\\n            text.split(),\\n            is_split_into_words = True,\\n            padding = 'max_length',\\n            truncation = True,\\n            max_length = self.max_len\\n        )\\n        word_ids = encoding.word_ids()\\n\\n        # targets\\n        if self.has_labels:\\n            word_labels = self.data.entities[index]\\n            prev_word_idx = None\\n            labels_ids = []\\n            for word_idx in word_ids:\\n                if word_idx is None:\\n                    labels_ids.append(IGNORE_INDEX)\\n                elif word_idx != prev_word_idx:\\n                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                else:\\n                    if HyperParameters.label_subtokens:\\n                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                    else:\\n                        labels_ids.append(IGNORE_INDEX)\\n                prev_word_idx = word_idx\\n            encoding['labels'] = labels_ids\\n        # convert to torch.tensor\\n        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\\n        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\\n        item['word_ids'] = torch.as_tensor(word_ids2)\\n        return item\\n\\n    def __len__(self):\\n        return self.len\", 'FeedbackPrizeDataset': <class '__main__.FeedbackPrizeDataset'>, '_i17': \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'FeedbackModel': <class '__main__.FeedbackModel'>, '_i18': 'def build_model_tokenizer():\\n    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\\n    model = FeedbackModel()\\n    return model, tokenizer', 'build_model_tokenizer': <function build_model_tokenizer at 0x7fafce3013b0>, '_i19': '# Need help with this: used in training to transform raw logits to labels needed\\ndef active_logits(raw_logits, word_ids):\\n    word_ids = word_ids.view(-1)\\n    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\\n    active_mask = active_mask != NON_LABEL\\n    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\\n    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\\n    active_logits = active_logits.view(-1, HyperParameters.num_labels) \\n    return active_logits\\n\\ndef active_labels(labels):\\n    active_mask = labels.view(-1) != IGNORE_INDEX\\n    active_labels = torch.masked_select(labels.view(-1), active_mask)\\n    return active_labels\\n\\ndef active_preds_prob(active_logits):\\n    active_preds = torch.argmax(active_logits, axis = 1)\\n    active_preds_prob, _ = torch.max(active_logits, axis = 1)\\n    return active_preds, active_preds_prob', 'active_logits': <function active_logits at 0x7fafce301b00>, 'active_labels': <function active_labels at 0x7fafce3017a0>, 'active_preds_prob': <function active_preds_prob at 0x7fafce301560>, '_i20': 'def calculate_overlap(set_pred, set_gt):\\n    \"\"\"\\n    Calculates if the overlap between prediction and\\n    ground truth is enough fora potential True positive\\n    \"\"\"\\n    # Length of each and intersection\\n    try:\\n        len_gt = len(set_gt)\\n        len_pred = len(set_pred)\\n        inter = len(set_gt & set_pred)\\n        overlap_1 = inter / len_gt\\n        overlap_2 = inter/ len_pred\\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\\n    except:  # at least one of the input is NaN\\n        return False\\n\\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\\n    \"\"\"\\n    A function that scores for the kaggle\\n        Student Writing Competition\\n        \\n    Uses the steps in the evaluation page here:\\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\\n    \"\"\"\\n    gt_df = gt_df.loc[gt_df[\\'discourse_type\\'] == discourse_type, \\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df = pred_df.loc[pred_df[\\'class\\'] == discourse_type,\\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df[\\'pred_id\\'] = pred_df.index\\n    gt_df[\\'gt_id\\'] = gt_df.index\\n    pred_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in pred_df[\\'predictionstring\\']]\\n    gt_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in gt_df[\\'predictionstring\\']]\\n    \\n    # Step 1. all ground truths and predictions for a given class are compared.\\n    joined = pred_df.merge(gt_df,\\n                           left_on=\\'id\\',\\n                           right_on=\\'id\\',\\n                           how=\\'outer\\',\\n                           suffixes=(\\'_pred\\',\\'_gt\\')\\n                          )\\n    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \\n                                                     joined.predictionstring_gt)]\\n    \\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \\n    # and the overlap between the prediction and the ground truth >= 0.5,\\n    # the prediction is a match and considered a true positive.\\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\\n    # we don\\'t need to compute the match to compute the score\\n    TP = joined.loc[overlaps][\\'gt_id\\'].nunique()\\n\\n    # 3. Any unmatched ground truths are false negatives\\n    # and any unmatched predictions are false positives.\\n    TPandFP = len(pred_df)\\n    TPandFN = len(gt_df)\\n    \\n    #calc microf1\\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\\n    return my_f1_score\\n\\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\\n    \"\"\"\\n    Final helper function for model evaluation.\\n    \\n    Args:\\n    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: [\\'id\\',\\'class\\',\\'predictionstring\\']\\n    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\\n    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\\n    \\n    Returns:\\n    f1                      (float): F1 score of the model\\n    (optional) class_scores  (dict): Dictionary of per-class F1 score\\n    \"\"\"\\n    class_scores = {}\\n    for discourse_type in gt_df.discourse_type.unique():\\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\\n        class_scores[discourse_type] = class_score\\n    f1 = np.mean([v for v in class_scores.values()])\\n    if return_class_scores:\\n        return f1, class_scores\\n    return f1', 'calculate_overlap': <function calculate_overlap at 0x7fb018ec6d40>, 'score_feedback_comp_micro': <function score_feedback_comp_micro at 0x7fb018ec6cb0>, 'score_feedback_comp': <function score_feedback_comp at 0x7fb018ea84d0>, '_i21': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n            \\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'train_fn': <function train_fn at 0x7faf5aac9950>, '_i22': 'def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\\n    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\\n    f1score =[]\\n    # classes = oof[\\'class\\'].unique()\\n    classes = [\\'Lead\\', \\'Position\\', \\'Claim\\',\\'Counterclaim\\', \\'Rebuttal\\',\\'Evidence\\',\\'Concluding Statement\\']\\n    print(f\"Validation F1 scores\")\\n\\n    for c in classes:\\n        pred_df = oof.loc[oof[\\'class\\'] == c].copy()\\n        gt_df = df_val_eval.loc[df_val_eval[\\'discourse_type\\'] == c].copy()\\n        f1 = score_feedback_comp(pred_df, gt_df)\\n        print(f\\' * {c:<10}: {f1:4f}\\')\\n        f1score.append(f1)\\n    f1avg = np.mean(f1score)\\n    print(f\\'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}\\')\\n    return valid_loss, oof', 'valid_fn': <function valid_fn at 0x7fafce310a70>, '_i23': 'def inference(model, data_loader, criterion, valid_flg):\\n    stream = tqdm(data_loader)\\n    model.eval()\\n    \\n    valid_loss = 0\\n    valid_accuracy = 0\\n    all_logits = None\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch[\\'input_ids\\'].to(device, dtype = torch.long)\\n        mask = batch[\\'attention_mask\\'].to(device, dtype = torch.long)\\n        with torch.no_grad():\\n            raw_logits = model(input_ids=ids, mask = mask)\\n        del ids, mask\\n        \\n        word_ids = batch[\\'word_ids\\'].to(device, dtype = torch.long)\\n        logits = active_logits(raw_logits, word_ids)\\n        sf_logits = torch.softmax(logits, dim= -1)\\n        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\\n        if valid_flg:    \\n            raw_labels = batch[\\'labels\\'].to(device, dtype = torch.long)\\n            labels = active_labels(raw_labels)\\n            preds, preds_prob = active_preds_prob(sf_logits)\\n            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n            loss = criterion(logits, labels)\\n            valid_loss += loss.item()\\n        \\n        if batch_idx == 1:\\n            all_logits = sf_raw_logits.cpu().numpy()\\n        else:\\n            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\\n\\n    \\n    if valid_flg:        \\n        epoch_loss = valid_loss / batch_idx\\n        epoch_accuracy = valid_accuracy / batch_idx\\n    else:\\n        epoch_loss, epoch_accuracy = 0, 0\\n    return all_logits, epoch_loss, epoch_accuracy\\n\\n\\ndef preds_class_prob(all_logits, data_loader):\\n    print(\"predict target class and its probabilty\")\\n    final_predictions = []\\n    final_predictions_score = []\\n    stream = tqdm(data_loader)\\n    len_sample = all_logits.shape[0]\\n\\n    for batch_idx, batch in enumerate(stream, start=0):\\n        for minibatch_idx in range(HyperParameters.valid_batch_size):\\n            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\\n            if sample_idx > len_sample - 1 : break\\n            word_ids = batch[\\'word_ids\\'][minibatch_idx].numpy()\\n            predictions =[]\\n            predictions_prob = []\\n            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\\n            pred_score = np.max(all_logits[sample_idx], axis=1)\\n            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\\n            prev_word_idx = -1\\n            for idx, word_idx in enumerate(word_ids):\\n                if word_idx == -1:\\n                    pass\\n                elif word_idx != prev_word_idx:\\n                    predictions.append(pred_class_labels[idx])\\n                    predictions_prob.append(pred_score[idx])\\n                    prev_word_idx = word_idx\\n            final_predictions.append(predictions)\\n            final_predictions_score.append(predictions_prob)\\n    return final_predictions, final_predictions_score', 'inference': <function inference at 0x7fb010251d40>, 'preds_class_prob': <function preds_class_prob at 0x7fb010251dd0>, '_i24': 'def get_preds_onefold(model, df, dl, criterion, valid_flg):\\n    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n    all_preds, all_preds_prob = preds_class_prob(logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred, valid_loss, valid_acc\\n\\ndef get_preds_folds(model, df, dl, criterion, valid_flg=False):\\n    for i_fold in range(HyperParameters.n_fold):\\n        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\\n        print(f\"{model_filename} inference\")\\n        model = model.to(device)\\n        model.load_state_dict(torch.load(model_filename))\\n        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n        if i_fold == 0:\\n            avg_pred_logits = logits\\n        else:\\n            avg_pred_logits += logits\\n    avg_pred_logits /= HyperParameters.n_fold\\n    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred\\n\\ndef post_process_pred(df, all_preds, all_preds_prob):\\n    final_preds = []\\n    for i in range(len(df)):\\n        idx = df.id.values[i]\\n        pred = all_preds[i]\\n        pred_prob = all_preds_prob[i]\\n        j = 0\\n        while j < len(pred):\\n            cls = pred[j]\\n            if cls == \\'O\\': j += 1\\n            else: cls = cls.replace(\\'B\\', \\'I\\')\\n            end = j + 1\\n            while end < len(pred) and pred[end] == cls:\\n                end += 1\\n            if cls != \\'O\\' and cls !=\\'\\':\\n                avg_score = np.mean(pred_prob[j:end])\\n                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\\n                    final_preds.append((idx, cls.replace(\\'I-\\', \\'\\'), \\' \\'.join(map(str, list(range(j, end))))))\\n            j = end\\n    df_pred = pd.DataFrame(final_preds)\\n    df_pred.columns = [\\'id\\', \\'class\\', \\'new_predictionstring\\']\\n    return df_pred', 'get_preds_onefold': <function get_preds_onefold at 0x7fafce32a0e0>, 'get_preds_folds': <function get_preds_folds at 0x7fafce32a200>, 'post_process_pred': <function post_process_pred at 0x7fafce32a4d0>, '_i25': 'print_gpu_utilization()', '_i26': 'def pretty_size(size):\\n\\t\"\"\"Pretty prints a torch.Size object\"\"\"\\n\\tassert(isinstance(size, torch.Size))\\n\\treturn \" × \".join(map(str, size))\\n\\ndef dump_tensors(gpu_only=True):\\n\\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\\n\\timport gc\\n\\ttotal_size = 0\\n\\tfor obj in gc.get_objects():\\n\\t\\ttry:\\n\\t\\t\\tif torch.is_tensor(obj):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" pinned\" if obj.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  pretty_size(obj.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.numel()\\n\\t\\t\\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   type(obj.data).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" pinned\" if obj.data.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" grad\" if obj.requires_grad else \"\", \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" volatile\" if obj.volatile else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   pretty_size(obj.data.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.data.numel()\\n\\t\\texcept Exception as e:\\n\\t\\t\\tpass        \\n\\tprint(\"Total size:\", total_size)', 'pretty_size': <function pretty_size at 0x7fafce32acb0>, 'dump_tensors': <function dump_tensors at 0x7fafce32ad40>, '_i27': 'dump_tensors()', '_i28': 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'oof': Empty DataFrame\n",
            "Columns: []\n",
            "Index: [], 'i_fold': 0, 'tokenizer': PreTrainedTokenizerFast(name_or_path='allenai/longformer-base-4096', vocab_size=50265, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}), 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 4e-05\n",
            "    weight_decay: 0\n",
            "), 'df_train':                  id                                               text  \\\n",
            "0      3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1      DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2      EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "3      A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "4      616F8E0EFABF  \"Can you imagine a time in the future when no ...   \n",
            "...             ...                                                ...   \n",
            "12472  1C899F124FEB  While some students may think it's a beneficia...   \n",
            "12473  4453444AF383  There has been a strong arguement going on wea...   \n",
            "12474  EF0D75BF48DA  I favor in to changing election by popular vot...   \n",
            "12475  8FFDA5B9D359  Do you think students would benefit from being...   \n",
            "12476  ACAB1FCA0A30  I would like to change the election for the pr...   \n",
            "\n",
            "                                              text_split  \\\n",
            "0      [I, do, agree, that, some, students, would, be...   \n",
            "1      [Should, students, design, a, summer, project,...   \n",
            "2      [People, sometimes, have, a, different, opinio...   \n",
            "3      [Dear, senator,, As, you, know, the, Electoral...   \n",
            "4      [\"Can, you, imagine, a, time, in, the, future,...   \n",
            "...                                                  ...   \n",
            "12472  [While, some, students, may, think, it's, a, b...   \n",
            "12473  [There, has, been, a, strong, arguement, going...   \n",
            "12474  [I, favor, in, to, changing, election, by, pop...   \n",
            "12475  [Do, you, think, students, would, benefit, fro...   \n",
            "12476  [I, would, like, to, change, the, election, fo...   \n",
            "\n",
            "                                                entities  kfold  \\\n",
            "0      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2   \n",
            "1      [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4   \n",
            "2      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3   \n",
            "3      [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1   \n",
            "4      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      1   \n",
            "...                                                  ...    ...   \n",
            "12472  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3   \n",
            "12473  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3   \n",
            "12474  [B-Position, I-Position, I-Position, I-Positio...      3   \n",
            "12475  [B-Position, I-Position, I-Position, I-Positio...      2   \n",
            "12476  [B-Position, I-Position, I-Position, I-Positio...      4   \n",
            "\n",
            "                                                  labels  \n",
            "0      [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "1      [0, 0, 0, 0, 0, 0, 0, 0, 2, 9, 9, 9, 9, 9, 9, ...  \n",
            "2      [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "3      [0, 0, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "4      [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "...                                                  ...  \n",
            "12472  [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "12473  [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "12474  [2, 9, 9, 9, 9, 9, 9, 9, 9, 4, 11, 11, 11, 11,...  \n",
            "12475  [2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...  \n",
            "12476  [2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...  \n",
            "\n",
            "[12477 rows x 6 columns], 'ds_train': <__main__.FeedbackPrizeDataset object at 0x7faf5a68b510>, 'df_val':                 id                                               text  \\\n",
            "0     2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "1     F9368F6BAB62  You should join the Seagoing Cowboys program. ...   \n",
            "2     617D56A15483  Do you want to go on daily trips and help peop...   \n",
            "3     D4AA8EC389C6  School Grades:\\n\\nI don't think that it is fai...   \n",
            "4     D92FAEE355E1  Dear Mrs. Principal:\\n\\nI think this new schoo...   \n",
            "...            ...                                                ...   \n",
            "3112  3BEFA845FC27  There are people all around the world that mak...   \n",
            "3113  F166DA36A4A3  In this essay im going to prove to you why tha...   \n",
            "3114  8F33151F0334  He joined the programe for many reasons to hel...   \n",
            "3115  DDA2A181F486  I'm against this technology that reads your em...   \n",
            "3116  ACCD71550365  Do you think it is a good idea to succeed? I b...   \n",
            "\n",
            "                                             text_split  \\\n",
            "0     [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "1     [You, should, join, the, Seagoing, Cowboys, pr...   \n",
            "2     [Do, you, want, to, go, on, daily, trips, and,...   \n",
            "3     [School, Grades:, I, don't, think, that, it, i...   \n",
            "4     [Dear, Mrs., Principal:, I, think, this, new, ...   \n",
            "...                                                 ...   \n",
            "3112  [There, are, people, all, around, the, world, ...   \n",
            "3113  [In, this, essay, im, going, to, prove, to, yo...   \n",
            "3114  [He, joined, the, programe, for, many, reasons...   \n",
            "3115  [I'm, against, this, technology, that, reads, ...   \n",
            "3116  [Do, you, think, it, is, a, good, idea, to, su...   \n",
            "\n",
            "                                               entities  kfold  \n",
            "0     [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "1     [B-Position, I-Position, I-Position, I-Positio...      0  \n",
            "2     [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "3     [O, O, B-Position, I-Position, I-Position, I-P...      0  \n",
            "4     [O, O, O, B-Position, I-Position, I-Position, ...      0  \n",
            "...                                                 ...    ...  \n",
            "3112  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "3113  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "3114  [B-Evidence, I-Evidence, I-Evidence, I-Evidenc...      0  \n",
            "3115  [B-Position, I-Position, I-Position, I-Positio...      0  \n",
            "3116  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "\n",
            "[3117 rows x 5 columns], 'val_idlist': ['2E4AFCD3987F', 'F9368F6BAB62', '617D56A15483', 'D4AA8EC389C6', 'D92FAEE355E1', '58EE193B3F7D', '4A2693C68175', '3741D275F8BC', 'FC9E2A38AFBB', '8B5CB2AA72E9', '2DEC8D3BA891', '217A7429341D', '7E558B4E7009', 'CC38587E9634', '8DD596F5F5B5', '632C8DA9E2B1', '76C8FE4FB3D8', 'D7D66E258830', 'C20011D6FA9D', '508DB90F68C7', '1655FBD1D22A', '0DE6D016C7FC', '1D07A65F9B7F', '0257D9E42247', '31DB1BDE89A1', '13FB62D377E3', '33C081EC2A59', '90691C2B3D5A', '6D6215C23989', '7FB898313EA9', 'BCAE5B0783CF', 'C5C81327D129', '5942FB27D5C3', '185FD725317F', 'ACBC86CD65B4', '027616F7ED1D', '6068A54ED99D', 'B75E90DC14B3', 'BA545022DFCD', '2D654BBA9310', 'F53C55BBD326', '0821EB1AEDED', '7F74864D0865', 'DCDBC5C7873F', 'EA9FA5DA65F3', '486B53D5F8A0', '1CCEB6CD629D', '1F8DDCFCA766', 'DBCC1F8F92B5', '51E211998DF0', 'D447CD5A0C67', 'C50FE04C4D29', '32B2D5592E1D', '2962D1BB17A2', '59E8C8BE4ACA', '155F508445E2', 'C3F35CEC4B5A', 'B5113469D13C', 'E09064F496DF', 'E5EF52219F63', 'C43C4EE10535', '2DFFB145BE07', '2F3210D23414', '084DC646D8E3', '2B002FEE337E', '8DB4751C05A1', '8C7464101130', '3752D33FDE11', 'F3CB6601CF24', 'FE1DEACABE37', 'BF0241C10324', '8F91417B8A7D', '9DBB8B0C3367', '69891F080D3A', 'EE63F23F10B3', 'D7C33D289E26', '325B922888A7', '3A08788DCA7D', 'CFA4E76C506A', 'B3C2E6274311', '7F77FE73B338', 'E1130AA34EE8', 'CFB99F0D0D31', '3712A5E56380', '1B1FBDE25C2C', '6D6B05E5478D', 'D1673363C56A', '1BA91BC3D89A', 'D5724EDF0DBA', '5FFC25FE5E00', '6ED00F0E1BFE', '4C0EB60473E1', 'F605023B87A7', '2617310033A4', 'D284AB2165C8', '29B80B7D1092', 'EA205A51D32C', '5137514F130A', '0C0A8EBDC931', 'BF5D82D486F5', '4A8AC603C9C5', '3188A9E2D162', '92887B4B5F2A', 'AE844BA96314', '21A162D4B0CB', '4390794B2751', '46E97FDCA469', 'B5838ACCDBA5', '318B9B177D68', '41A159819E12', '29BE597866EE', '9396FCEB118A', '48FB3005F4DC', '8134AD7791FC', '8E938A38BA43', '220A26E7E802', '04ACF5873DE9', 'ED4186119E60', '1CC9A9598942', '9A5D8C898877', 'D6466B4502D5', 'B60A2DC4B93C', '7ECB7FDA805A', '3AFA78FFFC15', '611228818239', '619D1D7FDC04', '85255F76A261', '94CCB4234EAE', '971D0C1C75F0', '710A2280E20F', '7B64BC831CD9', 'C9DA3BE5EB24', '479F6A965C8C', 'D24777F39289', 'D7946F0C2A83', '65590D44B68A', '2B4A794ED0D8', '242E9BA55C0C', 'E3F3C192B3A2', '61671FCB6624', '3F8C4D2CA16E', '2E9B5CD4716D', 'D022656B1A36', 'D5CB4FD63313', 'D66B89FC3757', 'D7401AE3A016', 'AC7C84C2A13E', '727DE3CD0AEC', '7C1FF9F7C5C8', '2A7C4A1CBA41', 'BA18C6F80423', '23C6ED53A179', 'C05809232F0E', 'C8D3EB90EB29', '4AD3DB40F5D3', '46284DF2F31C', 'AF34B1CDD152', 'A6CC10322C3A', '0DBE82E9933F', 'F5A0BD46D3AF', '3CA61DE9B5C6', '899E6252BD6F', 'EA5E9342C547', '89CD5C5D9180', 'FE47325AD93F', '6CD4FAAB7653', 'F503C7DC334F', '870D9B35F20A', '37FC9DB2D1DB', '8C9A286F5351', '7AD4CB4029C0', 'BAEE45819FA1', '7BBC565EFE33', 'F44F49940938', 'E7576760695E', '116FB053BEC7', '5EF4174A5264', 'B651AEF954C3', '06E3595C94BF', '0054850878E3', '7091F3705B94', 'FC90D46FBE1F', 'DE1EF5A9F3AA', 'E8FF0D2483FE', 'EF2D45E53F40', '75A613E09762', 'DA2D885A25C8', '5666D5642C98', '563122BF4796', 'A4035C4D846D', 'B0007528CB4A', 'B3526897360F', '2907A97CC7C4', '9B69DE203555', '387A6ABAC720', '3713AC622BFF', 'CEB3F5203744', '4B54BE9BD269', '11B942BD5A87', '3C9B9DAFB0AE', 'E05800F81426', '548A0120A1CA', 'AA48C183006F', '68A83805444F', 'BFAA8D19FFA9', '970A1848B187', 'AB5069821C4E', '27E6536433B9', 'F806675B9B70', 'A486E99E277D', 'A8445CABFECE', '14D6911BF51D', '5613F9FB2154', '46F64048E5F5', '6E80360E58BF', '1D459BD1F763', '72B22E89D0EE', '29AB81B05092', '50C6B70DF247', '6D616AB3E2A7', '958E3ACAC90C', '1F44A4A679C2', '453D762EF267', '1F539B2D1ED3', 'F32A84A0E29C', 'E7A45AEC5AE8', '2CF5EC4E36F2', 'C3C13F0C0274', '548A58E068EB', 'EAF72F67654F', 'DD627EE5CB27', '897438742BCC', 'BC32404D0719', '1693FF58E7B9', 'CA4E39042770', 'E10E42DFDA2B', 'B8CA9E3E3100', '0FE128A98388', 'DF2BE9E97393', '9E3359EBC812', '082598CC8955', '92E1CFD27E95', '095B59682071', '716543D2EA47', '371834E7E1DD', 'ECD0248B4146', '995330686089', '672D0F5241A4', 'CCD30CE1E903', '07E4962801FC', 'D88A1D12032B', '712A59B74C91', '7BC73C8FBDBC', '7A143C2BC7F9', '1B2470F552CC', '9C97C5A04635', '57AB2EB3B7B6', '6ED825F9F077', 'FA5CE3A393CF', 'C83EB6623D24', 'A5A55FE930C4', '58211510AB4F', '11CE345BAEF2', 'CC9290DC336A', '420557904E93', '6A088E8BBE02', 'CDBE82430B57', 'E249399CA04E', 'F711AD68730E', '0802AEA16E9F', '44735791FDA0', '9F7100885801', 'C02E687A4BB9', '51A5A4C583D2', '8A033A8BE5E7', '2F7B32094BCD', '65922C6DA1B8', '19C2D7C5171D', 'C5A56B482FAA', 'FAB02B654C4E', 'C1A275127D98', 'BF3CC3305C55', 'F2A79A33B020', '95BCD0EEE72B', 'F24FA9D60071', 'C5F1272FF970', 'D0A54C1D411F', '6DF747F4A277', '58E84BCE4B90', 'C9896C21A7AB', '91962F622C5A', 'BC2CAB7DB6F0', 'D9C9F849DC3E', 'A7B69F38FB55', '8D73796D52E9', 'AED0E44B59CE', 'CA5DB05415A5', '4478B3244F49', '1313ADFE62C6', 'B5FF33681C22', 'B437EB073109', '1EB5D99BF3CC', 'D087A8C24570', 'E02E7B3C880F', '3BF2F496AF76', 'BA655E4F7018', '954486CDA6EA', 'ABEB82F77B84', 'F6CFA4C7247B', 'E90A6BA3F6E8', '454FA9C8E4C0', '0C897BA8D8C1', '1010F320FABB', '21CB2FC1D56F', '7C45A871BF88', 'C70EE5903373', '1AD208F3F94A', '1EF99E72B0AE', 'C5E15819B7D6', 'A4E6E2D5658D', '2FC2AD6AF52E', '80DD6E906303', '69FABB805AB2', '5A6FAD4F6359', '124F5E51002C', 'A975CBC4A7E8', '262A455A7060', '250A3F61B6D8', '0D11CD49499E', '476E07AEA488', '8A3F92425ECC', '4B30291A725D', 'C9AC065BB597', '7A28AB016AA2', '85F85936F1E6', '1669F1CC90D9', '84B1E0E85878', 'F3593D9B1E95', '31467F92ED26', 'FF945B27A8F2', '2B557F0EF764', '862FEAF83C5B', '90ECCF04A979', '47C517E48BA4', '85B3EE483BEF', 'E6C2FD3578B3', '58567312BDAB', '464B19E7FBFF', 'FF23EEEB0945', '8EBD6FAE257F', '086603D5B319', 'E2B2ABB01662', '527D2EC10B3C', '2E6230521AC4', 'C0D0428510D5', '0DFAF28500A2', 'EB8B27A81300', '5986652F6023', 'ECE6DB354262', '58E92DF92D8C', 'E6337FDAD05A', '11B9AC1814C8', 'AAA152F374E0', 'E11931B2A6AC', 'DCE199D98B3D', '5B8AD3907163', '94927A32B286', '3171BAF9F164', 'E8D3A695C074', 'FF01B32BAC3E', '575829738D7F', '276048EF9F9A', '0DF3E1BC7D04', '6E381D4CCEDA', '429F4E63967A', 'EB4A3811B1AF', '36EB823B246D', '74B57E07F63D', '5E879A28D710', '0416D5BC09FB', 'E59DD4FF67E8', '309272016742', '9C6DAB936B71', '4DBDC14F77BA', '028A2B1C73AF', 'F497369C41C2', '572236CAAFCE', 'ACEDBB3C81A0', 'D228AB564BF6', '6851B722B3E0', '428FD33185B5', '326EE5515F2D', '6ECD533A72F4', '9D42FF4BDA82', '00EE23F071DE', 'C9C61B8A1FEC', 'EFE800DEFA4B', 'A8D52D24BF51', 'F8706CB01394', 'E5909247F839', '8555ED31A266', '40936430B82D', '42DC0DED6A11', 'DDA8E22052DD', '757E9E1595A7', '5CE34C643883', '053DA7D0417B', '248BA02C6B96', 'B948C72229B3', '43B8AFB8396B', 'F3EE1510C8EF', '855398650C01', '180159E53733', 'BF7DA06D503F', '9A8B992CE622', 'CCF236C2E533', 'C3C4D847175E', '0316F6288791', 'D972C6918584', '1D8D7B32B98A', '8E6CB58E9762', 'B4D91520B347', 'C737906554AE', '171E655CC8F1', 'DBC53691CD0A', '438ABF2E294B', '9C49F198557D', '80619550D955', '1006FE578078', 'D7D83D1EBFDB', 'E079C8DE0D1E', '2D36D77E0BF4', '207D7CEADF38', '07A023BE2629', '6FCD57CA5E41', '710928FFC6BC', '2B7612A3D793', '1FF01ED707B6', 'F497B058DEB9', '814488D5C66D', '9F7080A8AC9E', '9400D174DEC9', '43F92C43EB64', '03F55023E993', '9CE836599EEE', 'D1D38775CCFC', '0F3F6283B13B', '1CEDD5563788', 'BF17866F7014', 'BEE54D4D2B79', 'BA71F335957F', '058BD5D122D1', '28220DC55DA8', 'A535F9081747', 'CAAF7FF6EA89', 'DF764CB197A7', 'A6B380D5DEBB', '925C7FFD12C9', '850401568E9C', '81587E82D64B', 'F0B46DACB512', '15D5005F0ABE', 'C57E6EAB037F', '1111156C5EB8', '197A2E0615B2', '04ED5F75647F', '466945C86DFD', 'EAE3F8725A23', '4722B4D2C6E7', '7A1940992606', '3C26EC679114', 'AE425A221021', 'EE26DF6F6A1F', '4AFD5F644AB8', '2B48EAC02BF3', '3C2FB67A96CB', 'D4C789D85451', 'E15ECDD9A660', '3899C281559F', 'F280DC36C974', '9D4F9CC286B7', 'CFAF62184AB2', '5503B0FB1886', 'A2816855FB22', '45B7681DBE5C', '547A04414B54', '84187E561424', '4F5449021387', '3671DB5110C0', '5782F64504B9', '66ABFA4DA3A4', 'E595E218D423', '5721D0A1AD3A', 'A67C8BEE3935', '26E218908B4C', '1F694DB4D678', 'E243F4B2D0FA', 'CE64FA08E4CF', '738A2AE1D797', '2CFBD4AA7BE5', 'A996BE4B49FA', '543192A13FBE', '3C942012269E', '65688F5C5F82', 'F245631F4D97', '1C58F2AF08B4', 'E9428D8AFEF6', '81FD42FF7DBF', 'D282E21C2DB1', 'B5330C56B5B8', '3FC933A3FB55', '3FE607F4D3BC', '5C680F1815BD', '70FBC86A01C6', '2E1266682F4A', 'F92B3B747F33', 'C87A10617D8F', '8BFA9CC64A02', '00D304153840', '8EFDE74335BD', '16ED1E19CD0B', '763EF698F56B', 'A817C51F0787', '534353848265', 'DBA3D1D1D0B5', 'F953AA8577C2', '14409A305AB3', '4DC39F6792CA', '7600F1C7D884', 'F693E138BDAB', '66306888222D', '5208ECB70235', '5DD0E0AE3472', '97DD2D770B03', 'D7A9CF9A7786', '7A425A58F136', '3E6D1917A314', '15BA74A13D07', '7214C472823B', '5CCF95325E62', 'EBD98D754ECD', 'F468E21A6DEE', 'DE2A70D1284E', '27681E4EB137', '77C40DA262AC', 'A0DE768B504A', '9F0900A01266', 'BBF34C059226', 'F345260A27F3', '9752B7C08FFA', '8DDDD842BDDD', '1ED8279252FB', '124750B04447', '76BF3852B313', '5B20D8C3781D', '3B9A70FC3744', 'A4D269D38816', '2D89AB0B68A3', '0B1B547521F1', '5D724F93F765', '62EA3F855DB2', '51DEA81EDBA4', '3017001AD49F', 'B562D63C7817', 'C2ABDAC2BC2C', '64547CA6B2BF', '45775C79FA64', '779EADA2AEF5', '603407534EAE', 'A0DEA37279E9', '93E94C3BFE5B', '202C16A5B108', '2C8EA34FDC34', '95B09C17403B', 'D4BC5A89A5CD', 'FA2BC6C3A09B', 'A22DFE5E7337', '196E73AAF08F', 'A20D6E252177', 'B24EE1A93A0B', '18A22DA09EF0', 'C2A6C69714EC', '1D7EEE65F780', '343A95F8B1A2', '2E62A47405BC', 'E2E3F9182F8D', '688B11ADB595', '8BEC48C6C5E9', 'FDBC5C68ED9F', 'CB86688CF491', '95E622E22E04', 'F163B4249C07', '5EB99924C996', 'CCD4A267FA68', '6589766A4C78', '95CB51270760', 'EDE0E6181F61', 'A455B4430327', '75001A5FDEEF', '23AA5F45C05B', 'E5B076EB95A7', '75022F40463D', '988D787A291E', '911F68B99394', 'C5C6DED9C790', '5B6F224A710F', 'EFF473414159', 'C51752A9035C', 'F668F7A36DBF', '5816C1DCD336', '60B21E600112', '0347FB2B37A9', 'DA277EEDBA0C', 'FCF3A5BD8371', '13CFDB110C7C', '3245CF7D6A64', '5341197C5A59', '371DCD0EF85F', 'BE3B47A38A44', 'AFCE5307F69A', 'E099AB8BFFC5', 'D9C0995BC85E', '84956A4FA804', '4E1CF079ECDD', '3B2A626DE2B7', '9ABABB404DEE', '9407DB8BEDB6', '68BF11B32272', '410957337087', '504EF8AA4837', 'E2A75A854E28', '267F9C38F948', '2308726310EE', 'A8411CE08C68', '96A0B5E5A5CB', '014BF1790D44', 'B2F69B97BC2A', '148BB6325B8A', '31A3040F487C', '87A974F87B12', 'ABB7625615F2', 'D778E2E477F1', '5C094CEAB69B', 'A41214DF1DC5', '23F0ABEF8370', '9019F9FEA638', '01AEAC17451B', '7A347C69D3D1', '3F32AA4DA912', 'B67D550009E8', '7FB5A53C6DD8', '19E44CA7DF5A', '79CDD9460FB4', 'CE120A8B4336', '4BBC2E84ED08', '43596925AF2A', '3E5E0038C60A', 'C86676509204', 'AFFD150170C8', '83FA1948A3F4', '7476076AF500', '6D751D1BEB8D', 'E680D448BBA4', '475060D04C52', '69A0715FA8A5', 'EC1DD5C739B9', '8B3DB5B8F0B7', 'DFB0362070E2', '57807B412545', '15BDCD238E51', '2022B8E2B360', '459F27598FA9', '3F0E512E243F', '7BB808935B65', '0E6BB199BF93', 'C00E1BA4B107', '4D4A2F5E1948', '6E2EA3B14180', '83D5036778E7', '3C2E56DBA570', '7B751C9BE162', '3B9E54F5907B', 'CC4EEADA5262', 'ECFBEE2C79D7', '4D8415D5FABD', '1456B98CAF05', '8D242A52F9B3', '6397F69A35AB', '483A53143CF3', '7A5D38D49C13', 'D2DC98C026FD', '35D9FB44EBF8', 'DCB7655A11AC', '3CFA497859E6', '258C8CB1D40C', 'E1EC4869981F', '66CD59D78A9B', '1AA55FB101FD', '40A475C95D6B', 'BF2E86982E3A', '86787CB300DC', '52A09816A434', '5EF70094623C', '43EC1533429B', '9C6B4DBF759B', '27C1F6B19E89', '6C30C0076FAD', '20162733792B', '227853D28137', '28ED24DB496C', 'CD32C611650D', 'AA1F3740A24D', 'E6243EA8D1B0', '89D5A08E50C9', '7247BBE354B9', '1A067085A035', '62FC97D29846', '946581583AA8', '20A42E81AC2B', 'DA8D2BFAC0EA', '292FF100BEF8', 'A247FCC8D696', 'AB811AA2AB4A', '5D47B9B734D2', '4631AFEED2F7', 'E86C6679AAF2', '25A752FB734B', 'B8748AECBD72', 'EE580368CD1D', '8E7A674BF659', '857D1D3BC2C6', 'CAC4E6021973', '5681C8C42F1B', 'B65361F90B72', 'BE4E34FD45B6', 'A99895FA8F4B', 'B25D365F5C1C', '9E8925504E49', '9F12C5402E1C', 'B1AF3AD74555', 'C414E9F1DAA6', '90792E3137E6', '34FB9E0DF44A', 'E877D5AD16DA', 'BD128A3C6F33', '6081F7294031', '72279B65EB03', '10D09AB4D6C7', '78FFA7DA37CA', '9BE5C1E244EB', '0ABFA238F8BE', '8135441CED7B', 'AAA1665D47BD', '793800C02EE3', 'A65BB728AB32', 'ED8EE1F02203', '2DEC6F2AE21A', 'B36CF0CB30E5', '36D35B8A7915', 'D11AF7E28CDF', '081E861C5EE8', '4D2C67616599', '1A8074C87C0A', 'C6BB30D4C189', 'F4003A737E0A', '1232E9091138', '1A6338E69BC6', 'DBBF3EF47E93', '508DCB180063', 'D0279F114F87', 'BB4B5D896E96', '2F0BC63ECAF4', '20FE9FABD1C2', '127E7F587FBD', '60204ED9F4ED', 'D85690C1836A', '12C4FDD062F3', 'A2D7019DACF5', '8E0A4CFAEA9C', 'ECF8296772E9', '13BD2F675384', '63C087C70C54', '037C6126FDE4', 'D62E2B64236C', '9E61674DE1D4', '718800CC3C50', '229CBC81F9B7', '65E089A32CBB', '1065173BBE31', '43B12AB4BCAE', 'F44A8B59CF6A', 'F1EEA3FC1490', '2D23892ACDA0', '50DA28892569', '20E1ED11F61C', '91B840A41637', '65871B0D00E9', '38283A07296C', '1424DEF8AEA5', '2B7416B5A789', 'C755C1F6B4A7', '8D83D8D108F2', '4C3F39412E92', '56D513B3BBD4', '9D53E0729D4B', '824DAEDF9E75', '8E668341EE05', 'A8A25F94F5B6', '00B144412785', '4858280EA8BC', 'F2068036F26E', 'D8DC48B022F5', '6D0960F54CC0', '66DA2F1F5213', '32799982E5E3', '89183CAB903F', 'C5569B2725B4', '4920125BC38A', '76684944F17B', '442CFAE61CC3', 'AF077034AC88', 'ACABEA88860C', '7B6440A3F279', '9F0C2B822B76', '51DB76880081', 'E1412D9D1200', '961822095512', '1F20EF428962', '7C8094BF63DE', 'CE3DBEAAEAA8', '899CC108A4DB', 'BFF1386AFBE2', '099764B21768', '204E5D4294C2', 'EE426846F3D3', 'D225223D0260', '219670F16AF1', '163A7C9FCF34', '957EE90E67F9', '9C8126B25A26', '74FF3575E386', 'C9CAB778C94E', 'C93DCEE52172', '8C4DA60A580E', 'FEEA679DCBB3', '6786EE6112A7', '53C9F177EB03', '998E0F68318B', '2CEA555561C1', '7D89CBF638CE', '9A1DC81E4FB5', 'B0D5228E9FBD', 'FF7164909305', '155ADE36AE80', 'AB5EBB04C48D', 'D3C9713AEDC1', '5BB2AD58AF12', '6F8081D34E75', 'DA0FBE6F11C9', '5F897C00DB43', '304B55907101', '42017C1963DB', '99833F73DE44', '86A3A166AA67', 'AD7525091D85', '4776005B77F0', '057AF0C42467', '18DD6D2A09F2', '504CC3493DAF', '861588AA9001', '6770A2F02671', 'C28B8FFF66DE', '5E95EF84B113', '0B8A0777A6E5', '93E028526579', 'AE6DBB2AE6DB', 'F5EFE72B5E63', 'FF9E0379CD98', '768885A4B61A', 'C8C708EC29F7', '3AB30AB267DA', '9BBEDD9BD0C9', '2C344E66F358', '252C111E462C', '059DFFE3A3A9', '0FE5C9C13E7A', 'E4FC29A88EDD', 'A8DFF4D30133', '8E7975A0FCB5', 'FE40A50DF994', 'DB9838545463', 'B0F645CD20FF', 'D8FA1F4C6D5C', '2F19735DD6EC', '3DA81D432A0F', '6DD430E0A7CF', 'E03ACCEF14C2', '99216EEE458B', 'AD34E4ED347B', '2AAE2550DDAD', 'B4F6234EE053', '01267F2F5B01', '5218172D792D', '0C0E56A1FB05', '6794A463D2C5', 'E934DAA3C0BF', '96253054EFBB', 'FD97228DE00F', 'AE6BABB53D5E', '9AA193ECC201', 'B0D9B3EA4E5A', 'FE477BD61B22', 'DD0CD2B6F7CB', 'D150C1CD72D5', '31BC49520E9D', '754C88B5CA3C', 'A1A778253D39', '0326BEE07DE1', '2DFBA75B6C37', '3C4313611426', 'D20BEF9C2CD7', '39179028B2DA', '6660129B132A', '1F2351768406', 'E9BB013E0F33', '2D4643197395', '1B1E2883EDE6', 'DAD11E157730', 'A157CD4FC4C8', '4ECE014D0B37', '5B9E1E0553AB', 'EDBDDA26B49B', 'C20C1032FA1C', 'F763B20CBBF2', '2EDC00346850', '867A58ACE30E', '0DF37DD497A2', '8780E3CE2A3E', '9A03E46C0A51', '603D38C864DD', '3E89989D6585', '478D40552C87', 'F4E4C8EEF637', '515244292F97', '0C7890BF0A4E', 'A83032C6800B', '3DD7D9A53E39', '054C082D0304', '2F282FA175FF', '1F9C60A47E3D', '089DE1EB8CAF', '9DA131E0F720', 'FC27ECB471A4', 'B6AB1B1F7364', '5934838B26AD', '655EC70088AA', 'C8E9D6F3C80A', 'BEDC43C0FE6A', 'BF619ADE9BF4', '9F457CB98667', '2D95CCCD4DC0', '634A2AD49A62', '49AEA23D2E9A', '81AD1F069407', '077EA8162D03', '51FC77F6D507', '98947B33E164', '604B7BE00CDB', '0E66C9CE5B7C', '5171E1D8A82A', '5C5EB17E8684', 'AB45B8A7959A', '70B291155F0E', 'F748CE6BB514', '1A4194E02D4E', '10B5AC99E165', '57D5D51D6D7A', 'D485BE90E01E', 'A4C33694BE0C', 'CC3B51667B02', '8EB22AFF2A96', '3388A8CBA2A1', 'F949490671FA', '57859DBF4B4D', '54F6AD35A8D1', '90981BB41313', 'E8BF036DCFA5', '534835158D10', '367111CA290A', '88FD7FAAFA90', 'F883844CADD0', '372B34B48E84', '811DE32FCE53', 'DD8649A7235B', '6973D20B45C0', '1C9693E27251', 'F3E71A1A4F8B', 'BF9A3A65135E', '9B57F670E6DA', '32FE31495988', '7794085FE26D', 'FEA516299C94', '3DD1CCDA43A4', '985B2CF2C08F', 'AE7DE878CED5', '6AB7A38DBD70', '258F5D734299', '1706A44CC656', 'F6A92E43251E', '70F52F235E9A', 'A846B232FB69', '305F209C9A19', 'D4517AB595DB', '0961B819A4E9', '3BDE1763ECDA', 'AC1E6307E966', '3542AB52FF6C', 'A4FB165016A2', 'DEE347FC135C', '58DEF94A4FAB', '9A6F2CF32BFB', 'C944AAD86BA6', '6C394E3B51F2', '9C22F6D8C827', '2710BC110FD2', '89AB334EE7ED', '271011381AB9', '8D0E03A58CC9', '5E5A892CC4F5', '2BB14634D9FE', 'F32A0A41B697', '8846B3225DEB', 'CBF22594F6C2', '3063BD6D280D', 'ED42630A9FFF', '73AE93492275', '11C341083061', '6B34AE0F8DBF', '42B977C6E77F', 'F535CB6993A9', 'DA6BC6194FFF', 'E96117D22492', '5273831FA984', 'AD005493F9BF', '31A655925A8A', '86D1F207868B', '4BB5ADD5A1FE', '56CE93447F80', 'FA1E56F220C0', 'C7F1256D1E7F', 'F776F9881474', 'E09853233CCF', '656F48B15786', '16B7CA362E71', 'DC5C9B011657', 'B68ED8EDBA5D', '31CAE911027A', 'F955CE7D650C', '7A3E4D5F7F6D', 'B330C76B790E', 'A90544B36B14', '058CA87825F6', 'F5F96EBC0FDE', 'AA0B42CF00A6', 'BE68B9E53CCB', 'C5A3278EB458', '4F51DB73DF1E', 'D69D856DF697', '8F8B11DD9ACC', '159FA6D54C9B', '04084A8CDF16', '665D11F129DF', '98B95C21BEF8', '3009A8F385F7', 'D6B7EE7B040D', '2275932FD0B7', '4885E69D7C24', '9172F30C29E8', '139B1A57D6A3', '6BF54A9F8383', '8D3CFD31B4EE', '20A0C1C766DF', '060D9E77A5D2', 'DC9CBB208C6D', '4BC4DC58DB30', 'BE62C5534E8B', 'C43F61E4FFAA', '101256FB2FB2', 'AE6462B51991', 'F34DD475F104', '360F4E7D2631', '5C44B0B1C2E3', '44442D65C858', '23C514D3D801', 'F854C455EFBC', 'D0859D4FA7A8', 'AFB2995F886F', 'EB763115B6CD', '62185A8460AB', '201EC18E0A04', '322B33C931CA', '20EE7E422C2D', 'EC1FE04E8EE2', '08FBDF474AA2', 'AE021F85F4D5', '80031EC3D9B6', '14543F568419', 'B49ED6AC9AF3', '43D612E69031', '955C86F2C441', 'ECFCAAECAD31', 'D83490F6B0B5', 'EC0988E6BB43', '57A4BDE06D1B', 'D401F5D87E45', '5789A415F29D', 'F6DA839FE9AF', 'A97DE0D49AEA', '2629ECCFFE67', '58EBDB3621E5', '31DDE751E2BD', 'FA5C17345A74', '59A2F81ECDB5', 'D7D4F8C7FDFD', '64C09A835374', '255C21BE7CFB', '2AFDDA4D5D6F', '9252D20095BE', '3A240B2C1380', 'D7FFBE6E0434', '66B20410E2E1', '57691ED21F01', 'DC364EC72968', '161DEDCDD17D', 'D9A51EAE079C', '7C1EAA5096AC', '9D263A8A768B', '0E08D2567318', 'DA4EF8FD6A18', '0119F710D008', 'AB8EFBD82820', '560F96E36796', '6B9F6BDE002B', '0A6C0B6D3925', 'B04BEFD69D01', '3ED58343E367', '46CFE621048E', 'BBB0C6DD06E5', '6EEACF2CBF27', '17E36E840DAC', 'FA205817C3C5', '18A2E82D5F05', '01457B62F341', '16F6DD028926', '1E0E94F07481', 'ABE633932976', '650A40BBAB6D', '36E3A712A2CD', '0E37E20C3021', '78E39B7962F1', 'F566EA761D0E', 'F9470AD105B3', 'B1EF45488EA7', '17E0D40B3CB8', '6AB345CECE0B', '4E97D92F2119', '689A9ACF5486', '8498B3A8642D', '03A87384F260', '66ABB3042298', '63A014130F9E', 'A1F48D16C67B', '85D0379B23AF', 'AF744782945F', 'C4018603C469', '14DF107E8ADD', '675083F24FFB', '41395A333EE6', 'D5736E7D21F9', '0B606C33AC4D', '49604EAA151E', 'A300C28B76F2', 'B33BCDF39B04', 'E7E8F2F6314B', '3B3A47EA684D', '793D97BCA3C4', '5EF660E481FF', '1BB1E58F694A', '278C1879DA87', '60137928C48B', 'C160AFBD9D19', '9788FF5FBA3B', '89E1708E4BCA', '8A0A3FE01FE7', 'DBA1BE0F3883', 'C4F760DBDD16', '6DFB980B835D', 'C848C44E3004', 'EA9FCAE3C575', '03A9CF7C7141', '6B0402639A88', 'F9F27CA527E7', '3CF52C3ED074', 'CE97B1D61822', 'EAC03FA78D0C', '6B5D04317738', 'C1F74C98535E', '65AB218C2765', 'E5C0680158B6', '24E0B6DB7B49', '4F829961E964', '4F4C3717EEE8', 'F42FF5B7D208', 'BFC4CADADD49', 'A15649448050', '9B90E0AB50AF', '871287046F85', '180CC35C87D9', 'A9F764775832', '9451FC111896', 'B4BC1C61F5DA', '9D40E94D6935', '9BAE97BE9789', '9A7F7ACCC1D9', '77F3A3BCB5DD', '0504EEB01AD9', '63E8EF40EF1D', '0C751C31DE85', '75E100F355BB', '0459F1BBF74A', '547C43CF3291', 'BFB4A021437E', '2C7EDA83B0D4', '554DAEE7BBCF', 'A38F35A9F711', '54FB3EA961E4', 'B18CC50F5A1A', '7D391E32A173', 'AEB7346EA9A4', '0D944DDF9492', '92C8A41A0412', '0AC09FD13E72', '0D221F0798ED', 'DC6181711B08', '63B0C1CE1743', '8391AA01FFCF', '2BAA8312A66B', '5E1A660E4E8D', 'BA596EB98749', 'FC3BD2A0D23E', '03755AB0A62C', '3648A95056F3', '205A433D0CAC', '8726B24BA724', '33D741DE2BCA', '4125C404E34B', '1A243FC51E59', '4A19AA269331', '4EB6E7B8DEC0', '05A474FBA68F', 'A4FF5E47645F', '0322EFDA120C', '896F322AE281', '523EBD9ECA47', '97A506FAF875', 'D2946A413E6C', '3ACABC081389', 'C06037835919', 'DAAE03562B58', 'DBFD15D537CA', 'E0CF4993483B', '78B951722074', '9D7DC07877A6', '0CB2758A3289', 'EBF9131A638F', 'B736B35EC849', 'C6278E01D797', '286C3B565456', '26F0870BE37F', '2BB9A0AD9F82', 'D6E91124E3C1', '85D7C9ADFE4F', '42F0BEBB42B4', '453F56D5BB3E', 'A3285BFDEAA6', '39DD51C9D811', '411AE0653A8C', '0CB40DD64679', '569CBA2B3B8F', '150C879555C6', 'C7A316555DF7', '3C28743B377D', 'D2ACE090B6E0', '1719A6C849E0', 'A2A749DEC4BE', '9FC925C6A20D', 'FF7AFA7FF8B6', '644F08941EDD', '62E08644BD3B', 'EDF68039869C', 'DF1AA45CD325', '4B9F8C01BFA1', 'D63BA0463CD5', 'DB1F750E70E1', '15F4FA869635', 'BFF7296A29FE', '1CF95A009AE5', 'D932E9477A5A', 'BF7C67E62F30', 'E00E5656B607', 'F04A513C2963', 'C32D1DC4B036', '6A11804E60B6', 'B3165EE32FDC', '1529F615275D', 'BBBB5C4705E5', 'B80BB8B7FC15', 'AC594194F01C', 'F7CB662CA783', 'A4FA2C4BB8C8', '353CAC9AC658', 'E2630767E36B', 'E7F369EC8050', '7330313ED3F0', '62274F02BC50', 'ED830B1151E7', 'DFB4C78A64F1', '836BE35D2E45', '1D620B5FACBD', '242207118314', '8DCA8AB16C24', 'D20C40B1CBF3', 'B84A7F39D6A1', '56121F282D95', 'D2F390598EE0', '51E6646E0733', '123722DA6464', 'F60ED753621E', 'C39B288CDD5F', 'FC8DD899209B', '2C6E6657FBFA', '62AFBD98AFA9', 'A1B5025267D1', '39568660BDA5', 'CFDCEE813DF6', 'C8C168A9F3CC', 'F2825D154E51', 'B92B378E1264', 'A7EC6F462F8B', 'EF907722A512', '7648DFDA439B', '4AFC0853D08D', 'FC4FBC43EF32', '8B4B5C6DD5DA', 'C5C8126C1E65', '7C1D05780C33', '72D646FEEBCD', 'A29CD7941E1D', 'CE38BA1D5ECB', '92DFAC1BCB02', 'C3EEDCA21B42', 'BAF6A144BAAF', '072F9AEEC965', '60714BE8146A', '7767F0989650', 'B1B0D8237ED0', 'DA228B928B61', '6A97E65BBE95', '0878DADF1B3C', 'CCABE8EEA6F1', '15FD2E229EA8', '46425944AD00', 'A3BB032558E2', '0408B654C48C', '13299580C8AD', '9D1F1FD728E3', '22C152CE5B3E', 'E106B08D9BD5', '477B00F899E2', '5FD6686E9FE8', '0CAF8CEA967C', 'CAA042CADA38', 'DA757F0B202B', '09F7F5FAB692', 'CAE7D71D66A3', 'CC02E6A858C4', '1B4B715EAC9A', '19E57B0657A6', 'DFBB3878349A', '8CEB8AAC385C', '75C4CC1ABECF', '4B4A73217C3A', '99AE1C6789B0', '3923909DE518', '3F51F5840A15', '2CE725C8E928', 'DAA1B0DC9448', '4B64E914C68B', '51C6FEE51683', 'EA4763734037', '7D08A1D7086A', '86FB71F29A12', '214BC7CE00D1', '140434C2B180', '0B25AA5D6644', 'CAB6E91445DF', 'D514ED6A3665', '01D04F3DF6CA', '46EB864A3362', 'E04E83CEF5DA', '5A4B52D6101D', '38F38B0E6F05', '48D4153FEA5C', 'C6ACC74915ED', '9DF9D4AC395A', '21A359FEFC20', '606B556FD61E', '9FCF605CC740', 'E5C524E8ECB8', '4C1B24495AB7', '40FDE82C864D', '134BAA2D43CF', '7AFF516471C0', '3CBB0AD46687', '19B5612E4AC4', '8EB7CE98B8EA', 'CD7714C1464D', '9EF555637FBC', '3BA0E26A657F', '6D8AB204C4AA', '2505E3B0777A', '1D549C77CB16', '2F740510B666', '848CD0BA745D', '784ADFA51958', 'B90C3ED7033F', '1830EFB03CF9', 'E659800B906C', '8E446F4DC56A', 'BB92579586BF', '1842B5E46878', '044D921A8C99', '0FC97658A027', '01DB053153E0', 'BB3615AA4AF1', '53B68DAFE78B', 'DA6D07A177C7', '942ECB176B3A', '649B2A197C4A', '378005A2C405', 'ACF8D85BE4F4', '9510F68977D7', '1CE967B8E5C9', 'C86A77CEF091', 'B7BB7AA2B5BC', 'BA5C438CC087', '081A0EDC2648', 'B4F3E5D8D7D2', 'E8C1C58F8A0A', 'C9EF98006357', '50E905BAF3AD', '89E3EAF87D82', '1BDDBCF9B071', '4D30DAAE61A0', '0333283809F6', '198ECA5E2E5A', '4000B8222A07', '1FBDEE7FC78D', '603D3019DCC5', '70E008E55102', 'BD0D7907E4D7', '2D508D3CD27E', '202A2848311C', '859CA9917AE5', 'B92A891555CA', 'AC134C9AB3CC', 'EAD0F017B5D4', '3828201E7783', '1CCDBD707F65', '13C09EA02E56', 'C2FA7273B525', '8A33C5C1A579', '45A87F46D89C', '5086805A319F', '61F3B1F549CD', '2AEF9CECD887', '7EECF753189D', '1343B0769174', 'E9E9AEC9100C', '71D831DE643E', '0222E19C41BC', '9FDD8A146288', 'AFCF10293B9A', '7863164BA896', '80E78F9EB937', 'FD8B18756C68', 'D84271FC252A', '4A0B12C1D10B', '4A383338D870', 'CC5894978F31', '929CEA8DDB32', 'E4F9AEC51ECF', 'F1A9A822A619', 'C4BCD14367F4', '387FEC6C2CB7', 'FB0053511F2F', 'D8406ADB1493', '766376432498', 'C2A49B846D05', '8248E7E6E2A0', 'CE3B81539AE0', '65E3280D899A', '29F5392497DA', '8742BE58A671', '51C3D7E06ED2', '5A510998E08B', 'FF9202FAA566', '6A290D8D6729', '70ED78541A7E', '95D8D99C517E', '36FCA185E7E3', '6B56C630FFC5', '2EEA9454E26C', '403C86C75FEB', '5F931A3FF8F9', 'A0928D4B7EC7', 'BEC88463148B', 'EE194115BAAF', 'A9E9851F3E7E', '0168325D0E24', '30AA3CFC1A36', '62D796BB3724', 'A8683674019F', '35242F4AB789', 'D828BE60BE57', '46DFE5507DA3', '38F4CE28F1FC', '7359BD785A25', 'E459422C794C', 'B4AC3F641AFF', '05EC08C05019', 'C318777ECE9F', 'DE17AE0AFF7D', '0EE9980F4C4B', 'BECA14914CFB', '3C315C4345C7', '4155D50399D1', '8CA7DA405671', '2D9010823094', 'D8824F42C5CF', '38066765F81C', '09F5B52032A1', '5CE46A992889', 'D962FD67FF4C', 'F11D449380E6', '94DDDE49F8DA', 'EDE56797770A', 'FE129177A067', '6805FE96E548', '675D63C3D265', '94810298EAEB', '43B24ED85767', '33008B95DE9C', 'FDB27AC9357D', 'BD7F5D94AFED', '8E915F00ACCE', '2A2CDE97B76F', 'C70E3EEED307', 'FE3CA06DDCA1', 'C9B23C5B1064', '24C5ED476177', 'ACA7AD2E5716', '2B4BE109A9D7', '530F622827DE', '163AC455E1DF', '94DF4BBF522F', 'BCC1433ECAEB', 'FED359DCA0BF', 'BF3EE6D66E3D', '1A068661D19E', '4336E25D1D6C', '73B8CA4B266A', '26432FDB0644', 'D6A32ABCAB53', 'E22D53ADC06A', 'BA5056B8C67C', '3FCA40583533', '7CF24BC81BE6', 'AB0403712FA9', 'FB8A6D53974D', 'CC68BA9DCB2F', '52835AA731FA', '6587820089F3', '377548575048', '9C4444C557FB', '46527BA000FB', '01F69867D1BE', '18EAB669FDE2', '8DECBFDC0EB8', '2DFFBEFF2B10', '9399DBE21A35', 'DB4FE19B8FDA', 'D3B5E0743515', 'B7685AA86F85', '01A4C886D048', '1C346211657F', 'AE47AB399CF4', '503E1602666C', 'BD304A35EE44', '64DF7B801614', 'BE6FC06B8795', '288183B3184A', '1C3440A3FFD6', '331588D6432C', '4779C9C9093F', '89822D57FE56', '6E1B4AF3CDC3', '3919F3440FB6', '4FFA8AA9820C', '28797216EAA7', '8B4BBB3F601D', '25C37AACECA3', '67680A4C81E7', '0BE6935B5A54', '0BD82CC1FC8C', 'DD95B403F95F', '6B3AF4AD81EA', 'A27F8CE559C8', '63C72E480CA7', '098353275507', 'EBA1B05B6624', '18554EA0420F', 'FF62E6972211', '22916F4BFF97', '5B9FE6CD8325', '0BB0E957B6FE', 'F362313AF4DE', '23A195AB438D', 'A7BD93C544AE', 'BD33192DA18E', 'FFACF4B90AB1', '4FF7C90E8071', '8F46AC3C99AA', 'FE62304F4B86', 'BAC681996825', 'AE5B6C4FD5B7', 'E87BDAB97148', '3799343B6C0A', '87ABD416A9CF', 'F955CADBB6B4', 'E418CA5F2D7A', 'B9347CDF2461', 'B38B82E3F538', '27D34E27FD4A', '3BA15254E27F', '4A28DE0DB485', 'DDB424217004', 'DDE62E929C6B', 'CED03E44340E', 'F98C8AAA5BE3', '3206E3F5CA39', '12C40C11C57C', '8CB1593BFA5F', '8B095EB24288', '022DD0BE5A38', 'F9E1434B2151', 'C1BDFD85234D', '308DA02A219C', 'ABBF7FEB056D', 'C75B9C4FFEAD', '4B3862D5B402', '62AFFAE17116', 'EF08234FA6BE', 'C48F269A6ED8', 'DD259D664877', '987ECCF90CE8', '0F13AFF35C0C', '9127434E8CF8', 'A282A18AA03D', '3DB6FCCA1445', '437E9376FB71', 'FDC5C6E0B4A7', '71DF40C12652', '04DEFB0B11C5', '008C191EF21C', '8D509BBA7DD5', 'CA92495BE92E', '532D74D761F5', '91FA9C603A6B', '83948102B382', '17C13BE317AF', '0EF91AC5D457', '5173FB3C75A9', 'F9A5B72FAD82', '043E9B5AFC5F', '908906779AB1', '934168169D89', '109FC36735A0', '8A00BBFCFABB', 'FB9569BDE4BB', 'F3454E01E31C', '9FEEEA2734D2', '83D8A875FA7A', 'A550B8EFC039', '313229323B6E', '75B94E37D75A', '655A3B3F19E1', 'C74D540B715B', 'E223E52018C1', '87328D07C574', '2AEEA0A97907', '4425CB6C8585', '4A4ED75E7112', '2B3601A69135', '7DC8112AA69C', 'C20A6EB7FBCA', '0E5278E12B82', '558BB83BB400', '916117BFE2A1', '201EB1D0F1D5', '94A5C5B0F0AC', 'BEAA8D8A3EFD', 'F5BC72AE5196', 'D4A3E7EC982E', '9D0665A39328', '8A533A6E8808', 'FD48976C9913', '542FD18E9ADA', '905A76FFA540', '2446D9BE78B8', '922038AC1578', '6EC2740A88BB', 'C1B32D871830', '5DDEE376DDFD', '138BB07570E0', '91A358248CFC', 'BB207C714EBA', '6F8286D752B2', 'E125A0E0FD41', '6BFE834C4FD5', '7A532625FFDC', 'AED200889835', '1DCF42D2B503', 'DEAF202F9EB2', '0EBD16DD8BBD', '3EB880F4A4FE', '65994ACB31EF', '8D3BE9EAB1D5', '2333F75DB5E9', '8783DB652376', '11823089B61E', '26981B151FD2', '4CD7E8A7BA22', '96E0F82A7BCF', '31FFFAFD3987', '46462FB89C84', '8CD442C14869', '61BABBBB9822', '3A9F1E5CEF61', '2FAE6DFF45F1', '03C54838ED36', '586269D6471F', '23156B9E3E53', '37B05EDB1C50', 'DCDE7152B94E', '154897339487', 'E94074738406', 'D4108B56F2DF', 'A66F364491FF', '471611351FAB', '81F1DBD56E0D', '89039D21E960', '614C338B9D97', '268675CF9C2B', '135D6AEDDA9F', '856462BFBBB1', '7F63036D0C59', 'AE359CF88362', 'DC2EE6A2F024', '791E0902EAD5', '65F5C8D8AAEC', '91125D5EEA38', '6C4D40695123', 'E397850CD3D2', 'F7E9EF1AC55A', 'C541FA8BBFDB', 'B18624117239', '83B1B27DE86E', 'F106A7FB6C5E', 'B5B5BB3CB4EC', '7D9F1CFFFC95', '4D0233F6C1E8', '0226E802EEEA', '58C092C01C79', '10F4C4CDB76D', 'AA38015B5E93', '8C2D0E1314DB', '4F7F24918F42', '2C871443D686', '25664FBAC604', 'FBFAA80ADC60', '2B605EED5E4E', 'B0A19F407570', '82BB2259D709', '35011B0C0225', '9680FAE8D811', 'A53E02BB0A41', '1B061D85405F', '2B49B48C3B2C', 'C10C521A47C3', '7ACEACD37436', 'B0C7779B7276', '89FCD2B81990', '1613BD216385', '5448E486707D', '9F1CC7603B64', '1033B94279B0', 'E88B3A355149', '49ED588E79B1', 'E7B3A115D111', '5D3F76E36715', '2FDB1B6086C9', '7804C4E627BF', '8BDA09CC9FD2', '66E83D44F4A3', 'E4DE4FCBFA63', '5CD4B139FCE1', '8E6166FA732A', '763AE37F4BD7', '715555B20F91', 'E7C8FB41C6BB', '15B1E4516018', '52432D25C86B', '665A2D50F034', '2E1C25D6F265', 'AF938D285908', 'C879A485810D', 'D35AE4A7CE83', '5B14E2CB36CB', 'CBC776495BE0', '0AD7535C58C0', '0D89C8189022', 'A67D3F549FAD', '86D78A2CD591', '2B2696900DDF', 'EAD34355445F', '20B696B5B9EE', '1D45838B86D6', 'AE82BD8A926A', '2E4A8624CB67', '03F81E8838D0', 'BA9B801E3540', 'D88B4501DA8E', 'A70A57F5752D', '0F7DB6BD1AD5', '83F0695E035E', '6FF14DD83450', '44C7E9C4260F', '0BAF08001514', '8732FA1BF586', '74324AD36BD0', '6F817602FD7E', '3A38C6310D54', '281B30A5EB30', '55C612B32725', '5E7DCF4A5293', '7E37567419F5', '8945D160056D', '0AA050C570B0', '1202C1C507A0', 'FF7BA63A1993', 'CD86F7930D58', '8AB07A248878', '0B54CC968D4A', '24FA36D49DB7', '4918D9A545C5', 'B0E17505BCB3', '47B65C7E16F9', 'D14F3F93CF54', 'A00DAFA2E4E8', '7E026B760EA0', '4BD9A9FE0FA4', '93A9B27859BB', '0D3737E9F443', '60B9CA06540D', '58FE015253B7', '2FE09CD5CD8C', '336F8093CF86', 'C212B1F1C14B', '25F9B9BAA02A', '7DD34B45537F', '317BC7AD88EF', 'FACA3592FCD0', '9DAE6D2462A8', '86A74374E724', 'B3C8CE820C5D', 'FF21CAA2F125', '2C2CF03355C9', '8A895792F3C7', '66031F0D0329', '75EDF0FEA5F1', '5E9B17A298AB', '9263362F551D', 'CC3A5AD37340', 'A602D45D22B2', '12725EFCD35C', 'DC5B8E6C23E6', 'F24858D7F8F7', '3C879A5C86D9', '727F83C82B08', '5C29D02F268A', 'ED1EFE97C40F', '9C20E2584A8B', '5394485B02AB', '00D719F00D9F', 'A851E38A7344', 'E4630B623165', 'FAA1621106BE', '86E1F1C251F8', 'C68C60B718D4', 'A2745099DA0B', 'A975647F8219', 'C1CB555641B6', '688370E9E6F1', 'BFCE87556F2A', 'F7561C027920', '312C67D3E8B1', '824B1332670B', '29203AAA6CC5', '952569CCDD6E', 'E5F47974A6EF', 'EE6FAC759013', '0CE521F8D172', '0CCFB5F4E6D4', '45A5FD941F4A', 'F10FDD44873B', 'BA3D6BB860C6', 'F18B05581BAA', '505E1F0E9AEA', '4CDA5FCEDDB1', 'B9688F9C3150', 'D703C053AEC6', 'B6261FF12B56', '7A2D0B189298', '98A9BF4E795A', 'F407D70D45AD', 'FA6FD7F076C6', 'EBFB5DD26BF0', '2F889A4F325B', 'E1B4C22E7785', '22C6C0356324', 'B056CFFBFF3C', 'A5DEB6E371EB', '7782DC357595', '1C41622B0A7A', 'B3D92EA18E0F', '8E83FB707B87', '5F58F22A24B5', 'CA0C21200F78', 'EF9E9C4E5D1E', '18860E0DA892', '2CD086E87F2D', '10E592ECBDB0', '34A463C4B1B4', '0B4FAC7A4A8B', 'FED2903D0697', 'C07E3C23CE7B', 'B8A754A0B9DD', 'F17D56052066', '2275D39DC5AD', 'EF814D7762B0', 'A7F8EA8D1E7A', '4BD30FA12457', '31C7E435A083', 'DF071F7D4B04', '68E7029BE2C0', 'A6FCBC1C476D', '4C60FE6BE07C', '91C1E1BA845E', '6B34BD2CA144', '47E3B3E558E1', 'F9BFE8C8AD17', '703C3A3D102B', 'C7C9C01597EA', 'A4436BAB9532', '66FE148BE39B', '008B27E80228', '04AAE2E8EFB4', 'B9D9B02EFC86', '3F30E7FDE6F1', '4698AB6B27C5', '269E80DF1073', '1BB48664CC6A', '0D6F9BEBACE4', 'C3D4F36886E5', 'F98C913B5446', '849911F5126B', '99451900AE17', 'D6278021E119', '95D479E8C7E8', '79B0B907CF87', '34AB53DA73EF', '40C8F8E9E046', 'AB1AAD6295FC', 'C9C826197027', 'A5CDE9900195', '7F513E5DA945', 'AA04FD5FC3DA', '75614E82027E', '857F67692BC4', '0F571C3502D3', '887CE761179C', '45CBE29D23E0', 'C8EBB096262B', '7B4FAE7A57C1', 'E018497ED277', '45DB353F7471', '422A3A47C33E', '06A9B4BC2E90', '546C0401D25D', '5B2822CE075A', '7440926E8961', 'AFF882607973', 'E52E56670B24', '0C7C06A6D656', '46D1A8FC5D49', '7D14E283380C', '13F1DF1DB4DE', '95ED30690B8C', '4F39C001C736', '2983A5DC1E8E', 'BE5D3ADC4F2B', '7CB443BFD73C', '20D2FEBD3853', 'EB5021B098DD', 'C4D39AEFAA87', 'DE0275A222C1', '99FB1D4C4F5A', 'B51E027B51AF', 'CD33A5B0A307', '2E7B7DA84C7A', '31A3727D27B8', 'D889C48DD497', 'CC2FAE6B5135', '94EFC5F3ABB2', 'D44639122EAF', '9411BF4CF714', 'F65618376429', 'C59ED272B57C', 'BA991FE296BD', '2FE7098FEF58', 'DE842CB71C02', '5AF8EE583A33', 'DF723D10CC23', 'A46BF87BBB48', '564FEF97AC52', '2BB45C0C7802', 'B15B3D1D9CB5', '5E8BECE0067B', 'FBB35C3EF339', 'A5FCA4CE2EC3', '158F2C1BBEF3', 'BFEDA9FE9F1E', '1C9E388C17B8', '2DE70542C32D', '3FDC2CD9FB9F', 'BEB797E37AB6', '9A6C18B25037', 'B996860E00A4', 'F0E863BDEB82', 'FAA7B824C589', '2B6B50852E0B', '9898C2839018', 'D2A1577EB7A8', '9BFBA5356167', 'D3DB341F4932', 'F45A719EB22B', 'C55EA5FFDB2D', '910B844ACCA1', '3E914F66058C', '718EEB2328FC', 'C2BF4C553358', 'DD40AE6E1E6F', '761B16CEE6C1', '6E919842050C', 'E7C5E12F4E84', '13E8A2F76C57', 'C6E43B28660D', 'DA41A7B07DF0', 'A07A05AD9104', '97E8C5E4AEA9', 'D75B4D14CCAB', '956B5E2F3BEB', 'AF642CA41345', '62E0E0159843', '70B6F3CF220A', '52043A84C3BC', 'AC871BBF14B5', 'BBEB4231A8AF', '7CEB4836BAFA', 'AE8CA585DC1F', '8975E6D0380E', 'BE243683124A', '09FF4F0D2B79', '288312CEAF6E', '4DCE9956E3DE', 'C7A018905E52', 'F038AB4D788D', '64077B385B48', '65C1010EFD70', 'CBF57C499977', '493DE86CC0CD', '7E20F422A3EB', '1BC811EC52DD', '980850873AB7', '3F65AF00FC67', 'ACBB0618CF1A', '5E75902533E2', '53EEE7A5B5A1', '02A6AE54EB79', '05E82CD3394E', '855FC08FC096', 'C4AC99D9CF4A', 'AD3821286500', 'C646479A45F3', '5445F0F5D9ED', 'E518FBD13C3A', 'E04DC60DB4F5', '8CFD338AFB98', '25AB1A453D24', '29687C1F98F1', 'DECAC443C667', 'EC37D36974CC', '2114A12D2095', 'F6CA5589DCDA', '0581A6F4E804', '6F326F032E91', 'F01B0824E489', '6DEFC88D4E0B', '04AA83899997', 'B864F3DC71FC', '7E26851E8EA9', '571BE3159B48', '1D369D51E5ED', '4A5AB644C41E', '4579B625E1E8', '60FFF26260C1', '79CF90622262', 'DAE3E4057810', '8352B07F2708', 'AB6284069E18', 'E4BFE6EC3275', '58C4A1F4BE4C', 'FFE46004CDFB', '20C0FA255C6E', 'A7B788E51FF5', '971158C1AE69', '3D64CDCC5B12', 'C5606BE37BD0', 'BDDD49DF0C7D', '51464A01F6FC', 'AD19FCCF4D66', '6A2FCEB4F2D1', '943370C19A7A', 'ED27B5020694', '67683A36D7BF', '51F05D3678EA', '1A88DC345340', '793517BB1BB3', '2C8BAD341085', '079F9B97A2AD', '882808C92A6E', '22075172920B', '2CB0A75DDF7F', '4B51E3917E0E', '71D3F6E61C14', '5E57B80BFC66', 'A6B3770935B6', '2C191F7F9FA1', '4EEB3CAA6F48', '97EA28FC8ECE', 'D784E06655CB', 'D2BD6E5B0D1F', 'BEEE3AC55C0E', '9E1B8D4EA6B0', '93FAE079492B', 'DDDB09DD580A', '8855A0898835', '26788B95F2F2', 'B1F95DB388DB', '4896AD6DD91E', '9483857CAA80', 'E5F3EC28F670', 'F0095A6449E9', '84CD770A3BA1', 'E67A118F9AFA', 'EBCABB635458', '16D3EFBE11FD', '4FFC4DB08B42', 'A02D5B12D66D', '29BE895CD900', 'A603DBD5D182', 'B0E846E42C45', '2FA02A1E3654', '2C3E3E48A880', '00A4CB36E006', 'BFBD47C22FB3', '9F7BBA5FC646', '4ACB29936C94', '0A23334002E0', '3A748C1D650A', '48B50A9C7D0F', '352FAD192BB9', '6FA5454B11D4', 'E7DA3EBD8366', 'F4EC42B28077', '448D92E762E2', '840A520F0D99', 'F40F9FB07FAD', '3F7A0163C8E7', '4DF55925F544', 'BE3908E29A1D', '22E223A16B82', '7DE381011700', '1871E7C9919E', '6B3C70C249F0', 'FEB736959AD7', '923043ECE82F', '29BB45AA907C', 'AC525B50F33C', 'AB69A8EDFA20', '0433D61E8652', '0B720B5EA0E9', '273863D43441', '07C6107B301F', 'A2B0C80404FD', '3B355996A407', 'CC50C8238771', 'A7AB35E04C19', 'D54F90711EA3', '5672F9F49336', '5CD2BCF64F4C', '0B508FD837EF', '6D681ACAF7F7', '0A5BA91AA8B5', '0C0C473FA794', '44D73D95E310', 'C94D88474063', '446DFCDAD7F4', '192A27F0608C', '738E215A237A', '13D83C05F4D8', 'FACF20D0D847', '85B5621E1876', '44E343ECD9B7', '0F6250B6D784', '58358B109222', '0AFAD06336F3', '2252B166CE0D', '75146054E72F', '7992F4E1E3D3', '5E1DEB2AAE39', 'D9A50E9D3E5A', '0DDCAFAEA487', '46DC70997CEB', '84DB11599420', 'EDC00F1B2823', '0F43EDB760C4', '6CCF20B2E727', '36929B945E29', '2056B171116E', '2AC878B831DE', 'C5565E4761B7', '37019C8C1ECD', '5F27253980BB', '65DB75B187F2', 'AE587E8BC0D3', '2BB327C90883', '651CE6F22A06', '4E1F37A9728A', '21C30597D17F', 'A1B2F28757E3', 'C7BB1DCC0537', 'B44F4F2E6178', '5B762EBD64C1', 'CCD422BE9DBB', '20498D2BFDBE', '1788B6653312', '27C2C3000FBC', 'D77794179080', '7FCBE68F5923', '0646F7BFC681', 'E059AC481E5D', 'C321DF7BCA1B', '126747A179A8', 'A5977AF16661', '6E691E18CED4', '64BED9417DB2', 'BEF5E55E29CB', '463F959EAF89', 'FA15A1E3BF2C', 'F6056CD0CCCA', '3F59617806D4', 'C55E03052E48', '25DECA78E234', '9DC3CADE8953', '5729D5AE055C', '0B998187123D', 'E8D55D0C4BAF', '2AD8A9FEE4C0', 'E06960D4A6C1', '3BBF8A0059DE', '61B15EC2F7B6', '9C3DE25AF92A', '70D2F22E29D4', '53BA0A8722FF', '80E2B7016611', 'B23941DFCB72', '6F220B8B2495', 'EA3B8E4F74DB', '3316349275E3', '825C7DDC0008', 'CF0214901725', '232A55A50FF0', '49F6198DE921', '6E154A9B7943', '552ABBDE1296', '55B4B2BFF231', 'D22D39A0C3AE', 'B2EE7ADD5BFA', 'DE01C3FDCA0A', 'A7441C4D1FF3', '3A9A95F48F86', '3B2C8BB36BD4', 'F19978B8C2A7', 'A34FF5F00E3A', 'D354980E4F07', '1FC7106860A2', '5DB4FE5D6A00', '8D4DEA77D545', '8FE284BA4D25', '1BB2EB85C41C', '5656301109BE', 'C8F035B467A7', 'F3ABEAC1432D', '8BA8BA5260AD', 'E797D7F9B452', '1E4261B396CD', '0985602832CD', 'B727B720430B', 'DCDE1C43C218', 'B6F12A1662D4', 'A15776CBECB7', 'EA6471CC4E00', '9EF6AC7590A2', '5AA83726798F', '7A56F29D9707', '6FB96EE78D6F', '04810832D23A', 'F6F8929C3714', 'D470DB16B85E', 'A54C0AA815DE', 'B6C009160238', 'D4375D1ECD7D', 'FC9BC150809F', 'B133FCABD0AF', '90345B5B064D', '84425797BC28', '1D6220DAB645', '1B4AFEB6E86E', '160C8B8C0E64', 'D85B93C13556', '47FCD0F878B6', '32E3A5BDD0B2', '80CAF7FFFD56', 'D195E4354F21', '8F899DE6880E', 'DE34EBAA18EE', '6F096C626578', '37A77BEAD718', '1E68FF259297', '0BCA18283DBE', 'F1860C4149BF', '3C4368A07D88', '56774BA72B7B', '25086B7087D6', '52B24BAEDC8C', '9DDC8FDAC3DA', '89C921B360F7', 'DECECA25BCBD', '58D5E21C7B46', '4B14E9E6D731', '7422D711C2FC', 'CD17678005B9', 'FD311907E3DB', 'AF05AA1A49DA', '0CF568439ADB', 'EF5579B524B3', '69ADC83F4BBE', '410754A81FB2', 'D58DD07BC0D6', 'E63689D747FA', 'A4C42F2B3A57', 'A550B41E7CD7', '097FE8053507', 'F86616365497', '5403979D49E5', 'EB16CD7EC3E6', 'C4E233F5D9A9', 'DC93D8BDE631', '60E98C89CF04', '232A864E5A15', '82FF684D4DFE', '8F625951F759', '9BA24165EE28', 'CF84C9D788A9', '080E2CA04B67', 'BC73CEB66875', '0306244BF63F', '5843D4C6EFCC', 'E95621C07D95', 'EAC3596461E7', '6796C7366713', '113942A18C27', '446F3C285AD0', '9D000F734774', '06936C8AA35D', 'D23F65F4652B', '8B696D4A84A0', '399C14D08042', '0C1D8179D5F3', 'F6C40C564E5E', 'EC7C81E59E9E', 'DE7F02980A1D', 'AC8F2F26AB2E', '1B2083B9F74C', '30B617145AF6', '5AC16E188F1B', '030A70B9F194', 'A5B9C9F26FD6', '2D10F2D7A580', '24FCC5A84ED7', '7AB72C44704C', '8EEB5226121C', '24241413242F', '79BFD0B9910D', '1B87EDD72ED2', '8CDFCA2C42DA', 'C7BFC84A7A01', '005026E0386C', '7D6BE3FB87CA', '18D68AEC46D6', '3D84D6011B1E', '49D008420FDE', 'AF80EE1F7435', '44F6DC9C8326', '151645FFA225', '31EBF68660C1', '0BC531D9A1FD', '927708B6ACDF', 'FCA553D1B819', '4B048F74F3E7', '24CB17912088', '457D56541EA3', '0FF049B01070', '468D150FEC7E', '8C57C1D980FD', '159424F57C24', '2F159741CBBE', '58F2F77D8FD6', '6848515B048E', '7F1E6FE5D698', '970171A0AC8E', '831F54B2B19D', '0949D328CAC8', '413384DAACAB', '8A57EE89F26C', 'B86D09F6A43A', '45CF27F8EB92', '49CAABCFD99D', '38EA1B4F88C8', '84C6F616721B', '6058BEF565C3', 'DEB6EF459B10', '4B1EE53F33D4', '111D45750F1E', 'B1FC2FA3AFB3', '8D260545092F', '0FF781FB6050', '1F8DEBD4D55C', '623D70660238', '0D1493FDAAD3', 'E9621B1CE4F0', 'D84CB197450F', '3F6E2E7E7734', 'C8F378A4AC05', '7F7B48A486BE', '79766A7A3911', '1C1D49349242', '9C52EAA3F005', '5C89C0506A97', '8DD8AC01540E', '4D55CD8CCF85', 'D4620A5134E7', 'D5D8E7DFCF5B', '531C5AD9451A', 'A11D007DBECD', 'DE499912068F', 'C2B40B0DCC34', 'CFF21231DFEC', 'FE91D719ADAC', '8C887CDDBA32', '2BA142AF4BEF', 'D2ED07A7CE6C', '01AFEC143032', 'F789157CB6E6', '3D5C564AACDD', 'F2A3BE5019F0', '31F08BEA89C9', '2FF6A393234F', '71DFDFAB8621', 'C535ACEE801E', 'F492EFD91D3C', 'BD0A4A94AC7B', '85983B2516C5', '93B4EABFBBF3', '36BD7E8B0A6E', '3A0B15335CAE', '7BA3A439D22E', '0F5879D60008', 'DA645EE46524', '3AC9C50357E9', '1F11C3F72282', 'EC36E0BE58D8', 'B181B21A4FA6', '0FC32E083BDE', '080912B0093E', 'B50B1FFB4697', '5A3EB7A7A627', '4EA35584BB87', '986C125C24A1', 'BE58ABE09429', '8F8BCC081DC6', '8F85E4509835', '44B06B4A6D7E', 'FCF9802D7F21', 'F538469756EF', '6B6A7138292B', '2408A9D495EB', 'F21A02C66DAB', '84D2A6C75C67', 'C5B52323339E', 'DD586A625C89', '44975DEF87EC', '5B1C8A89B4DB', 'EF9D79A33BE7', '5BB702C07F75', 'BF00A0DFDA72', '4B7B7D384FFF', '2ADE4CD09C5B', '93C0100883BB', '838005D1D7DC', '9940EBE441DA', '57846986862C', '6783CF0808E5', '6141232BA814', '6910A7FECEEA', '939E66CC3A0C', 'C51E0C551CE7', '10C813389A74', 'E758B77AC952', '16180FD0AEF8', 'C3C371CA957E', '93106B20FE87', 'E30209138F4A', 'E51DC9CFC464', 'E76E617CBAA9', 'D3AFCB51AA40', '233969D3AD1C', '21EFAE02832D', 'EF1A6D00C3D1', 'DE78893DB845', 'E60CBA7670DE', '416FEFFA5DC6', '883880ACBA01', 'EAF6BCA737BD', '26C28DEA2690', '43CBE96B374E', '49083564C202', '052BD823F2E5', '011DDBA781B0', 'D6668C2DBD4D', 'BA1E2A47DC1B', '88B0F016492E', 'F788D4C475D4', '79F319143CA6', '288CB26FB8F0', 'E85C45D90D1D', '54D9E8287B96', 'DF5281802CF1', '84D821F541E7', 'EB9C5ADE320C', '0A43A08B43DA', '1CA9E28FCB00', '0E16BB8C40AE', '318C3D719F7C', 'E9E6AC0ABFB1', 'B5BFAD94E4E7', '32AA7D5E04E0', '27ACC3AD950F', 'A8BBE9026DC2', '23CC3629CEC0', 'A3EF457B5F61', 'D1FC358F8C68', '62BF23872BFE', '2568B64DB374', '25DE215F8B82', '0E8CBECB7CCB', '6F271187D67D', 'BEBB31733A38', '69E3314D361E', 'B8164EA79177', '602A632736D2', '3DAE999D059A', 'ABF30AB2C625', 'F983FFA8BB50', 'F460F653B33D', '4E0179E2F024', '6097EE3B178B', '0A7B1C9CEF07', '6C83EED20357', '9F93DDF89408', '98FD2BF096A9', '218490FD2E1B', 'C5382404A84B', 'F82207B7F26C', '203C3C0F1250', '11854A885ED6', 'B3B624F3F792', '302D33AA69E8', 'DFE2D3C4E33C', '34262AC688A1', '6DC268B2F2F3', '424419B0718B', '4C30EEDA3A8F', '04F277F6562D', '627AF801730D', '2C2C377C45D0', '905BDB331B7A', '286D497ADBEF', '45EA1B11A1F7', '1EB2AB9D50B1', '767676854264', 'F760756904D5', '35C0AB9FA522', '33E25E6A8028', 'B4CB3063482A', 'F5DD72FA313E', '435A90A63FEE', '71EF8337424C', '73886FC7FAA1', 'C739DE328A1A', '2554E3E50652', '8E98853DF374', 'A112011A5F17', '294CA62F98EC', '233A9C9D748F', '37A2E6EA2337', 'E036E3908A19', '71DFA360D5EC', '0BEF6D61E359', 'B6D833F8240B', '2017D32A0639', '1CA7B8FAA58E', 'AEFDD857A5D9', '7CF49BA27233', '91A00DFD46DC', 'E55C5624FD0E', '5E9E3C993595', '9BD314B82CC0', '16C808F5F642', '68F6A6DB3E6C', '71A434779420', '5EA13764DA34', '21B7E5DC72D6', 'EB3398FD0FF5', '902A59ECB525', '74ACC8E357FE', '301B3A868A3E', '57596DF3AD20', '30045307DB37', '878BE2C1F6FB', '871CE2BE3370', 'D8F3781C0ADC', '9B1CF41C7F5D', '121DCFFBC671', '34969C5FF82C', '440790E93DC0', 'EBE406D40544', 'C731FDC59282', '82856A9600F1', 'EB901EEC8E98', '6872BC56971B', 'DAC21768A84A', '780B8C757C48', 'E84C23AA6CF1', '7C12F166B5F3', 'D4FC7257FBA8', '6BCBEA10B6F8', 'EFB0DDC8EA77', 'B7075CD10BCC', '3FCDB5437831', 'D2C7D096655F', '1CAC3BAE4A39', 'A4FBD7A27089', '63263E72A196', '8AC33F8E9C35', '296D2E892341', 'C7AD757540AE', 'A4B47ECED161', '85C8AC30C12F', '82DEE52546A1', '47F7E409D879', 'EBA2F18D9666', '95874BE012EB', '49A315660C2E', 'DFE90DC3F01F', '39539B71D857', '2BD0B513E03D', 'AD8BBB12682E', '3193A59E5543', '7936460A2B7B', 'B8E816DA2049', '6E1194215953', '6EB681AEA81C', 'F7B9D106E6DC', '78A6A188B132', 'DE67BE4E7A3E', 'AF9079B5EF21', 'AAFC3D84F9A1', 'F3C14C00A0CD', 'FBC4F9ADED9A', '88B016E70B03', 'EB1500B8EA75', '5BC582396E5B', 'FA3E37900E3D', '077394BE6603', 'FD6E0C78E9BB', 'D840AC3957E5', 'F32665CC66E7', '69C237AC88FF', '6A11844C8AD2', 'E4DC7BF5147E', 'A6504C463F7E', 'EE440E670405', '96141BA9C7BA', 'A2291A8A41F3', '02D6A23E0CEB', 'E7741CA1BDC2', 'CEA16ADD393C', '2EB94A444839', '709EF21B80F2', 'FA87416EF173', '6B4829C2E5E2', '2761843E3DB6', '44A5406282B8', '4F822DEAEE6E', '5FB1A7C1D694', 'ED7F7B6E4C8E', '38B237B2F190', '0AA8E5C89F0F', 'F3A980C4613A', 'C4379D733479', '91A7412303BD', '6515C1CC0F57', 'AA4CF5537352', 'F6FFC6846BEC', 'D33CD8778DE7', '00F87647163F', 'A7036C8E5E6E', '2B39201EEEE0', '92A78941DF77', '8727FFFF8441', '71C6704EA7FD', 'DB0E754610DB', 'E1C5F6D592F4', '4F7E14B7D72F', '3DAE21FF789C', '8EDD5A7B2310', 'DE68828CC67E', '8A7B4B823C6E', '42B5C14AEDAA', 'F52B9A0882BB', 'C8D4EB858DA4', '1BA120ABF733', 'B0DB49483D7C', 'A3A00BA86199', 'D1A2D5B3B446', 'F8ABA40201AB', '4E07EED92E2E', 'C0E55316F183', '9986CAEDD2B7', 'E1DA1E513BE1', '18DFC8ABFA92', 'D88E55D44EAD', '0F2FF9C6091C', '00E3F86E3E6A', '7C236B88E0E6', '17B15F103D26', '86E5ADAE8206', '849BE041D1C8', '004AC288D833', '8A2EC5DE84D4', 'F6E5F858A19A', '2690D4B28188', 'ECE1DAD2A886', '66CF61A7F865', '8B7E97D3310D', '4EE35E322F9F', 'B74EC94193F8', '693503BD8CD6', 'C30B52D6E340', '179A2FE8AF05', '931D5E447FA7', '0299B6FC9E9C', '3862E2B8AD54', 'C76E52421ACB', 'F2D6B8E26072', '5A94F91C9AA4', '24AD94A6AB3E', 'DC76E46519A7', '15F149B4A87F', 'A932B7BEAC6A', '658428760D94', 'FD5AF33900D9', 'D0EEB2FA5D83', '0428DEB82659', '32E2949B0C66', '415672DB1873', '4510E0D4FDEE', '7044BE538016', '1F07CBB60940', 'ECAD3D8FAB97', '78F04BE79449', '996FD5290FB3', 'ABC648A86887', '1B2F469FBC51', '2388E2CAC922', 'A6405231F460', '3A9853B9390D', '4C1F76298E24', '08C05F2480E4', '5BED2E95A983', '6EE9F61F7CDA', 'D2851DC6AF41', 'BB705FBA29C7', '9114D5C32D36', 'E55640B069F5', '9BE033155063', 'CCF388E0F66C', '87258EA0A73C', '155CAAB3DC95', 'ABE45B3CCAD9', 'E7F67FEC3CA1', '38043B171DAC', '53B50E488C81', 'E0ED571AED2A', '7987F926DA46', '7C8C9A176F90', '40E7A7FDF48D', '3E95CBF3E357', 'C323ACC08DC7', '28DDECBB02BD', '9BC192B91D83', '759754F07F7F', 'E10D5E79B0A0', 'ECFAD2825EC6', '669C59404DC1', 'B4E8CF251A0D', 'A165288534B1', '3B067F759865', '7E7B3B164338', 'E22E187E1FF7', '9D0934987831', '4A58986914C8', '99D8B323FBB5', 'D815B0814A62', 'B867F487B9F3', 'A25945256A69', 'A885B3EA9C99', '44E1D37DCE9B', '504FFF4C522D', 'C1442695A931', '83CAAFEF8194', 'AB5D516FC4C3', '30687EEA6586', 'DE11D98D685E', 'C68A69F99946', 'C46CDD7E69A4', '710EFFD2AFE1', 'F2B82ED3AAFE', 'CA4E45151A4A', '4397809A2D94', '8E9007F863C7', '97E4E42863A3', '7348AB38D68E', '0D29A98FE8FB', '5312458F4FC2', '5E5678C3195A', 'DF9580284892', 'AB3053E08AEC', 'B150E66D6C5B', '193591CD2E86', '868D445794AB', '48C1CBC6D003', '3C7F30DAE13A', '09861D1CE966', '0FB32337D223', 'B9FFF4311016', '694238280A93', 'FC73A83CF59C', 'E6E97420C195', 'AB0F4F1767F8', '92C09304882D', 'AA6D6F1B1B54', '11F14A342E56', '743F1B934A96', 'E0D3580584ED', '3D613F2FF95F', '9EBA120196BC', 'F73ADEA6A74B', '958229D0AE9F', '899FE6A07507', '4C2FA98111DA', 'D7DF114BB271', 'D395BD24262F', 'A459492C73CE', '3901A0BB6B92', '54CB8D1DFDD0', '883E45198895', 'A99C834B122D', 'A3A08FDF9D3C', '434C1F133BA0', '5538A4817C01', 'CEFC37AFA236', 'B0D2190F7312', 'ADD3D2BDCC6B', '1A9B01EDB6C4', 'F320A6767646', 'AB59EE521967', 'A6527612255D', '7CF49F831E85', 'F71859A3B336', '4C2B18838D9D', '784E5F6EF6E1', 'F5152BBB18F6', '7D83374556C1', '5C503E0A2A01', 'C068F7168129', '506BF8E786CE', '6E6BCEE073C6', 'D698ED5E6B70', 'CEA8008D03D4', 'E286AD066204', 'F7E8813101B4', 'B39F2225875C', 'B57C860BE032', 'A817A13D651B', '6FB60C290F5A', 'F74B4B5496F0', '4354903A1D8A', 'C0916625AAB0', 'B36BDB586EEF', '363F69979EAB', '10D7845FC5D4', 'DCF72C5A4C3B', 'A5FF36F08845', '253F3DB00C5A', 'B7BC85276656', 'DBE16C234CF3', '466103CE7D5D', 'C66E655CF34A', 'FB7A0BB5DBE4', '144E2BFD59A3', 'DD861E42F43C', 'C74CBEFD0B9E', 'F1973E9DB4CF', '4BB688100D15', 'DECDC46197E4', '62EA05528EFF', '83AAAD315F57', '937CDD403137', 'A2237589F0FE', '9D9A5677B299', 'E125061E7C8A', '8A7F6AD46E4E', '98C855AF1F41', 'D47240EFB904', '2E86ACD1A766', 'E0641441E7B6', '8C928B2D6AB9', 'EC488F9F8910', '6E18981CEAAD', '871B7DB49A3F', '22FBB7ED7597', '656C7C849144', 'D97105156839', 'C5BAE31EFC62', '0C71097D3AE1', 'CE47E51746E3', '9E4100586EDC', '8F6F0CD4986A', '2E9563678CAA', '359A2C8A46C4', 'C80F42A2B53B', '11A4E36A48AF', '8B2B8A57D84B', '9ABAA03ACB4B', '19A9B6881B50', '3B7E7C18280B', '007812CC14B2', '3864A8B9C6B5', '37EA4D44939C', '9DB6D2ADE58A', 'EEB58B11D40C', '485E595428A2', '371C9C47A787', '89E73E0B8EF5', 'EC29A4F154B1', '759C3FAFA8BF', 'B2FF6A761EC9', 'F4C2261D0379', '31F19F915306', '84A0F6E847E0', '216054C99572', '7A0D5E468AB3', 'CF48D9415493', '6743F5B9985B', '51D70845DFA5', '83CEF7164BBB', '2A665076893F', 'EF478C3A8264', '1308F5C3347E', '2EEB19BB460B', 'D85A53759452', '12C119C37C62', '82D32B4D6852', '5C8E678EACDD', 'D8F3209E84CF', '505316DE125D', '1D374A0B26EA', 'B526BA896931', 'D8DBF197A624', '55664500EFAA', '8A4D99910F10', '8DA0993625FF', '229D813111B2', 'C2D8F1B0A984', '2657B9F9CCDC', 'D8C149300D31', '5CC94F3773B1', '555EC033AAE4', '37E07916B614', '0814426B27DF', 'BD300B07564D', '9B948850F6A8', '10F1ACB5559E', '95A6F680F078', '81B6F6345F55', 'E2F92BCD20E5', '7F1905176793', '8C003FEE68D7', '6F868B506EB5', 'AD0F893FA473', '20D81D1BE1A6', 'BE28ACA50D03', '6FF3C3A95428', '0527DB04DCC0', 'A9EBC5E40694', '99504CB04F5D', 'B082F1A267EE', '41BEF59908B1', '900A879708F0', 'EFAD20A5F4A0', '0F71E5F49E17', 'F0DF7FC426DB', '6A49432C3001', '672DB1A41215', '312860BEC6C5', 'B5C3DC4A876E', '04D4F9F350F6', '60772C514A37', 'AC0B87BB101A', '16364DA86C3D', 'F29E589C11F2', '54CCF07F322A', '977103A9DFE1', '5B1604AE437E', '8A1A6161F47C', '27DE9583F170', '07C7A31386A0', 'B7C3128E0973', '39B6908F453E', 'D2E2D4CCBF8C', 'CFE773F1600B', '527EB3C89F2F', 'A66B88F03338', 'B936994E140D', '497079209629', '102F13317092', 'C4ED4217DA09', '45BF01B7EF4D', '3BEFA845FC27', 'F166DA36A4A3', '8F33151F0334', 'DDA2A181F486', 'ACCD71550365'], 'df_val_eval':                  id  discourse_id  discourse_start  discourse_end  \\\n",
            "0      A8445CABFECE  1.622576e+12             18.0           85.0   \n",
            "1      A8445CABFECE  1.622576e+12             86.0          202.0   \n",
            "2      A8445CABFECE  1.622576e+12            203.0         1030.0   \n",
            "3      A8445CABFECE  1.622576e+12           1031.0         1243.0   \n",
            "4      A97DE0D49AEA  1.622645e+12             63.0          129.0   \n",
            "...             ...           ...              ...            ...   \n",
            "28992  0814426B27DF  1.617896e+12           1440.0         1563.0   \n",
            "28993  0814426B27DF  1.617896e+12           1564.0         1955.0   \n",
            "28994  0814426B27DF  1.617896e+12           1956.0         2003.0   \n",
            "28995  0814426B27DF  1.617896e+12           2004.0         2075.0   \n",
            "28996  0814426B27DF  1.617896e+12           2076.0         2359.0   \n",
            "\n",
            "                                          discourse_text  \\\n",
            "0      Drivers should not be able to use phones while...   \n",
            "1      Drivers who used their phone while operating a...   \n",
            "2      According to an article by the Edgar Snyder Fi...   \n",
            "3      In conclusion, drivers should not able to work...   \n",
            "4       Driver's should desist from using their Cell ...   \n",
            "...                                                  ...   \n",
            "28992  that one of the people you asked for advice ma...   \n",
            "28993  Some don't think all the same things if you we...   \n",
            "28994    Some people will disagree with my three reasons   \n",
            "28995  but I don't like to listen to the people who d...   \n",
            "28996  Maybe this helped you maybe it didn't.\\n\\nTo s...   \n",
            "\n",
            "             discourse_type      discourse_type_num  \\\n",
            "0                  Position              Position 1   \n",
            "1                     Claim                 Claim 1   \n",
            "2                  Evidence              Evidence 1   \n",
            "3      Concluding Statement  Concluding Statement 1   \n",
            "4                  Position              Position 1   \n",
            "...                     ...                     ...   \n",
            "28992                 Claim                 Claim 5   \n",
            "28993              Evidence              Evidence 3   \n",
            "28994          Counterclaim          Counterclaim 1   \n",
            "28995              Rebuttal              Rebuttal 1   \n",
            "28996  Concluding Statement  Concluding Statement 1   \n",
            "\n",
            "                                        predictionstring  \\\n",
            "0                           3 4 5 6 7 8 9 10 11 12 13 14   \n",
            "1      15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 3...   \n",
            "2      36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 5...   \n",
            "3      177 178 179 180 181 182 183 184 185 186 187 18...   \n",
            "4                          11 12 13 14 15 16 17 18 19 20   \n",
            "...                                                  ...   \n",
            "28992  290 291 292 293 294 295 296 297 298 299 300 30...   \n",
            "28993  315 316 317 318 319 320 321 322 323 324 325 32...   \n",
            "28994                    391 392 393 394 395 396 397 398   \n",
            "28995  399 400 401 402 403 404 405 406 407 408 409 41...   \n",
            "28996  413 414 415 416 417 418 419 420 421 422 423 42...   \n",
            "\n",
            "                                           text_by_index  new_start  new_end  \\\n",
            "0      Drivers should not be able to use phones while...         18       86   \n",
            "1      Drivers who used their phone while operating a...         86      202   \n",
            "2      According to an article by the Edgar Snyder Fi...        203     1030   \n",
            "3      In conclusion, drivers should not able to work...       1031     1231   \n",
            "4       Driver's should desist from using their Cell ...         64      129   \n",
            "...                                                  ...        ...      ...   \n",
            "28992  that one of the people you asked for advice ma...       1440     1563   \n",
            "28993  Some don't think all the same things if you we...       1564     1955   \n",
            "28994    Some people will disagree with my three reasons       1956     2004   \n",
            "28995  but I don't like to listen to the people who d...       2004     2075   \n",
            "28996  Maybe this helped you maybe it didn't.\\n\\nTo s...       2076     2359   \n",
            "\n",
            "                                       text_by_new_index  \\\n",
            "0      Drivers should not be able to use phones while...   \n",
            "1      Drivers who used their phone while operating a...   \n",
            "2      According to an article by the Edgar Snyder Fi...   \n",
            "3      In conclusion, drivers should not able to work...   \n",
            "4      Driver's should desist from using their Cell P...   \n",
            "...                                                  ...   \n",
            "28992  that one of the people you asked for advice ma...   \n",
            "28993  Some don't think all the same things if you we...   \n",
            "28994   Some people will disagree with my three reasons    \n",
            "28995  but I don't like to listen to the people who d...   \n",
            "28996  Maybe this helped you maybe it didn't.\\n\\nTo s...   \n",
            "\n",
            "                                    new_predictionstring  \n",
            "0                           3 4 5 6 7 8 9 10 11 12 13 14  \n",
            "1      15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 3...  \n",
            "2      36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 5...  \n",
            "3      177 178 179 180 181 182 183 184 185 186 187 18...  \n",
            "4                          11 12 13 14 15 16 17 18 19 20  \n",
            "...                                                  ...  \n",
            "28992  290 291 292 293 294 295 296 297 298 299 300 30...  \n",
            "28993  315 316 317 318 319 320 321 322 323 324 325 32...  \n",
            "28994                    391 392 393 394 395 396 397 398  \n",
            "28995  399 400 401 402 403 404 405 406 407 408 409 41...  \n",
            "28996  413 414 415 416 417 418 419 420 421 422 423 42...  \n",
            "\n",
            "[28997 rows x 13 columns], 'ds_val': <__main__.FeedbackPrizeDataset object at 0x7faf5a76b410>, 'dl_train': <torch.utils.data.dataloader.DataLoader object at 0x7faf5a5c8ad0>, 'dl_val': <torch.utils.data.dataloader.DataLoader object at 0x7faf5a5c8550>, 'best_val_loss': inf, 'criterion': CrossEntropyLoss(), 'epoch': 1, '_i29': 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i30': 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'KaggleDataset': <class '__main__.KaggleDataset'>, '_i31': 'try:\\n  print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i32': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i33': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'MAPPING': {'O': 0, 'B-Lead': 1, 'B-Position': 2, 'B-Evidence': 3, 'B-Claim': 4, 'B-Concluding Statement': 5, 'B-Counterclaim': 6, 'B-Rebuttal': 7, 'I-Lead': 8, 'I-Position': 9, 'I-Evidence': 10, 'I-Claim': 11, 'I-Concluding Statement': 12, 'I-Counterclaim': 13, 'I-Rebuttal': 14}, '_i34': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i35': 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        #if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', '_i36': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i37': \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        print(model_config)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", '_i38': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i39': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i40': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n            print(raw_logits.shape)\\n            print(word_ids.shape)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i41': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i42': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i43': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i44': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i45': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i46': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'model': FeedbackModel(\n",
            "  (backbone): LongformerModel(\n",
            "    (embeddings): LongformerEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): LongformerEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): LongformerPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.2, inplace=False)\n",
            "  (dropout3): Dropout(p=0.3, inplace=False)\n",
            "  (dropout4): Dropout(p=0.4, inplace=False)\n",
            "  (dropout5): Dropout(p=0.5, inplace=False)\n",
            "  (head): Linear(in_features=768, out_features=15, bias=True)\n",
            "), '_i47': 'try:\\n  print(gc.get_referrers(model))\\n  raise Exception()\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])'}, {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'ON_COLAB = True\\nif ON_COLAB:\\n  # Mount drive:\\n  from google.colab import drive, files\\n  # mount Google Drive\\n  drive.mount(\"/content/drive\")', \"get_ipython().run_cell_magic('capture', '', '# if on Colab, we need to install missing stuff!\\\\nif ON_COLAB:\\\\n  !pip install transformers\\\\n  !pip install iterative-stratification\\\\n  !pip install nvidia-ml-py3')\", 'import gc\\nimport os\\nimport torch\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport torch.nn as nn\\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\\n\\nfrom tqdm.notebook import tqdm\\nfrom sklearn.metrics import accuracy_score\\nfrom torch.cuda.amp import autocast, GradScaler\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold', 'def print_gpu_utilization():\\n    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\\n\\ndef get_gpu_utilization():\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    info = nvmlDeviceGetMemoryInfo(handle)\\n    return info.used//1024**2\\n\\ndef print_summary(result):\\n    print(f\"Time: {result.metrics[\\'train_runtime\\']:.2f}\")\\n    print(f\"Samples/second: {result.metrics[\\'train_samples_per_second\\']:.2f}\")\\n    print_gpu_utilization()\\n\\nprint_gpu_utilization()', \"if ON_COLAB:\\n  get_ipython().system('cd /content/drive/MyDrive/NLP_project')\\n\\n\\n# DATA DIR ---- TO CHANGE\\nDATA_DIR = 'drive/MyDrive/NLP_project/'\", \"class HyperParameters:\\n    \\n    # Here we choose model type. Can be changed for others\\n    name = 'longformer'\\n    model_savename = 'longformer'\\n    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\\n    \\n    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\\n    base_dir = DATA_DIR\\n    data_dir = os.path.join(base_dir, 'data')\\n    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\\n    model_dir = os.path.join(base_dir, f'model/{name}')\\n    output_dir = os.path.join(base_dir, f'output/{name}')\\n    \\n    # Training hyperparameters\\n    is_debug = False\\n    n_epoch = 2 # not to exceed runtime limit\\n    n_fold = 5\\n    verbose_steps = 500\\n    random_seed = 42\\n\\n    # Model specific hyperparameters\\n    max_length = 1024\\n    inference_max_length = 4096\\n    train_batch_size = 4\\n    valid_batch_size = 4\\n    lr = 4e-5\\n\\n    # Task hyperparameters\\n    num_labels = 15\\n    label_subtokens = True\\n    output_hidden_states = True\\n    hidden_dropout_prob = 0.1\\n    layer_norm_eps = 1e-7\\n    add_pooling_layer = False\\n    verbose_steps = 500\\n    if is_debug:\\n        debug_sample = 1000\\n        verbose_steps = 16\\n        n_epoch = 1\\n        n_fold = 2\\n\\nif not os.path.exists(HyperParameters.model_dir):\\n    get_ipython().system('mkdir $HyperParameters.model_dir')\", 'IGNORE_INDEX = -100\\nNON_LABEL = -1\\nOUTPUT_LABELS = [\\'O\\', \\'B-Lead\\', \\'I-Lead\\', \\'B-Position\\', \\'I-Position\\', \\'B-Claim\\', \\'I-Claim\\', \\'B-Counterclaim\\', \\'I-Counterclaim\\', \\n                 \\'B-Rebuttal\\', \\'I-Rebuttal\\', \\'B-Evidence\\', \\'I-Evidence\\', \\'B-Concluding Statement\\', \\'I-Concluding Statement\\']\\nLABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\\nIDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\\n\\nMIN_THRESH = {\\n    \"I-Lead\": 9,\\n    \"I-Position\": 5,\\n    \"I-Evidence\": 14,\\n    \"I-Claim\": 3,\\n    \"I-Concluding Statement\": 11,\\n    \"I-Counterclaim\": 6,\\n    \"I-Rebuttal\": 4,\\n}\\n\\nPROB_THRESH = {\\n    \"I-Lead\": 0.7,\\n    \"I-Position\": 0.55,\\n    \"I-Evidence\": 0.65,\\n    \"I-Claim\": 0.55,\\n    \"I-Concluding Statement\": 0.7,\\n    \"I-Counterclaim\": 0.5,\\n    \"I-Rebuttal\": 0.55,\\n}', \"def set_seed(seed=HyperParameters.random_seed):\\n    np.random.seed(seed)\\n    \\n    random.seed(seed)\\n    \\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    \\n    torch.backends.cudnn.deterministic =True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed()\\n\\n# Set proper device\\nif torch.cuda.is_available():\\n    device = torch.device('cuda')\\nelse:\\n    device = torch.device('cpu')\\n\\nprint(f'Using device: {device}')\", \"df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')\", 'def agg_essays(train_flg):\\n    \"\"\"\\n    Splits every word in an essay and adds the text of each essay to a dataframe.\\n    \"\"\"\\n    folder = \\'train\\' if train_flg else \\'test\\'\\n    names, texts =[], []\\n    for f in tqdm(list(os.listdir(f\\'{HyperParameters.data_dir}/{folder}\\'))):\\n        names.append(f.replace(\\'.txt\\', \\'\\'))\\n        texts.append(open(f\\'{HyperParameters.data_dir}/{folder}/\\' + f, \\'r\\').read())\\n        df_texts = pd.DataFrame({\\'id\\': names, \\'text\\': texts})\\n\\n    df_texts[\\'text_split\\'] = df_texts.text.str.split()\\n    print(\\'Completed tokenizing texts.\\')\\n    return df_texts', 'def ner(df_texts, df_train):\\n    \"\"\"\\n    Maps discourse type to each word of the text, according to the train.csv file.\\n    \"\"\"\\n    all_entities = []\\n    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\\n        total = len(row[\\'text_split\\'])\\n        entities = [\\'O\\'] * total\\n\\n        for _, row2 in df_train[df_train[\\'id\\'] == row[\\'id\\']].iterrows():\\n            discourse = row2[\\'discourse_type\\']\\n            list_ix = [int(x) for x in row2[\\'predictionstring\\'].split(\\' \\')]\\n            entities[list_ix[0]] = f\\'B-{discourse}\\'\\n            for k in list_ix[1:]: entities[k] = f\\'I-{discourse}\\'\\n        all_entities.append(entities)\\n\\n    df_texts[\\'entities\\'] = all_entities\\n    print(\\'Completed mapping discourse to each token.\\')\\n    return df_texts', 'def preprocess(df_train = None):\\n    \"\"\"\\n    Generates the dataframe we will use for training.\\n    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\\n    \"\"\"\\n    if df_train is None:\\n        train_flg = False\\n    else:\\n        train_flg = True\\n    \\n    df_texts = agg_essays(train_flg)\\n\\n    if train_flg:\\n        df_texts = ner(df_texts, df_train)\\n    return df_texts\\n\\n# Make sure we only run pre-processing if we did not do it in the past:\\n\\nif not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    alltrain_texts = preprocess(df_alltrain)\\n    test_texts = preprocess()\\nelse:\\n    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")', \"# Visualize preprocessing result:\\nparse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\\nalltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\\nalltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\\n\\nalltrain_texts.head()\", 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \\n    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\\n\\n    # Generate name for the dummy columns\\n    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\\n    # dfx is now only the dataset with dummy columns selected: don\\'t need to pass the data to do the splits\\n    dfx = dfx[dummy_cols]', 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Generate cross validation object\\n    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n    # Extract labels\\n    labels = [c for c in dfx.columns if c != \"id\"]\\n    dfx_labels = dfx[labels]\\n\\n    # Dummy kfold assignment\\n    dfx[\"kfold\"] = -1\\n\\n    # Split\\n    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\\n        print(len(trn_), len(val_))\\n        \\n        # Change the value of the kfold column at the validation index to the value of the fold\\n        # This will tell us when to use the current entry in the validation set\\n        dfx.loc[val_, \"kfold\"] = fold\\n\\n    # merge back to original dataframe\\n    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\\n    print(alltrain_texts.kfold.value_counts())\\n\\n    # Save so next time we import it directly\\n    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)', \"# need help with this\\nclass FeedbackPrizeDataset(Dataset):\\n    def __init__(self, dataframe, tokenizer, max_len, has_labels):\\n        self.len = len(dataframe)\\n        self.data = dataframe\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.has_labels = has_labels\\n    \\n    def __getitem__(self, index):\\n        text = self.data.text[index]\\n        encoding = self.tokenizer(\\n            text.split(),\\n            is_split_into_words = True,\\n            padding = 'max_length',\\n            truncation = True,\\n            max_length = self.max_len\\n        )\\n        word_ids = encoding.word_ids()\\n\\n        # targets\\n        if self.has_labels:\\n            word_labels = self.data.entities[index]\\n            prev_word_idx = None\\n            labels_ids = []\\n            for word_idx in word_ids:\\n                if word_idx is None:\\n                    labels_ids.append(IGNORE_INDEX)\\n                elif word_idx != prev_word_idx:\\n                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                else:\\n                    if HyperParameters.label_subtokens:\\n                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                    else:\\n                        labels_ids.append(IGNORE_INDEX)\\n                prev_word_idx = word_idx\\n            encoding['labels'] = labels_ids\\n        # convert to torch.tensor\\n        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\\n        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\\n        item['word_ids'] = torch.as_tensor(word_ids2)\\n        return item\\n\\n    def __len__(self):\\n        return self.len\", \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'def build_model_tokenizer():\\n    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\\n    model = FeedbackModel()\\n    return model, tokenizer', '# Need help with this: used in training to transform raw logits to labels needed\\ndef active_logits(raw_logits, word_ids):\\n    word_ids = word_ids.view(-1)\\n    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\\n    active_mask = active_mask != NON_LABEL\\n    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\\n    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\\n    active_logits = active_logits.view(-1, HyperParameters.num_labels) \\n    return active_logits\\n\\ndef active_labels(labels):\\n    active_mask = labels.view(-1) != IGNORE_INDEX\\n    active_labels = torch.masked_select(labels.view(-1), active_mask)\\n    return active_labels\\n\\ndef active_preds_prob(active_logits):\\n    active_preds = torch.argmax(active_logits, axis = 1)\\n    active_preds_prob, _ = torch.max(active_logits, axis = 1)\\n    return active_preds, active_preds_prob', 'def calculate_overlap(set_pred, set_gt):\\n    \"\"\"\\n    Calculates if the overlap between prediction and\\n    ground truth is enough fora potential True positive\\n    \"\"\"\\n    # Length of each and intersection\\n    try:\\n        len_gt = len(set_gt)\\n        len_pred = len(set_pred)\\n        inter = len(set_gt & set_pred)\\n        overlap_1 = inter / len_gt\\n        overlap_2 = inter/ len_pred\\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\\n    except:  # at least one of the input is NaN\\n        return False\\n\\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\\n    \"\"\"\\n    A function that scores for the kaggle\\n        Student Writing Competition\\n        \\n    Uses the steps in the evaluation page here:\\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\\n    \"\"\"\\n    gt_df = gt_df.loc[gt_df[\\'discourse_type\\'] == discourse_type, \\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df = pred_df.loc[pred_df[\\'class\\'] == discourse_type,\\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df[\\'pred_id\\'] = pred_df.index\\n    gt_df[\\'gt_id\\'] = gt_df.index\\n    pred_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in pred_df[\\'predictionstring\\']]\\n    gt_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in gt_df[\\'predictionstring\\']]\\n    \\n    # Step 1. all ground truths and predictions for a given class are compared.\\n    joined = pred_df.merge(gt_df,\\n                           left_on=\\'id\\',\\n                           right_on=\\'id\\',\\n                           how=\\'outer\\',\\n                           suffixes=(\\'_pred\\',\\'_gt\\')\\n                          )\\n    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \\n                                                     joined.predictionstring_gt)]\\n    \\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \\n    # and the overlap between the prediction and the ground truth >= 0.5,\\n    # the prediction is a match and considered a true positive.\\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\\n    # we don\\'t need to compute the match to compute the score\\n    TP = joined.loc[overlaps][\\'gt_id\\'].nunique()\\n\\n    # 3. Any unmatched ground truths are false negatives\\n    # and any unmatched predictions are false positives.\\n    TPandFP = len(pred_df)\\n    TPandFN = len(gt_df)\\n    \\n    #calc microf1\\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\\n    return my_f1_score\\n\\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\\n    \"\"\"\\n    Final helper function for model evaluation.\\n    \\n    Args:\\n    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: [\\'id\\',\\'class\\',\\'predictionstring\\']\\n    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\\n    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\\n    \\n    Returns:\\n    f1                      (float): F1 score of the model\\n    (optional) class_scores  (dict): Dictionary of per-class F1 score\\n    \"\"\"\\n    class_scores = {}\\n    for discourse_type in gt_df.discourse_type.unique():\\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\\n        class_scores[discourse_type] = class_score\\n    f1 = np.mean([v for v in class_scores.values()])\\n    if return_class_scores:\\n        return f1, class_scores\\n    return f1', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n            \\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\\n    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\\n    f1score =[]\\n    # classes = oof[\\'class\\'].unique()\\n    classes = [\\'Lead\\', \\'Position\\', \\'Claim\\',\\'Counterclaim\\', \\'Rebuttal\\',\\'Evidence\\',\\'Concluding Statement\\']\\n    print(f\"Validation F1 scores\")\\n\\n    for c in classes:\\n        pred_df = oof.loc[oof[\\'class\\'] == c].copy()\\n        gt_df = df_val_eval.loc[df_val_eval[\\'discourse_type\\'] == c].copy()\\n        f1 = score_feedback_comp(pred_df, gt_df)\\n        print(f\\' * {c:<10}: {f1:4f}\\')\\n        f1score.append(f1)\\n    f1avg = np.mean(f1score)\\n    print(f\\'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}\\')\\n    return valid_loss, oof', 'def inference(model, data_loader, criterion, valid_flg):\\n    stream = tqdm(data_loader)\\n    model.eval()\\n    \\n    valid_loss = 0\\n    valid_accuracy = 0\\n    all_logits = None\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch[\\'input_ids\\'].to(device, dtype = torch.long)\\n        mask = batch[\\'attention_mask\\'].to(device, dtype = torch.long)\\n        with torch.no_grad():\\n            raw_logits = model(input_ids=ids, mask = mask)\\n        del ids, mask\\n        \\n        word_ids = batch[\\'word_ids\\'].to(device, dtype = torch.long)\\n        logits = active_logits(raw_logits, word_ids)\\n        sf_logits = torch.softmax(logits, dim= -1)\\n        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\\n        if valid_flg:    \\n            raw_labels = batch[\\'labels\\'].to(device, dtype = torch.long)\\n            labels = active_labels(raw_labels)\\n            preds, preds_prob = active_preds_prob(sf_logits)\\n            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n            loss = criterion(logits, labels)\\n            valid_loss += loss.item()\\n        \\n        if batch_idx == 1:\\n            all_logits = sf_raw_logits.cpu().numpy()\\n        else:\\n            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\\n\\n    \\n    if valid_flg:        \\n        epoch_loss = valid_loss / batch_idx\\n        epoch_accuracy = valid_accuracy / batch_idx\\n    else:\\n        epoch_loss, epoch_accuracy = 0, 0\\n    return all_logits, epoch_loss, epoch_accuracy\\n\\n\\ndef preds_class_prob(all_logits, data_loader):\\n    print(\"predict target class and its probabilty\")\\n    final_predictions = []\\n    final_predictions_score = []\\n    stream = tqdm(data_loader)\\n    len_sample = all_logits.shape[0]\\n\\n    for batch_idx, batch in enumerate(stream, start=0):\\n        for minibatch_idx in range(HyperParameters.valid_batch_size):\\n            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\\n            if sample_idx > len_sample - 1 : break\\n            word_ids = batch[\\'word_ids\\'][minibatch_idx].numpy()\\n            predictions =[]\\n            predictions_prob = []\\n            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\\n            pred_score = np.max(all_logits[sample_idx], axis=1)\\n            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\\n            prev_word_idx = -1\\n            for idx, word_idx in enumerate(word_ids):\\n                if word_idx == -1:\\n                    pass\\n                elif word_idx != prev_word_idx:\\n                    predictions.append(pred_class_labels[idx])\\n                    predictions_prob.append(pred_score[idx])\\n                    prev_word_idx = word_idx\\n            final_predictions.append(predictions)\\n            final_predictions_score.append(predictions_prob)\\n    return final_predictions, final_predictions_score', 'def get_preds_onefold(model, df, dl, criterion, valid_flg):\\n    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n    all_preds, all_preds_prob = preds_class_prob(logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred, valid_loss, valid_acc\\n\\ndef get_preds_folds(model, df, dl, criterion, valid_flg=False):\\n    for i_fold in range(HyperParameters.n_fold):\\n        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\\n        print(f\"{model_filename} inference\")\\n        model = model.to(device)\\n        model.load_state_dict(torch.load(model_filename))\\n        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n        if i_fold == 0:\\n            avg_pred_logits = logits\\n        else:\\n            avg_pred_logits += logits\\n    avg_pred_logits /= HyperParameters.n_fold\\n    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred\\n\\ndef post_process_pred(df, all_preds, all_preds_prob):\\n    final_preds = []\\n    for i in range(len(df)):\\n        idx = df.id.values[i]\\n        pred = all_preds[i]\\n        pred_prob = all_preds_prob[i]\\n        j = 0\\n        while j < len(pred):\\n            cls = pred[j]\\n            if cls == \\'O\\': j += 1\\n            else: cls = cls.replace(\\'B\\', \\'I\\')\\n            end = j + 1\\n            while end < len(pred) and pred[end] == cls:\\n                end += 1\\n            if cls != \\'O\\' and cls !=\\'\\':\\n                avg_score = np.mean(pred_prob[j:end])\\n                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\\n                    final_preds.append((idx, cls.replace(\\'I-\\', \\'\\'), \\' \\'.join(map(str, list(range(j, end))))))\\n            j = end\\n    df_pred = pd.DataFrame(final_preds)\\n    df_pred.columns = [\\'id\\', \\'class\\', \\'new_predictionstring\\']\\n    return df_pred', 'print_gpu_utilization()', 'def pretty_size(size):\\n\\t\"\"\"Pretty prints a torch.Size object\"\"\"\\n\\tassert(isinstance(size, torch.Size))\\n\\treturn \" × \".join(map(str, size))\\n\\ndef dump_tensors(gpu_only=True):\\n\\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\\n\\timport gc\\n\\ttotal_size = 0\\n\\tfor obj in gc.get_objects():\\n\\t\\ttry:\\n\\t\\t\\tif torch.is_tensor(obj):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" pinned\" if obj.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  pretty_size(obj.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.numel()\\n\\t\\t\\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   type(obj.data).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" pinned\" if obj.data.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" grad\" if obj.requires_grad else \"\", \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" volatile\" if obj.volatile else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   pretty_size(obj.data.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.data.numel()\\n\\t\\texcept Exception as e:\\n\\t\\t\\tpass        \\n\\tprint(\"Total size:\", total_size)', 'dump_tensors()', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        #if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        print(model_config)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n            print(raw_logits.shape)\\n            print(word_ids.shape)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  raise Exception()\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])'], '_oh': {13:              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  }, '_dh': ['/content'], '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>, 'In': ['', 'ON_COLAB = True\\nif ON_COLAB:\\n  # Mount drive:\\n  from google.colab import drive, files\\n  # mount Google Drive\\n  drive.mount(\"/content/drive\")', \"get_ipython().run_cell_magic('capture', '', '# if on Colab, we need to install missing stuff!\\\\nif ON_COLAB:\\\\n  !pip install transformers\\\\n  !pip install iterative-stratification\\\\n  !pip install nvidia-ml-py3')\", 'import gc\\nimport os\\nimport torch\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport torch.nn as nn\\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\\n\\nfrom tqdm.notebook import tqdm\\nfrom sklearn.metrics import accuracy_score\\nfrom torch.cuda.amp import autocast, GradScaler\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold', 'def print_gpu_utilization():\\n    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\\n\\ndef get_gpu_utilization():\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    info = nvmlDeviceGetMemoryInfo(handle)\\n    return info.used//1024**2\\n\\ndef print_summary(result):\\n    print(f\"Time: {result.metrics[\\'train_runtime\\']:.2f}\")\\n    print(f\"Samples/second: {result.metrics[\\'train_samples_per_second\\']:.2f}\")\\n    print_gpu_utilization()\\n\\nprint_gpu_utilization()', \"if ON_COLAB:\\n  get_ipython().system('cd /content/drive/MyDrive/NLP_project')\\n\\n\\n# DATA DIR ---- TO CHANGE\\nDATA_DIR = 'drive/MyDrive/NLP_project/'\", \"class HyperParameters:\\n    \\n    # Here we choose model type. Can be changed for others\\n    name = 'longformer'\\n    model_savename = 'longformer'\\n    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\\n    \\n    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\\n    base_dir = DATA_DIR\\n    data_dir = os.path.join(base_dir, 'data')\\n    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\\n    model_dir = os.path.join(base_dir, f'model/{name}')\\n    output_dir = os.path.join(base_dir, f'output/{name}')\\n    \\n    # Training hyperparameters\\n    is_debug = False\\n    n_epoch = 2 # not to exceed runtime limit\\n    n_fold = 5\\n    verbose_steps = 500\\n    random_seed = 42\\n\\n    # Model specific hyperparameters\\n    max_length = 1024\\n    inference_max_length = 4096\\n    train_batch_size = 4\\n    valid_batch_size = 4\\n    lr = 4e-5\\n\\n    # Task hyperparameters\\n    num_labels = 15\\n    label_subtokens = True\\n    output_hidden_states = True\\n    hidden_dropout_prob = 0.1\\n    layer_norm_eps = 1e-7\\n    add_pooling_layer = False\\n    verbose_steps = 500\\n    if is_debug:\\n        debug_sample = 1000\\n        verbose_steps = 16\\n        n_epoch = 1\\n        n_fold = 2\\n\\nif not os.path.exists(HyperParameters.model_dir):\\n    get_ipython().system('mkdir $HyperParameters.model_dir')\", 'IGNORE_INDEX = -100\\nNON_LABEL = -1\\nOUTPUT_LABELS = [\\'O\\', \\'B-Lead\\', \\'I-Lead\\', \\'B-Position\\', \\'I-Position\\', \\'B-Claim\\', \\'I-Claim\\', \\'B-Counterclaim\\', \\'I-Counterclaim\\', \\n                 \\'B-Rebuttal\\', \\'I-Rebuttal\\', \\'B-Evidence\\', \\'I-Evidence\\', \\'B-Concluding Statement\\', \\'I-Concluding Statement\\']\\nLABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\\nIDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\\n\\nMIN_THRESH = {\\n    \"I-Lead\": 9,\\n    \"I-Position\": 5,\\n    \"I-Evidence\": 14,\\n    \"I-Claim\": 3,\\n    \"I-Concluding Statement\": 11,\\n    \"I-Counterclaim\": 6,\\n    \"I-Rebuttal\": 4,\\n}\\n\\nPROB_THRESH = {\\n    \"I-Lead\": 0.7,\\n    \"I-Position\": 0.55,\\n    \"I-Evidence\": 0.65,\\n    \"I-Claim\": 0.55,\\n    \"I-Concluding Statement\": 0.7,\\n    \"I-Counterclaim\": 0.5,\\n    \"I-Rebuttal\": 0.55,\\n}', \"def set_seed(seed=HyperParameters.random_seed):\\n    np.random.seed(seed)\\n    \\n    random.seed(seed)\\n    \\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    \\n    torch.backends.cudnn.deterministic =True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed()\\n\\n# Set proper device\\nif torch.cuda.is_available():\\n    device = torch.device('cuda')\\nelse:\\n    device = torch.device('cpu')\\n\\nprint(f'Using device: {device}')\", \"df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')\", 'def agg_essays(train_flg):\\n    \"\"\"\\n    Splits every word in an essay and adds the text of each essay to a dataframe.\\n    \"\"\"\\n    folder = \\'train\\' if train_flg else \\'test\\'\\n    names, texts =[], []\\n    for f in tqdm(list(os.listdir(f\\'{HyperParameters.data_dir}/{folder}\\'))):\\n        names.append(f.replace(\\'.txt\\', \\'\\'))\\n        texts.append(open(f\\'{HyperParameters.data_dir}/{folder}/\\' + f, \\'r\\').read())\\n        df_texts = pd.DataFrame({\\'id\\': names, \\'text\\': texts})\\n\\n    df_texts[\\'text_split\\'] = df_texts.text.str.split()\\n    print(\\'Completed tokenizing texts.\\')\\n    return df_texts', 'def ner(df_texts, df_train):\\n    \"\"\"\\n    Maps discourse type to each word of the text, according to the train.csv file.\\n    \"\"\"\\n    all_entities = []\\n    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\\n        total = len(row[\\'text_split\\'])\\n        entities = [\\'O\\'] * total\\n\\n        for _, row2 in df_train[df_train[\\'id\\'] == row[\\'id\\']].iterrows():\\n            discourse = row2[\\'discourse_type\\']\\n            list_ix = [int(x) for x in row2[\\'predictionstring\\'].split(\\' \\')]\\n            entities[list_ix[0]] = f\\'B-{discourse}\\'\\n            for k in list_ix[1:]: entities[k] = f\\'I-{discourse}\\'\\n        all_entities.append(entities)\\n\\n    df_texts[\\'entities\\'] = all_entities\\n    print(\\'Completed mapping discourse to each token.\\')\\n    return df_texts', 'def preprocess(df_train = None):\\n    \"\"\"\\n    Generates the dataframe we will use for training.\\n    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\\n    \"\"\"\\n    if df_train is None:\\n        train_flg = False\\n    else:\\n        train_flg = True\\n    \\n    df_texts = agg_essays(train_flg)\\n\\n    if train_flg:\\n        df_texts = ner(df_texts, df_train)\\n    return df_texts\\n\\n# Make sure we only run pre-processing if we did not do it in the past:\\n\\nif not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    alltrain_texts = preprocess(df_alltrain)\\n    test_texts = preprocess()\\nelse:\\n    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")', \"# Visualize preprocessing result:\\nparse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\\nalltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\\nalltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\\n\\nalltrain_texts.head()\", 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \\n    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\\n\\n    # Generate name for the dummy columns\\n    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\\n    # dfx is now only the dataset with dummy columns selected: don\\'t need to pass the data to do the splits\\n    dfx = dfx[dummy_cols]', 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Generate cross validation object\\n    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n    # Extract labels\\n    labels = [c for c in dfx.columns if c != \"id\"]\\n    dfx_labels = dfx[labels]\\n\\n    # Dummy kfold assignment\\n    dfx[\"kfold\"] = -1\\n\\n    # Split\\n    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\\n        print(len(trn_), len(val_))\\n        \\n        # Change the value of the kfold column at the validation index to the value of the fold\\n        # This will tell us when to use the current entry in the validation set\\n        dfx.loc[val_, \"kfold\"] = fold\\n\\n    # merge back to original dataframe\\n    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\\n    print(alltrain_texts.kfold.value_counts())\\n\\n    # Save so next time we import it directly\\n    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)', \"# need help with this\\nclass FeedbackPrizeDataset(Dataset):\\n    def __init__(self, dataframe, tokenizer, max_len, has_labels):\\n        self.len = len(dataframe)\\n        self.data = dataframe\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.has_labels = has_labels\\n    \\n    def __getitem__(self, index):\\n        text = self.data.text[index]\\n        encoding = self.tokenizer(\\n            text.split(),\\n            is_split_into_words = True,\\n            padding = 'max_length',\\n            truncation = True,\\n            max_length = self.max_len\\n        )\\n        word_ids = encoding.word_ids()\\n\\n        # targets\\n        if self.has_labels:\\n            word_labels = self.data.entities[index]\\n            prev_word_idx = None\\n            labels_ids = []\\n            for word_idx in word_ids:\\n                if word_idx is None:\\n                    labels_ids.append(IGNORE_INDEX)\\n                elif word_idx != prev_word_idx:\\n                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                else:\\n                    if HyperParameters.label_subtokens:\\n                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                    else:\\n                        labels_ids.append(IGNORE_INDEX)\\n                prev_word_idx = word_idx\\n            encoding['labels'] = labels_ids\\n        # convert to torch.tensor\\n        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\\n        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\\n        item['word_ids'] = torch.as_tensor(word_ids2)\\n        return item\\n\\n    def __len__(self):\\n        return self.len\", \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'def build_model_tokenizer():\\n    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\\n    model = FeedbackModel()\\n    return model, tokenizer', '# Need help with this: used in training to transform raw logits to labels needed\\ndef active_logits(raw_logits, word_ids):\\n    word_ids = word_ids.view(-1)\\n    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\\n    active_mask = active_mask != NON_LABEL\\n    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\\n    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\\n    active_logits = active_logits.view(-1, HyperParameters.num_labels) \\n    return active_logits\\n\\ndef active_labels(labels):\\n    active_mask = labels.view(-1) != IGNORE_INDEX\\n    active_labels = torch.masked_select(labels.view(-1), active_mask)\\n    return active_labels\\n\\ndef active_preds_prob(active_logits):\\n    active_preds = torch.argmax(active_logits, axis = 1)\\n    active_preds_prob, _ = torch.max(active_logits, axis = 1)\\n    return active_preds, active_preds_prob', 'def calculate_overlap(set_pred, set_gt):\\n    \"\"\"\\n    Calculates if the overlap between prediction and\\n    ground truth is enough fora potential True positive\\n    \"\"\"\\n    # Length of each and intersection\\n    try:\\n        len_gt = len(set_gt)\\n        len_pred = len(set_pred)\\n        inter = len(set_gt & set_pred)\\n        overlap_1 = inter / len_gt\\n        overlap_2 = inter/ len_pred\\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\\n    except:  # at least one of the input is NaN\\n        return False\\n\\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\\n    \"\"\"\\n    A function that scores for the kaggle\\n        Student Writing Competition\\n        \\n    Uses the steps in the evaluation page here:\\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\\n    \"\"\"\\n    gt_df = gt_df.loc[gt_df[\\'discourse_type\\'] == discourse_type, \\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df = pred_df.loc[pred_df[\\'class\\'] == discourse_type,\\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df[\\'pred_id\\'] = pred_df.index\\n    gt_df[\\'gt_id\\'] = gt_df.index\\n    pred_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in pred_df[\\'predictionstring\\']]\\n    gt_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in gt_df[\\'predictionstring\\']]\\n    \\n    # Step 1. all ground truths and predictions for a given class are compared.\\n    joined = pred_df.merge(gt_df,\\n                           left_on=\\'id\\',\\n                           right_on=\\'id\\',\\n                           how=\\'outer\\',\\n                           suffixes=(\\'_pred\\',\\'_gt\\')\\n                          )\\n    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \\n                                                     joined.predictionstring_gt)]\\n    \\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \\n    # and the overlap between the prediction and the ground truth >= 0.5,\\n    # the prediction is a match and considered a true positive.\\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\\n    # we don\\'t need to compute the match to compute the score\\n    TP = joined.loc[overlaps][\\'gt_id\\'].nunique()\\n\\n    # 3. Any unmatched ground truths are false negatives\\n    # and any unmatched predictions are false positives.\\n    TPandFP = len(pred_df)\\n    TPandFN = len(gt_df)\\n    \\n    #calc microf1\\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\\n    return my_f1_score\\n\\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\\n    \"\"\"\\n    Final helper function for model evaluation.\\n    \\n    Args:\\n    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: [\\'id\\',\\'class\\',\\'predictionstring\\']\\n    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\\n    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\\n    \\n    Returns:\\n    f1                      (float): F1 score of the model\\n    (optional) class_scores  (dict): Dictionary of per-class F1 score\\n    \"\"\"\\n    class_scores = {}\\n    for discourse_type in gt_df.discourse_type.unique():\\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\\n        class_scores[discourse_type] = class_score\\n    f1 = np.mean([v for v in class_scores.values()])\\n    if return_class_scores:\\n        return f1, class_scores\\n    return f1', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n            \\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\\n    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\\n    f1score =[]\\n    # classes = oof[\\'class\\'].unique()\\n    classes = [\\'Lead\\', \\'Position\\', \\'Claim\\',\\'Counterclaim\\', \\'Rebuttal\\',\\'Evidence\\',\\'Concluding Statement\\']\\n    print(f\"Validation F1 scores\")\\n\\n    for c in classes:\\n        pred_df = oof.loc[oof[\\'class\\'] == c].copy()\\n        gt_df = df_val_eval.loc[df_val_eval[\\'discourse_type\\'] == c].copy()\\n        f1 = score_feedback_comp(pred_df, gt_df)\\n        print(f\\' * {c:<10}: {f1:4f}\\')\\n        f1score.append(f1)\\n    f1avg = np.mean(f1score)\\n    print(f\\'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}\\')\\n    return valid_loss, oof', 'def inference(model, data_loader, criterion, valid_flg):\\n    stream = tqdm(data_loader)\\n    model.eval()\\n    \\n    valid_loss = 0\\n    valid_accuracy = 0\\n    all_logits = None\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch[\\'input_ids\\'].to(device, dtype = torch.long)\\n        mask = batch[\\'attention_mask\\'].to(device, dtype = torch.long)\\n        with torch.no_grad():\\n            raw_logits = model(input_ids=ids, mask = mask)\\n        del ids, mask\\n        \\n        word_ids = batch[\\'word_ids\\'].to(device, dtype = torch.long)\\n        logits = active_logits(raw_logits, word_ids)\\n        sf_logits = torch.softmax(logits, dim= -1)\\n        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\\n        if valid_flg:    \\n            raw_labels = batch[\\'labels\\'].to(device, dtype = torch.long)\\n            labels = active_labels(raw_labels)\\n            preds, preds_prob = active_preds_prob(sf_logits)\\n            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n            loss = criterion(logits, labels)\\n            valid_loss += loss.item()\\n        \\n        if batch_idx == 1:\\n            all_logits = sf_raw_logits.cpu().numpy()\\n        else:\\n            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\\n\\n    \\n    if valid_flg:        \\n        epoch_loss = valid_loss / batch_idx\\n        epoch_accuracy = valid_accuracy / batch_idx\\n    else:\\n        epoch_loss, epoch_accuracy = 0, 0\\n    return all_logits, epoch_loss, epoch_accuracy\\n\\n\\ndef preds_class_prob(all_logits, data_loader):\\n    print(\"predict target class and its probabilty\")\\n    final_predictions = []\\n    final_predictions_score = []\\n    stream = tqdm(data_loader)\\n    len_sample = all_logits.shape[0]\\n\\n    for batch_idx, batch in enumerate(stream, start=0):\\n        for minibatch_idx in range(HyperParameters.valid_batch_size):\\n            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\\n            if sample_idx > len_sample - 1 : break\\n            word_ids = batch[\\'word_ids\\'][minibatch_idx].numpy()\\n            predictions =[]\\n            predictions_prob = []\\n            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\\n            pred_score = np.max(all_logits[sample_idx], axis=1)\\n            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\\n            prev_word_idx = -1\\n            for idx, word_idx in enumerate(word_ids):\\n                if word_idx == -1:\\n                    pass\\n                elif word_idx != prev_word_idx:\\n                    predictions.append(pred_class_labels[idx])\\n                    predictions_prob.append(pred_score[idx])\\n                    prev_word_idx = word_idx\\n            final_predictions.append(predictions)\\n            final_predictions_score.append(predictions_prob)\\n    return final_predictions, final_predictions_score', 'def get_preds_onefold(model, df, dl, criterion, valid_flg):\\n    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n    all_preds, all_preds_prob = preds_class_prob(logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred, valid_loss, valid_acc\\n\\ndef get_preds_folds(model, df, dl, criterion, valid_flg=False):\\n    for i_fold in range(HyperParameters.n_fold):\\n        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\\n        print(f\"{model_filename} inference\")\\n        model = model.to(device)\\n        model.load_state_dict(torch.load(model_filename))\\n        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n        if i_fold == 0:\\n            avg_pred_logits = logits\\n        else:\\n            avg_pred_logits += logits\\n    avg_pred_logits /= HyperParameters.n_fold\\n    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred\\n\\ndef post_process_pred(df, all_preds, all_preds_prob):\\n    final_preds = []\\n    for i in range(len(df)):\\n        idx = df.id.values[i]\\n        pred = all_preds[i]\\n        pred_prob = all_preds_prob[i]\\n        j = 0\\n        while j < len(pred):\\n            cls = pred[j]\\n            if cls == \\'O\\': j += 1\\n            else: cls = cls.replace(\\'B\\', \\'I\\')\\n            end = j + 1\\n            while end < len(pred) and pred[end] == cls:\\n                end += 1\\n            if cls != \\'O\\' and cls !=\\'\\':\\n                avg_score = np.mean(pred_prob[j:end])\\n                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\\n                    final_preds.append((idx, cls.replace(\\'I-\\', \\'\\'), \\' \\'.join(map(str, list(range(j, end))))))\\n            j = end\\n    df_pred = pd.DataFrame(final_preds)\\n    df_pred.columns = [\\'id\\', \\'class\\', \\'new_predictionstring\\']\\n    return df_pred', 'print_gpu_utilization()', 'def pretty_size(size):\\n\\t\"\"\"Pretty prints a torch.Size object\"\"\"\\n\\tassert(isinstance(size, torch.Size))\\n\\treturn \" × \".join(map(str, size))\\n\\ndef dump_tensors(gpu_only=True):\\n\\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\\n\\timport gc\\n\\ttotal_size = 0\\n\\tfor obj in gc.get_objects():\\n\\t\\ttry:\\n\\t\\t\\tif torch.is_tensor(obj):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" pinned\" if obj.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  pretty_size(obj.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.numel()\\n\\t\\t\\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   type(obj.data).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" pinned\" if obj.data.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" grad\" if obj.requires_grad else \"\", \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" volatile\" if obj.volatile else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   pretty_size(obj.data.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.data.numel()\\n\\t\\texcept Exception as e:\\n\\t\\t\\tpass        \\n\\tprint(\"Total size:\", total_size)', 'dump_tensors()', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        #if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        print(model_config)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n            print(raw_logits.shape)\\n            print(word_ids.shape)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'try:\\n  print(gc.get_referrers(model))\\n  raise Exception()\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])'], 'Out': {13:              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  }, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fb1405eccd0>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fb13d36efd0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fb13d36efd0>, '_':              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  , '__': '', '___': '', '_i': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_ii': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_iii': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i1': 'ON_COLAB = True\\nif ON_COLAB:\\n  # Mount drive:\\n  from google.colab import drive, files\\n  # mount Google Drive\\n  drive.mount(\"/content/drive\")', 'ON_COLAB': True, 'drive': <module 'google.colab.drive' from '/usr/local/lib/python3.7/dist-packages/google/colab/drive.py'>, 'files': <module 'google.colab.files' from '/usr/local/lib/python3.7/dist-packages/google/colab/files.py'>, '_i2': '%%capture\\n# if on Colab, we need to install missing stuff!\\nif ON_COLAB:\\n  !pip install transformers\\n  !pip install iterative-stratification\\n  !pip install nvidia-ml-py3', '_exit_code': 0, '_i3': 'import gc\\nimport os\\nimport torch\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport torch.nn as nn\\nfrom pynvml import nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlInit\\n\\nfrom tqdm.notebook import tqdm\\nfrom sklearn.metrics import accuracy_score\\nfrom torch.cuda.amp import autocast, GradScaler\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold', 'gc': <module 'gc' (built-in)>, 'os': <module 'os' from '/usr/lib/python3.7/os.py'>, 'torch': <module 'torch' from '/usr/local/lib/python3.7/dist-packages/torch/__init__.py'>, 'random': <module 'random' from '/usr/lib/python3.7/random.py'>, 'np': <module 'numpy' from '/usr/local/lib/python3.7/dist-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/usr/local/lib/python3.7/dist-packages/pandas/__init__.py'>, 'nn': <module 'torch.nn' from '/usr/local/lib/python3.7/dist-packages/torch/nn/__init__.py'>, 'nvmlDeviceGetHandleByIndex': <function nvmlDeviceGetHandleByIndex at 0x7fb02a320b00>, 'nvmlDeviceGetMemoryInfo': <function nvmlDeviceGetMemoryInfo at 0x7fb02a32f320>, 'nvmlInit': <function nvmlInit at 0x7fb02a399f80>, 'tqdm': <class 'tqdm.notebook.tqdm_notebook'>, 'accuracy_score': <function accuracy_score at 0x7fb01bc59680>, 'autocast': <class 'torch.cuda.amp.autocast_mode.autocast'>, 'GradScaler': <class 'torch.cuda.amp.grad_scaler.GradScaler'>, 'Dataset': <class 'torch.utils.data.dataset.Dataset'>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'AutoConfig': <class 'transformers.models.auto.configuration_auto.AutoConfig'>, 'AutoModel': <class 'transformers.models.auto.modeling_auto.AutoModel'>, 'AutoTokenizer': <class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>, 'MultilabelStratifiedKFold': <class 'iterstrat.ml_stratifiers.MultilabelStratifiedKFold'>, '_i4': 'def print_gpu_utilization():\\n    print(f\"GPU memory occupied: {get_gpu_utilization()} MB.\")\\n\\ndef get_gpu_utilization():\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    info = nvmlDeviceGetMemoryInfo(handle)\\n    return info.used//1024**2\\n\\ndef print_summary(result):\\n    print(f\"Time: {result.metrics[\\'train_runtime\\']:.2f}\")\\n    print(f\"Samples/second: {result.metrics[\\'train_samples_per_second\\']:.2f}\")\\n    print_gpu_utilization()\\n\\nprint_gpu_utilization()', 'print_gpu_utilization': <function print_gpu_utilization at 0x7fb018ee60e0>, 'get_gpu_utilization': <function get_gpu_utilization at 0x7fb018ea1680>, 'print_summary': <function print_summary at 0x7fb02a36a050>, '_i5': \"if ON_COLAB:\\n  !cd /content/drive/MyDrive/NLP_project\\n\\n\\n# DATA DIR ---- TO CHANGE\\nDATA_DIR = 'drive/MyDrive/NLP_project/'\", 'DATA_DIR': 'drive/MyDrive/NLP_project/', '_i6': \"class HyperParameters:\\n    \\n    # Here we choose model type. Can be changed for others\\n    name = 'longformer'\\n    model_savename = 'longformer'\\n    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\\n    \\n    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\\n    base_dir = DATA_DIR\\n    data_dir = os.path.join(base_dir, 'data')\\n    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\\n    model_dir = os.path.join(base_dir, f'model/{name}')\\n    output_dir = os.path.join(base_dir, f'output/{name}')\\n    \\n    # Training hyperparameters\\n    is_debug = False\\n    n_epoch = 2 # not to exceed runtime limit\\n    n_fold = 5\\n    verbose_steps = 500\\n    random_seed = 42\\n\\n    # Model specific hyperparameters\\n    max_length = 1024\\n    inference_max_length = 4096\\n    train_batch_size = 4\\n    valid_batch_size = 4\\n    lr = 4e-5\\n\\n    # Task hyperparameters\\n    num_labels = 15\\n    label_subtokens = True\\n    output_hidden_states = True\\n    hidden_dropout_prob = 0.1\\n    layer_norm_eps = 1e-7\\n    add_pooling_layer = False\\n    verbose_steps = 500\\n    if is_debug:\\n        debug_sample = 1000\\n        verbose_steps = 16\\n        n_epoch = 1\\n        n_fold = 2\\n\\nif not os.path.exists(HyperParameters.model_dir):\\n    !mkdir $HyperParameters.model_dir\", 'HyperParameters': <class '__main__.HyperParameters'>, '_i7': 'IGNORE_INDEX = -100\\nNON_LABEL = -1\\nOUTPUT_LABELS = [\\'O\\', \\'B-Lead\\', \\'I-Lead\\', \\'B-Position\\', \\'I-Position\\', \\'B-Claim\\', \\'I-Claim\\', \\'B-Counterclaim\\', \\'I-Counterclaim\\', \\n                 \\'B-Rebuttal\\', \\'I-Rebuttal\\', \\'B-Evidence\\', \\'I-Evidence\\', \\'B-Concluding Statement\\', \\'I-Concluding Statement\\']\\nLABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\\nIDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\\n\\nMIN_THRESH = {\\n    \"I-Lead\": 9,\\n    \"I-Position\": 5,\\n    \"I-Evidence\": 14,\\n    \"I-Claim\": 3,\\n    \"I-Concluding Statement\": 11,\\n    \"I-Counterclaim\": 6,\\n    \"I-Rebuttal\": 4,\\n}\\n\\nPROB_THRESH = {\\n    \"I-Lead\": 0.7,\\n    \"I-Position\": 0.55,\\n    \"I-Evidence\": 0.65,\\n    \"I-Claim\": 0.55,\\n    \"I-Concluding Statement\": 0.7,\\n    \"I-Counterclaim\": 0.5,\\n    \"I-Rebuttal\": 0.55,\\n}', 'IGNORE_INDEX': -100, 'NON_LABEL': -1, 'OUTPUT_LABELS': ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement'], 'LABELS_TO_IDS': {'O': 0, 'B-Lead': 1, 'I-Lead': 2, 'B-Position': 3, 'I-Position': 4, 'B-Claim': 5, 'I-Claim': 6, 'B-Counterclaim': 7, 'I-Counterclaim': 8, 'B-Rebuttal': 9, 'I-Rebuttal': 10, 'B-Evidence': 11, 'I-Evidence': 12, 'B-Concluding Statement': 13, 'I-Concluding Statement': 14}, 'IDS_TO_LABELS': {0: 'O', 1: 'B-Lead', 2: 'I-Lead', 3: 'B-Position', 4: 'I-Position', 5: 'B-Claim', 6: 'I-Claim', 7: 'B-Counterclaim', 8: 'I-Counterclaim', 9: 'B-Rebuttal', 10: 'I-Rebuttal', 11: 'B-Evidence', 12: 'I-Evidence', 13: 'B-Concluding Statement', 14: 'I-Concluding Statement'}, 'MIN_THRESH': {'I-Lead': 9, 'I-Position': 5, 'I-Evidence': 14, 'I-Claim': 3, 'I-Concluding Statement': 11, 'I-Counterclaim': 6, 'I-Rebuttal': 4}, 'PROB_THRESH': {'I-Lead': 0.7, 'I-Position': 0.55, 'I-Evidence': 0.65, 'I-Claim': 0.55, 'I-Concluding Statement': 0.7, 'I-Counterclaim': 0.5, 'I-Rebuttal': 0.55}, '_i8': \"def set_seed(seed=HyperParameters.random_seed):\\n    np.random.seed(seed)\\n    \\n    random.seed(seed)\\n    \\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    \\n    torch.backends.cudnn.deterministic =True\\n    torch.backends.cudnn.benchmark = False\\n\\nset_seed()\\n\\n# Set proper device\\nif torch.cuda.is_available():\\n    device = torch.device('cuda')\\nelse:\\n    device = torch.device('cpu')\\n\\nprint(f'Using device: {device}')\", 'set_seed': <function set_seed at 0x7fb018ec60e0>, 'device': device(type='cuda'), '_i9': \"df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')\", 'df_alltrain':                   id  discourse_id  discourse_start  discourse_end  \\\n",
            "0       423A1CA112E2  1.622628e+12              8.0          229.0   \n",
            "1       423A1CA112E2  1.622628e+12            230.0          312.0   \n",
            "2       423A1CA112E2  1.622628e+12            313.0          401.0   \n",
            "3       423A1CA112E2  1.622628e+12            402.0          758.0   \n",
            "4       423A1CA112E2  1.622628e+12            759.0          886.0   \n",
            "...              ...           ...              ...            ...   \n",
            "144288  4C471936CD75  1.618153e+12           2234.0         3203.0   \n",
            "144289  4C471936CD75  1.618153e+12           3221.0         4509.0   \n",
            "144290  4C471936CD75  1.618025e+12           4510.0         4570.0   \n",
            "144291  4C471936CD75  1.618025e+12           4570.0         4922.0   \n",
            "144292  4C471936CD75  1.618025e+12           4935.0         5825.0   \n",
            "\n",
            "                                           discourse_text  \\\n",
            "0       Modern humans today are always on their phone....   \n",
            "1       They are some really bad consequences when stu...   \n",
            "2       Some certain areas in the United States ban ph...   \n",
            "3       When people have phones, they know about certa...   \n",
            "4       Driving is one of the way how to get around. P...   \n",
            "...                                                   ...   \n",
            "144288   if I'm not sure what college I want to attend...   \n",
            "144289   seeking multiple opinions before making a har...   \n",
            "144290  it is better to seek multiple opinions instead...   \n",
            "144291  The impact of asking people to help you make a...   \n",
            "144292  there are many other reasons one might want to...   \n",
            "\n",
            "              discourse_type      discourse_type_num  \\\n",
            "0                       Lead                  Lead 1   \n",
            "1                   Position              Position 1   \n",
            "2                   Evidence              Evidence 1   \n",
            "3                   Evidence              Evidence 2   \n",
            "4                      Claim                 Claim 1   \n",
            "...                      ...                     ...   \n",
            "144288              Evidence              Evidence 2   \n",
            "144289              Evidence              Evidence 3   \n",
            "144290              Position              Position 1   \n",
            "144291              Evidence              Evidence 4   \n",
            "144292  Concluding Statement  Concluding Statement 1   \n",
            "\n",
            "                                         predictionstring  \\\n",
            "0       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
            "1            45 46 47 48 49 50 51 52 53 54 55 56 57 58 59   \n",
            "2         60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75   \n",
            "3       76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...   \n",
            "4       139 140 141 142 143 144 145 146 147 148 149 15...   \n",
            "...                                                   ...   \n",
            "144288  386 387 388 389 390 391 392 393 394 395 396 39...   \n",
            "144289  576 577 578 579 580 581 582 583 584 585 586 58...   \n",
            "144290        828 829 830 831 832 833 834 835 836 837 838   \n",
            "144291  839 840 841 842 843 844 845 846 847 848 849 85...   \n",
            "144292  905 906 907 908 909 910 911 912 913 914 915 91...   \n",
            "\n",
            "                                            text_by_index  new_start  new_end  \\\n",
            "0       Modern humans today are always on their phone....          8      230   \n",
            "1       They are some really bad consequences when stu...        230      313   \n",
            "2       Some certain areas in the United States ban ph...        313      401   \n",
            "3       When people have phones, they know about certa...        402      758   \n",
            "4       Driving is one of the way how to get around. P...        759      887   \n",
            "...                                                   ...        ...      ...   \n",
            "144288   if I'm not sure what college I want to attend...       2235     3203   \n",
            "144289   seeking multiple opinions before making a har...       3222     4510   \n",
            "144290  it is better to seek multiple opinions instead...       4510     4570   \n",
            "144291  The impact of asking people to help you make a...       4570     4923   \n",
            "144292  there are many other reasons one might want to...       4935     5748   \n",
            "\n",
            "                                        text_by_new_index  \\\n",
            "0       Modern humans today are always on their phone....   \n",
            "1       They are some really bad consequences when stu...   \n",
            "2       Some certain areas in the United States ban ph...   \n",
            "3       When people have phones, they know about certa...   \n",
            "4       Driving is one of the way how to get around. P...   \n",
            "...                                                   ...   \n",
            "144288  if I'm not sure what college I want to attend,...   \n",
            "144289  seeking multiple opinions before making a hard...   \n",
            "144290  it is better to seek multiple opinions instead...   \n",
            "144291  The impact of asking people to help you make a...   \n",
            "144292  there are many other reasons one might want to...   \n",
            "\n",
            "                                     new_predictionstring  \n",
            "0       1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
            "1            45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
            "2         60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
            "3       76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
            "4       139 140 141 142 143 144 145 146 147 148 149 15...  \n",
            "...                                                   ...  \n",
            "144288  386 387 388 389 390 391 392 393 394 395 396 39...  \n",
            "144289  576 577 578 579 580 581 582 583 584 585 586 58...  \n",
            "144290        828 829 830 831 832 833 834 835 836 837 838  \n",
            "144291  839 840 841 842 843 844 845 846 847 848 849 85...  \n",
            "144292  905 906 907 908 909 910 911 912 913 914 915 91...  \n",
            "\n",
            "[144293 rows x 13 columns], '_i10': 'def agg_essays(train_flg):\\n    \"\"\"\\n    Splits every word in an essay and adds the text of each essay to a dataframe.\\n    \"\"\"\\n    folder = \\'train\\' if train_flg else \\'test\\'\\n    names, texts =[], []\\n    for f in tqdm(list(os.listdir(f\\'{HyperParameters.data_dir}/{folder}\\'))):\\n        names.append(f.replace(\\'.txt\\', \\'\\'))\\n        texts.append(open(f\\'{HyperParameters.data_dir}/{folder}/\\' + f, \\'r\\').read())\\n        df_texts = pd.DataFrame({\\'id\\': names, \\'text\\': texts})\\n\\n    df_texts[\\'text_split\\'] = df_texts.text.str.split()\\n    print(\\'Completed tokenizing texts.\\')\\n    return df_texts', 'agg_essays': <function agg_essays at 0x7fb018ea8440>, '_i11': 'def ner(df_texts, df_train):\\n    \"\"\"\\n    Maps discourse type to each word of the text, according to the train.csv file.\\n    \"\"\"\\n    all_entities = []\\n    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\\n        total = len(row[\\'text_split\\'])\\n        entities = [\\'O\\'] * total\\n\\n        for _, row2 in df_train[df_train[\\'id\\'] == row[\\'id\\']].iterrows():\\n            discourse = row2[\\'discourse_type\\']\\n            list_ix = [int(x) for x in row2[\\'predictionstring\\'].split(\\' \\')]\\n            entities[list_ix[0]] = f\\'B-{discourse}\\'\\n            for k in list_ix[1:]: entities[k] = f\\'I-{discourse}\\'\\n        all_entities.append(entities)\\n\\n    df_texts[\\'entities\\'] = all_entities\\n    print(\\'Completed mapping discourse to each token.\\')\\n    return df_texts', 'ner': <function ner at 0x7fb010272200>, '_i12': 'def preprocess(df_train = None):\\n    \"\"\"\\n    Generates the dataframe we will use for training.\\n    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\\n    \"\"\"\\n    if df_train is None:\\n        train_flg = False\\n    else:\\n        train_flg = True\\n    \\n    df_texts = agg_essays(train_flg)\\n\\n    if train_flg:\\n        df_texts = ner(df_texts, df_train)\\n    return df_texts\\n\\n# Make sure we only run pre-processing if we did not do it in the past:\\n\\nif not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    alltrain_texts = preprocess(df_alltrain)\\n    test_texts = preprocess()\\nelse:\\n    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")', 'preprocess': <function preprocess at 0x7fb010272320>, 'alltrain_texts':                  id                                               text  \\\n",
            "0      3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1      DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2      2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3      EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4      A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "...             ...                                                ...   \n",
            "15589  1C899F124FEB  While some students may think it's a beneficia...   \n",
            "15590  4453444AF383  There has been a strong arguement going on wea...   \n",
            "15591  EF0D75BF48DA  I favor in to changing election by popular vot...   \n",
            "15592  8FFDA5B9D359  Do you think students would benefit from being...   \n",
            "15593  ACAB1FCA0A30  I would like to change the election for the pr...   \n",
            "\n",
            "                                              text_split  \\\n",
            "0      [I, do, agree, that, some, students, would, be...   \n",
            "1      [Should, students, design, a, summer, project,...   \n",
            "2      [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3      [People, sometimes, have, a, different, opinio...   \n",
            "4      [Dear, senator,, As, you, know, the, Electoral...   \n",
            "...                                                  ...   \n",
            "15589  [While, some, students, may, think, it's, a, b...   \n",
            "15590  [There, has, been, a, strong, arguement, going...   \n",
            "15591  [I, favor, in, to, changing, election, by, pop...   \n",
            "15592  [Do, you, think, students, would, benefit, fro...   \n",
            "15593  [I, would, like, to, change, the, election, fo...   \n",
            "\n",
            "                                                entities  kfold  \n",
            "0      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1      [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2      [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4      [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  \n",
            "...                                                  ...    ...  \n",
            "15589  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "15590  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "15591  [B-Position, I-Position, I-Position, I-Positio...      3  \n",
            "15592  [B-Position, I-Position, I-Position, I-Positio...      2  \n",
            "15593  [B-Position, I-Position, I-Position, I-Positio...      4  \n",
            "\n",
            "[15594 rows x 5 columns], '_i13': \"# Visualize preprocessing result:\\nparse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\\nalltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\\nalltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\\n\\nalltrain_texts.head()\", 'parse_string': <function <lambda> at 0x7fb010272830>, '_13':              id                                               text  \\\n",
            "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "\n",
            "                                          text_split  \\\n",
            "0  [I, do, agree, that, some, students, would, be...   \n",
            "1  [Should, students, design, a, summer, project,...   \n",
            "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "3  [People, sometimes, have, a, different, opinio...   \n",
            "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
            "\n",
            "                                            entities  kfold  \n",
            "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
            "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
            "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
            "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  , '_i14': 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \\n    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\\n\\n    # Generate name for the dummy columns\\n    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\\n    # dfx is now only the dataset with dummy columns selected: don\\'t need to pass the data to do the splits\\n    dfx = dfx[dummy_cols]', '_i15': 'if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \\n    # Generate cross validation object\\n    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n    # Extract labels\\n    labels = [c for c in dfx.columns if c != \"id\"]\\n    dfx_labels = dfx[labels]\\n\\n    # Dummy kfold assignment\\n    dfx[\"kfold\"] = -1\\n\\n    # Split\\n    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\\n        print(len(trn_), len(val_))\\n        \\n        # Change the value of the kfold column at the validation index to the value of the fold\\n        # This will tell us when to use the current entry in the validation set\\n        dfx.loc[val_, \"kfold\"] = fold\\n\\n    # merge back to original dataframe\\n    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\\n    print(alltrain_texts.kfold.value_counts())\\n\\n    # Save so next time we import it directly\\n    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)', '_i16': \"# need help with this\\nclass FeedbackPrizeDataset(Dataset):\\n    def __init__(self, dataframe, tokenizer, max_len, has_labels):\\n        self.len = len(dataframe)\\n        self.data = dataframe\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.has_labels = has_labels\\n    \\n    def __getitem__(self, index):\\n        text = self.data.text[index]\\n        encoding = self.tokenizer(\\n            text.split(),\\n            is_split_into_words = True,\\n            padding = 'max_length',\\n            truncation = True,\\n            max_length = self.max_len\\n        )\\n        word_ids = encoding.word_ids()\\n\\n        # targets\\n        if self.has_labels:\\n            word_labels = self.data.entities[index]\\n            prev_word_idx = None\\n            labels_ids = []\\n            for word_idx in word_ids:\\n                if word_idx is None:\\n                    labels_ids.append(IGNORE_INDEX)\\n                elif word_idx != prev_word_idx:\\n                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                else:\\n                    if HyperParameters.label_subtokens:\\n                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\\n                    else:\\n                        labels_ids.append(IGNORE_INDEX)\\n                prev_word_idx = word_idx\\n            encoding['labels'] = labels_ids\\n        # convert to torch.tensor\\n        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\\n        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\\n        item['word_ids'] = torch.as_tensor(word_ids2)\\n        return item\\n\\n    def __len__(self):\\n        return self.len\", 'FeedbackPrizeDataset': <class '__main__.FeedbackPrizeDataset'>, '_i17': \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", 'FeedbackModel': <class '__main__.FeedbackModel'>, '_i18': 'def build_model_tokenizer():\\n    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\\n    model = FeedbackModel()\\n    return model, tokenizer', 'build_model_tokenizer': <function build_model_tokenizer at 0x7fafce3013b0>, '_i19': '# Need help with this: used in training to transform raw logits to labels needed\\ndef active_logits(raw_logits, word_ids):\\n    word_ids = word_ids.view(-1)\\n    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\\n    active_mask = active_mask != NON_LABEL\\n    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\\n    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\\n    active_logits = active_logits.view(-1, HyperParameters.num_labels) \\n    return active_logits\\n\\ndef active_labels(labels):\\n    active_mask = labels.view(-1) != IGNORE_INDEX\\n    active_labels = torch.masked_select(labels.view(-1), active_mask)\\n    return active_labels\\n\\ndef active_preds_prob(active_logits):\\n    active_preds = torch.argmax(active_logits, axis = 1)\\n    active_preds_prob, _ = torch.max(active_logits, axis = 1)\\n    return active_preds, active_preds_prob', 'active_logits': <function active_logits at 0x7fafce301b00>, 'active_labels': <function active_labels at 0x7fafce3017a0>, 'active_preds_prob': <function active_preds_prob at 0x7fafce301560>, '_i20': 'def calculate_overlap(set_pred, set_gt):\\n    \"\"\"\\n    Calculates if the overlap between prediction and\\n    ground truth is enough fora potential True positive\\n    \"\"\"\\n    # Length of each and intersection\\n    try:\\n        len_gt = len(set_gt)\\n        len_pred = len(set_pred)\\n        inter = len(set_gt & set_pred)\\n        overlap_1 = inter / len_gt\\n        overlap_2 = inter/ len_pred\\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\\n    except:  # at least one of the input is NaN\\n        return False\\n\\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\\n    \"\"\"\\n    A function that scores for the kaggle\\n        Student Writing Competition\\n        \\n    Uses the steps in the evaluation page here:\\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\\n    \"\"\"\\n    gt_df = gt_df.loc[gt_df[\\'discourse_type\\'] == discourse_type, \\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df = pred_df.loc[pred_df[\\'class\\'] == discourse_type,\\n                      [\\'id\\', \\'predictionstring\\']].reset_index(drop=True)\\n    pred_df[\\'pred_id\\'] = pred_df.index\\n    gt_df[\\'gt_id\\'] = gt_df.index\\n    pred_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in pred_df[\\'predictionstring\\']]\\n    gt_df[\\'predictionstring\\'] = [set(pred.split(\\' \\')) for pred in gt_df[\\'predictionstring\\']]\\n    \\n    # Step 1. all ground truths and predictions for a given class are compared.\\n    joined = pred_df.merge(gt_df,\\n                           left_on=\\'id\\',\\n                           right_on=\\'id\\',\\n                           how=\\'outer\\',\\n                           suffixes=(\\'_pred\\',\\'_gt\\')\\n                          )\\n    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \\n                                                     joined.predictionstring_gt)]\\n    \\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \\n    # and the overlap between the prediction and the ground truth >= 0.5,\\n    # the prediction is a match and considered a true positive.\\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\\n    # we don\\'t need to compute the match to compute the score\\n    TP = joined.loc[overlaps][\\'gt_id\\'].nunique()\\n\\n    # 3. Any unmatched ground truths are false negatives\\n    # and any unmatched predictions are false positives.\\n    TPandFP = len(pred_df)\\n    TPandFN = len(gt_df)\\n    \\n    #calc microf1\\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\\n    return my_f1_score\\n\\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\\n    \"\"\"\\n    Final helper function for model evaluation.\\n    \\n    Args:\\n    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: [\\'id\\',\\'class\\',\\'predictionstring\\']\\n    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\\n    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\\n    \\n    Returns:\\n    f1                      (float): F1 score of the model\\n    (optional) class_scores  (dict): Dictionary of per-class F1 score\\n    \"\"\"\\n    class_scores = {}\\n    for discourse_type in gt_df.discourse_type.unique():\\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\\n        class_scores[discourse_type] = class_score\\n    f1 = np.mean([v for v in class_scores.values()])\\n    if return_class_scores:\\n        return f1, class_scores\\n    return f1', 'calculate_overlap': <function calculate_overlap at 0x7fb018ec6d40>, 'score_feedback_comp_micro': <function score_feedback_comp_micro at 0x7fb018ec6cb0>, 'score_feedback_comp': <function score_feedback_comp at 0x7fb018ea84d0>, '_i21': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n            \\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", 'train_fn': <function train_fn at 0x7faf5aac9950>, '_i22': 'def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\\n    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\\n    f1score =[]\\n    # classes = oof[\\'class\\'].unique()\\n    classes = [\\'Lead\\', \\'Position\\', \\'Claim\\',\\'Counterclaim\\', \\'Rebuttal\\',\\'Evidence\\',\\'Concluding Statement\\']\\n    print(f\"Validation F1 scores\")\\n\\n    for c in classes:\\n        pred_df = oof.loc[oof[\\'class\\'] == c].copy()\\n        gt_df = df_val_eval.loc[df_val_eval[\\'discourse_type\\'] == c].copy()\\n        f1 = score_feedback_comp(pred_df, gt_df)\\n        print(f\\' * {c:<10}: {f1:4f}\\')\\n        f1score.append(f1)\\n    f1avg = np.mean(f1score)\\n    print(f\\'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}\\')\\n    return valid_loss, oof', 'valid_fn': <function valid_fn at 0x7fafce310a70>, '_i23': 'def inference(model, data_loader, criterion, valid_flg):\\n    stream = tqdm(data_loader)\\n    model.eval()\\n    \\n    valid_loss = 0\\n    valid_accuracy = 0\\n    all_logits = None\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch[\\'input_ids\\'].to(device, dtype = torch.long)\\n        mask = batch[\\'attention_mask\\'].to(device, dtype = torch.long)\\n        with torch.no_grad():\\n            raw_logits = model(input_ids=ids, mask = mask)\\n        del ids, mask\\n        \\n        word_ids = batch[\\'word_ids\\'].to(device, dtype = torch.long)\\n        logits = active_logits(raw_logits, word_ids)\\n        sf_logits = torch.softmax(logits, dim= -1)\\n        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\\n        if valid_flg:    \\n            raw_labels = batch[\\'labels\\'].to(device, dtype = torch.long)\\n            labels = active_labels(raw_labels)\\n            preds, preds_prob = active_preds_prob(sf_logits)\\n            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n            loss = criterion(logits, labels)\\n            valid_loss += loss.item()\\n        \\n        if batch_idx == 1:\\n            all_logits = sf_raw_logits.cpu().numpy()\\n        else:\\n            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\\n\\n    \\n    if valid_flg:        \\n        epoch_loss = valid_loss / batch_idx\\n        epoch_accuracy = valid_accuracy / batch_idx\\n    else:\\n        epoch_loss, epoch_accuracy = 0, 0\\n    return all_logits, epoch_loss, epoch_accuracy\\n\\n\\ndef preds_class_prob(all_logits, data_loader):\\n    print(\"predict target class and its probabilty\")\\n    final_predictions = []\\n    final_predictions_score = []\\n    stream = tqdm(data_loader)\\n    len_sample = all_logits.shape[0]\\n\\n    for batch_idx, batch in enumerate(stream, start=0):\\n        for minibatch_idx in range(HyperParameters.valid_batch_size):\\n            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\\n            if sample_idx > len_sample - 1 : break\\n            word_ids = batch[\\'word_ids\\'][minibatch_idx].numpy()\\n            predictions =[]\\n            predictions_prob = []\\n            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\\n            pred_score = np.max(all_logits[sample_idx], axis=1)\\n            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\\n            prev_word_idx = -1\\n            for idx, word_idx in enumerate(word_ids):\\n                if word_idx == -1:\\n                    pass\\n                elif word_idx != prev_word_idx:\\n                    predictions.append(pred_class_labels[idx])\\n                    predictions_prob.append(pred_score[idx])\\n                    prev_word_idx = word_idx\\n            final_predictions.append(predictions)\\n            final_predictions_score.append(predictions_prob)\\n    return final_predictions, final_predictions_score', 'inference': <function inference at 0x7fb010251d40>, 'preds_class_prob': <function preds_class_prob at 0x7fb010251dd0>, '_i24': 'def get_preds_onefold(model, df, dl, criterion, valid_flg):\\n    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n    all_preds, all_preds_prob = preds_class_prob(logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred, valid_loss, valid_acc\\n\\ndef get_preds_folds(model, df, dl, criterion, valid_flg=False):\\n    for i_fold in range(HyperParameters.n_fold):\\n        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\\n        print(f\"{model_filename} inference\")\\n        model = model.to(device)\\n        model.load_state_dict(torch.load(model_filename))\\n        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\\n        if i_fold == 0:\\n            avg_pred_logits = logits\\n        else:\\n            avg_pred_logits += logits\\n    avg_pred_logits /= HyperParameters.n_fold\\n    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\\n    df_pred = post_process_pred(df, all_preds, all_preds_prob)\\n    return df_pred\\n\\ndef post_process_pred(df, all_preds, all_preds_prob):\\n    final_preds = []\\n    for i in range(len(df)):\\n        idx = df.id.values[i]\\n        pred = all_preds[i]\\n        pred_prob = all_preds_prob[i]\\n        j = 0\\n        while j < len(pred):\\n            cls = pred[j]\\n            if cls == \\'O\\': j += 1\\n            else: cls = cls.replace(\\'B\\', \\'I\\')\\n            end = j + 1\\n            while end < len(pred) and pred[end] == cls:\\n                end += 1\\n            if cls != \\'O\\' and cls !=\\'\\':\\n                avg_score = np.mean(pred_prob[j:end])\\n                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\\n                    final_preds.append((idx, cls.replace(\\'I-\\', \\'\\'), \\' \\'.join(map(str, list(range(j, end))))))\\n            j = end\\n    df_pred = pd.DataFrame(final_preds)\\n    df_pred.columns = [\\'id\\', \\'class\\', \\'new_predictionstring\\']\\n    return df_pred', 'get_preds_onefold': <function get_preds_onefold at 0x7fafce32a0e0>, 'get_preds_folds': <function get_preds_folds at 0x7fafce32a200>, 'post_process_pred': <function post_process_pred at 0x7fafce32a4d0>, '_i25': 'print_gpu_utilization()', '_i26': 'def pretty_size(size):\\n\\t\"\"\"Pretty prints a torch.Size object\"\"\"\\n\\tassert(isinstance(size, torch.Size))\\n\\treturn \" × \".join(map(str, size))\\n\\ndef dump_tensors(gpu_only=True):\\n\\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\\n\\timport gc\\n\\ttotal_size = 0\\n\\tfor obj in gc.get_objects():\\n\\t\\ttry:\\n\\t\\t\\tif torch.is_tensor(obj):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  \" pinned\" if obj.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t  pretty_size(obj.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.numel()\\n\\t\\t\\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\\n\\t\\t\\t\\tif not gpu_only or obj.is_cuda:\\n\\t\\t\\t\\t\\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   type(obj.data).__name__, \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" GPU\" if obj.is_cuda else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" pinned\" if obj.data.is_pinned else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" grad\" if obj.requires_grad else \"\", \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   \" volatile\" if obj.volatile else \"\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   pretty_size(obj.data.size())))\\n\\t\\t\\t\\t\\ttotal_size += obj.data.numel()\\n\\t\\texcept Exception as e:\\n\\t\\t\\tpass        \\n\\tprint(\"Total size:\", total_size)', 'pretty_size': <function pretty_size at 0x7fafce32acb0>, 'dump_tensors': <function dump_tensors at 0x7fafce32ad40>, '_i27': 'dump_tensors()', '_i28': 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'oof': Empty DataFrame\n",
            "Columns: []\n",
            "Index: [], 'i_fold': 0, 'tokenizer': PreTrainedTokenizerFast(name_or_path='allenai/longformer-base-4096', vocab_size=50265, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}), 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 4e-05\n",
            "    weight_decay: 0\n",
            "), 'df_train':                  id                                               text  \\\n",
            "0      3321A3E87AD3  I do agree that some students would benefit fr...   \n",
            "1      DFEAEC512BAB  Should students design a summer project for sc...   \n",
            "2      EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
            "3      A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
            "4      616F8E0EFABF  \"Can you imagine a time in the future when no ...   \n",
            "...             ...                                                ...   \n",
            "12472  1C899F124FEB  While some students may think it's a beneficia...   \n",
            "12473  4453444AF383  There has been a strong arguement going on wea...   \n",
            "12474  EF0D75BF48DA  I favor in to changing election by popular vot...   \n",
            "12475  8FFDA5B9D359  Do you think students would benefit from being...   \n",
            "12476  ACAB1FCA0A30  I would like to change the election for the pr...   \n",
            "\n",
            "                                              text_split  \\\n",
            "0      [I, do, agree, that, some, students, would, be...   \n",
            "1      [Should, students, design, a, summer, project,...   \n",
            "2      [People, sometimes, have, a, different, opinio...   \n",
            "3      [Dear, senator,, As, you, know, the, Electoral...   \n",
            "4      [\"Can, you, imagine, a, time, in, the, future,...   \n",
            "...                                                  ...   \n",
            "12472  [While, some, students, may, think, it's, a, b...   \n",
            "12473  [There, has, been, a, strong, arguement, going...   \n",
            "12474  [I, favor, in, to, changing, election, by, pop...   \n",
            "12475  [Do, you, think, students, would, benefit, fro...   \n",
            "12476  [I, would, like, to, change, the, election, fo...   \n",
            "\n",
            "                                                entities  kfold  \\\n",
            "0      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2   \n",
            "1      [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4   \n",
            "2      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3   \n",
            "3      [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1   \n",
            "4      [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      1   \n",
            "...                                                  ...    ...   \n",
            "12472  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3   \n",
            "12473  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3   \n",
            "12474  [B-Position, I-Position, I-Position, I-Positio...      3   \n",
            "12475  [B-Position, I-Position, I-Position, I-Positio...      2   \n",
            "12476  [B-Position, I-Position, I-Position, I-Positio...      4   \n",
            "\n",
            "                                                  labels  \n",
            "0      [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "1      [0, 0, 0, 0, 0, 0, 0, 0, 2, 9, 9, 9, 9, 9, 9, ...  \n",
            "2      [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "3      [0, 0, 1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "4      [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "...                                                  ...  \n",
            "12472  [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "12473  [1, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...  \n",
            "12474  [2, 9, 9, 9, 9, 9, 9, 9, 9, 4, 11, 11, 11, 11,...  \n",
            "12475  [2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...  \n",
            "12476  [2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, ...  \n",
            "\n",
            "[12477 rows x 6 columns], 'ds_train': <__main__.FeedbackPrizeDataset object at 0x7faf5a68b510>, 'df_val':                 id                                               text  \\\n",
            "0     2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
            "1     F9368F6BAB62  You should join the Seagoing Cowboys program. ...   \n",
            "2     617D56A15483  Do you want to go on daily trips and help peop...   \n",
            "3     D4AA8EC389C6  School Grades:\\n\\nI don't think that it is fai...   \n",
            "4     D92FAEE355E1  Dear Mrs. Principal:\\n\\nI think this new schoo...   \n",
            "...            ...                                                ...   \n",
            "3112  3BEFA845FC27  There are people all around the world that mak...   \n",
            "3113  F166DA36A4A3  In this essay im going to prove to you why tha...   \n",
            "3114  8F33151F0334  He joined the programe for many reasons to hel...   \n",
            "3115  DDA2A181F486  I'm against this technology that reads your em...   \n",
            "3116  ACCD71550365  Do you think it is a good idea to succeed? I b...   \n",
            "\n",
            "                                             text_split  \\\n",
            "0     [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
            "1     [You, should, join, the, Seagoing, Cowboys, pr...   \n",
            "2     [Do, you, want, to, go, on, daily, trips, and,...   \n",
            "3     [School, Grades:, I, don't, think, that, it, i...   \n",
            "4     [Dear, Mrs., Principal:, I, think, this, new, ...   \n",
            "...                                                 ...   \n",
            "3112  [There, are, people, all, around, the, world, ...   \n",
            "3113  [In, this, essay, im, going, to, prove, to, yo...   \n",
            "3114  [He, joined, the, programe, for, many, reasons...   \n",
            "3115  [I'm, against, this, technology, that, reads, ...   \n",
            "3116  [Do, you, think, it, is, a, good, idea, to, su...   \n",
            "\n",
            "                                               entities  kfold  \n",
            "0     [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
            "1     [B-Position, I-Position, I-Position, I-Positio...      0  \n",
            "2     [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "3     [O, O, B-Position, I-Position, I-Position, I-P...      0  \n",
            "4     [O, O, O, B-Position, I-Position, I-Position, ...      0  \n",
            "...                                                 ...    ...  \n",
            "3112  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "3113  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "3114  [B-Evidence, I-Evidence, I-Evidence, I-Evidenc...      0  \n",
            "3115  [B-Position, I-Position, I-Position, I-Positio...      0  \n",
            "3116  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      0  \n",
            "\n",
            "[3117 rows x 5 columns], 'val_idlist': ['2E4AFCD3987F', 'F9368F6BAB62', '617D56A15483', 'D4AA8EC389C6', 'D92FAEE355E1', '58EE193B3F7D', '4A2693C68175', '3741D275F8BC', 'FC9E2A38AFBB', '8B5CB2AA72E9', '2DEC8D3BA891', '217A7429341D', '7E558B4E7009', 'CC38587E9634', '8DD596F5F5B5', '632C8DA9E2B1', '76C8FE4FB3D8', 'D7D66E258830', 'C20011D6FA9D', '508DB90F68C7', '1655FBD1D22A', '0DE6D016C7FC', '1D07A65F9B7F', '0257D9E42247', '31DB1BDE89A1', '13FB62D377E3', '33C081EC2A59', '90691C2B3D5A', '6D6215C23989', '7FB898313EA9', 'BCAE5B0783CF', 'C5C81327D129', '5942FB27D5C3', '185FD725317F', 'ACBC86CD65B4', '027616F7ED1D', '6068A54ED99D', 'B75E90DC14B3', 'BA545022DFCD', '2D654BBA9310', 'F53C55BBD326', '0821EB1AEDED', '7F74864D0865', 'DCDBC5C7873F', 'EA9FA5DA65F3', '486B53D5F8A0', '1CCEB6CD629D', '1F8DDCFCA766', 'DBCC1F8F92B5', '51E211998DF0', 'D447CD5A0C67', 'C50FE04C4D29', '32B2D5592E1D', '2962D1BB17A2', '59E8C8BE4ACA', '155F508445E2', 'C3F35CEC4B5A', 'B5113469D13C', 'E09064F496DF', 'E5EF52219F63', 'C43C4EE10535', '2DFFB145BE07', '2F3210D23414', '084DC646D8E3', '2B002FEE337E', '8DB4751C05A1', '8C7464101130', '3752D33FDE11', 'F3CB6601CF24', 'FE1DEACABE37', 'BF0241C10324', '8F91417B8A7D', '9DBB8B0C3367', '69891F080D3A', 'EE63F23F10B3', 'D7C33D289E26', '325B922888A7', '3A08788DCA7D', 'CFA4E76C506A', 'B3C2E6274311', '7F77FE73B338', 'E1130AA34EE8', 'CFB99F0D0D31', '3712A5E56380', '1B1FBDE25C2C', '6D6B05E5478D', 'D1673363C56A', '1BA91BC3D89A', 'D5724EDF0DBA', '5FFC25FE5E00', '6ED00F0E1BFE', '4C0EB60473E1', 'F605023B87A7', '2617310033A4', 'D284AB2165C8', '29B80B7D1092', 'EA205A51D32C', '5137514F130A', '0C0A8EBDC931', 'BF5D82D486F5', '4A8AC603C9C5', '3188A9E2D162', '92887B4B5F2A', 'AE844BA96314', '21A162D4B0CB', '4390794B2751', '46E97FDCA469', 'B5838ACCDBA5', '318B9B177D68', '41A159819E12', '29BE597866EE', '9396FCEB118A', '48FB3005F4DC', '8134AD7791FC', '8E938A38BA43', '220A26E7E802', '04ACF5873DE9', 'ED4186119E60', '1CC9A9598942', '9A5D8C898877', 'D6466B4502D5', 'B60A2DC4B93C', '7ECB7FDA805A', '3AFA78FFFC15', '611228818239', '619D1D7FDC04', '85255F76A261', '94CCB4234EAE', '971D0C1C75F0', '710A2280E20F', '7B64BC831CD9', 'C9DA3BE5EB24', '479F6A965C8C', 'D24777F39289', 'D7946F0C2A83', '65590D44B68A', '2B4A794ED0D8', '242E9BA55C0C', 'E3F3C192B3A2', '61671FCB6624', '3F8C4D2CA16E', '2E9B5CD4716D', 'D022656B1A36', 'D5CB4FD63313', 'D66B89FC3757', 'D7401AE3A016', 'AC7C84C2A13E', '727DE3CD0AEC', '7C1FF9F7C5C8', '2A7C4A1CBA41', 'BA18C6F80423', '23C6ED53A179', 'C05809232F0E', 'C8D3EB90EB29', '4AD3DB40F5D3', '46284DF2F31C', 'AF34B1CDD152', 'A6CC10322C3A', '0DBE82E9933F', 'F5A0BD46D3AF', '3CA61DE9B5C6', '899E6252BD6F', 'EA5E9342C547', '89CD5C5D9180', 'FE47325AD93F', '6CD4FAAB7653', 'F503C7DC334F', '870D9B35F20A', '37FC9DB2D1DB', '8C9A286F5351', '7AD4CB4029C0', 'BAEE45819FA1', '7BBC565EFE33', 'F44F49940938', 'E7576760695E', '116FB053BEC7', '5EF4174A5264', 'B651AEF954C3', '06E3595C94BF', '0054850878E3', '7091F3705B94', 'FC90D46FBE1F', 'DE1EF5A9F3AA', 'E8FF0D2483FE', 'EF2D45E53F40', '75A613E09762', 'DA2D885A25C8', '5666D5642C98', '563122BF4796', 'A4035C4D846D', 'B0007528CB4A', 'B3526897360F', '2907A97CC7C4', '9B69DE203555', '387A6ABAC720', '3713AC622BFF', 'CEB3F5203744', '4B54BE9BD269', '11B942BD5A87', '3C9B9DAFB0AE', 'E05800F81426', '548A0120A1CA', 'AA48C183006F', '68A83805444F', 'BFAA8D19FFA9', '970A1848B187', 'AB5069821C4E', '27E6536433B9', 'F806675B9B70', 'A486E99E277D', 'A8445CABFECE', '14D6911BF51D', '5613F9FB2154', '46F64048E5F5', '6E80360E58BF', '1D459BD1F763', '72B22E89D0EE', '29AB81B05092', '50C6B70DF247', '6D616AB3E2A7', '958E3ACAC90C', '1F44A4A679C2', '453D762EF267', '1F539B2D1ED3', 'F32A84A0E29C', 'E7A45AEC5AE8', '2CF5EC4E36F2', 'C3C13F0C0274', '548A58E068EB', 'EAF72F67654F', 'DD627EE5CB27', '897438742BCC', 'BC32404D0719', '1693FF58E7B9', 'CA4E39042770', 'E10E42DFDA2B', 'B8CA9E3E3100', '0FE128A98388', 'DF2BE9E97393', '9E3359EBC812', '082598CC8955', '92E1CFD27E95', '095B59682071', '716543D2EA47', '371834E7E1DD', 'ECD0248B4146', '995330686089', '672D0F5241A4', 'CCD30CE1E903', '07E4962801FC', 'D88A1D12032B', '712A59B74C91', '7BC73C8FBDBC', '7A143C2BC7F9', '1B2470F552CC', '9C97C5A04635', '57AB2EB3B7B6', '6ED825F9F077', 'FA5CE3A393CF', 'C83EB6623D24', 'A5A55FE930C4', '58211510AB4F', '11CE345BAEF2', 'CC9290DC336A', '420557904E93', '6A088E8BBE02', 'CDBE82430B57', 'E249399CA04E', 'F711AD68730E', '0802AEA16E9F', '44735791FDA0', '9F7100885801', 'C02E687A4BB9', '51A5A4C583D2', '8A033A8BE5E7', '2F7B32094BCD', '65922C6DA1B8', '19C2D7C5171D', 'C5A56B482FAA', 'FAB02B654C4E', 'C1A275127D98', 'BF3CC3305C55', 'F2A79A33B020', '95BCD0EEE72B', 'F24FA9D60071', 'C5F1272FF970', 'D0A54C1D411F', '6DF747F4A277', '58E84BCE4B90', 'C9896C21A7AB', '91962F622C5A', 'BC2CAB7DB6F0', 'D9C9F849DC3E', 'A7B69F38FB55', '8D73796D52E9', 'AED0E44B59CE', 'CA5DB05415A5', '4478B3244F49', '1313ADFE62C6', 'B5FF33681C22', 'B437EB073109', '1EB5D99BF3CC', 'D087A8C24570', 'E02E7B3C880F', '3BF2F496AF76', 'BA655E4F7018', '954486CDA6EA', 'ABEB82F77B84', 'F6CFA4C7247B', 'E90A6BA3F6E8', '454FA9C8E4C0', '0C897BA8D8C1', '1010F320FABB', '21CB2FC1D56F', '7C45A871BF88', 'C70EE5903373', '1AD208F3F94A', '1EF99E72B0AE', 'C5E15819B7D6', 'A4E6E2D5658D', '2FC2AD6AF52E', '80DD6E906303', '69FABB805AB2', '5A6FAD4F6359', '124F5E51002C', 'A975CBC4A7E8', '262A455A7060', '250A3F61B6D8', '0D11CD49499E', '476E07AEA488', '8A3F92425ECC', '4B30291A725D', 'C9AC065BB597', '7A28AB016AA2', '85F85936F1E6', '1669F1CC90D9', '84B1E0E85878', 'F3593D9B1E95', '31467F92ED26', 'FF945B27A8F2', '2B557F0EF764', '862FEAF83C5B', '90ECCF04A979', '47C517E48BA4', '85B3EE483BEF', 'E6C2FD3578B3', '58567312BDAB', '464B19E7FBFF', 'FF23EEEB0945', '8EBD6FAE257F', '086603D5B319', 'E2B2ABB01662', '527D2EC10B3C', '2E6230521AC4', 'C0D0428510D5', '0DFAF28500A2', 'EB8B27A81300', '5986652F6023', 'ECE6DB354262', '58E92DF92D8C', 'E6337FDAD05A', '11B9AC1814C8', 'AAA152F374E0', 'E11931B2A6AC', 'DCE199D98B3D', '5B8AD3907163', '94927A32B286', '3171BAF9F164', 'E8D3A695C074', 'FF01B32BAC3E', '575829738D7F', '276048EF9F9A', '0DF3E1BC7D04', '6E381D4CCEDA', '429F4E63967A', 'EB4A3811B1AF', '36EB823B246D', '74B57E07F63D', '5E879A28D710', '0416D5BC09FB', 'E59DD4FF67E8', '309272016742', '9C6DAB936B71', '4DBDC14F77BA', '028A2B1C73AF', 'F497369C41C2', '572236CAAFCE', 'ACEDBB3C81A0', 'D228AB564BF6', '6851B722B3E0', '428FD33185B5', '326EE5515F2D', '6ECD533A72F4', '9D42FF4BDA82', '00EE23F071DE', 'C9C61B8A1FEC', 'EFE800DEFA4B', 'A8D52D24BF51', 'F8706CB01394', 'E5909247F839', '8555ED31A266', '40936430B82D', '42DC0DED6A11', 'DDA8E22052DD', '757E9E1595A7', '5CE34C643883', '053DA7D0417B', '248BA02C6B96', 'B948C72229B3', '43B8AFB8396B', 'F3EE1510C8EF', '855398650C01', '180159E53733', 'BF7DA06D503F', '9A8B992CE622', 'CCF236C2E533', 'C3C4D847175E', '0316F6288791', 'D972C6918584', '1D8D7B32B98A', '8E6CB58E9762', 'B4D91520B347', 'C737906554AE', '171E655CC8F1', 'DBC53691CD0A', '438ABF2E294B', '9C49F198557D', '80619550D955', '1006FE578078', 'D7D83D1EBFDB', 'E079C8DE0D1E', '2D36D77E0BF4', '207D7CEADF38', '07A023BE2629', '6FCD57CA5E41', '710928FFC6BC', '2B7612A3D793', '1FF01ED707B6', 'F497B058DEB9', '814488D5C66D', '9F7080A8AC9E', '9400D174DEC9', '43F92C43EB64', '03F55023E993', '9CE836599EEE', 'D1D38775CCFC', '0F3F6283B13B', '1CEDD5563788', 'BF17866F7014', 'BEE54D4D2B79', 'BA71F335957F', '058BD5D122D1', '28220DC55DA8', 'A535F9081747', 'CAAF7FF6EA89', 'DF764CB197A7', 'A6B380D5DEBB', '925C7FFD12C9', '850401568E9C', '81587E82D64B', 'F0B46DACB512', '15D5005F0ABE', 'C57E6EAB037F', '1111156C5EB8', '197A2E0615B2', '04ED5F75647F', '466945C86DFD', 'EAE3F8725A23', '4722B4D2C6E7', '7A1940992606', '3C26EC679114', 'AE425A221021', 'EE26DF6F6A1F', '4AFD5F644AB8', '2B48EAC02BF3', '3C2FB67A96CB', 'D4C789D85451', 'E15ECDD9A660', '3899C281559F', 'F280DC36C974', '9D4F9CC286B7', 'CFAF62184AB2', '5503B0FB1886', 'A2816855FB22', '45B7681DBE5C', '547A04414B54', '84187E561424', '4F5449021387', '3671DB5110C0', '5782F64504B9', '66ABFA4DA3A4', 'E595E218D423', '5721D0A1AD3A', 'A67C8BEE3935', '26E218908B4C', '1F694DB4D678', 'E243F4B2D0FA', 'CE64FA08E4CF', '738A2AE1D797', '2CFBD4AA7BE5', 'A996BE4B49FA', '543192A13FBE', '3C942012269E', '65688F5C5F82', 'F245631F4D97', '1C58F2AF08B4', 'E9428D8AFEF6', '81FD42FF7DBF', 'D282E21C2DB1', 'B5330C56B5B8', '3FC933A3FB55', '3FE607F4D3BC', '5C680F1815BD', '70FBC86A01C6', '2E1266682F4A', 'F92B3B747F33', 'C87A10617D8F', '8BFA9CC64A02', '00D304153840', '8EFDE74335BD', '16ED1E19CD0B', '763EF698F56B', 'A817C51F0787', '534353848265', 'DBA3D1D1D0B5', 'F953AA8577C2', '14409A305AB3', '4DC39F6792CA', '7600F1C7D884', 'F693E138BDAB', '66306888222D', '5208ECB70235', '5DD0E0AE3472', '97DD2D770B03', 'D7A9CF9A7786', '7A425A58F136', '3E6D1917A314', '15BA74A13D07', '7214C472823B', '5CCF95325E62', 'EBD98D754ECD', 'F468E21A6DEE', 'DE2A70D1284E', '27681E4EB137', '77C40DA262AC', 'A0DE768B504A', '9F0900A01266', 'BBF34C059226', 'F345260A27F3', '9752B7C08FFA', '8DDDD842BDDD', '1ED8279252FB', '124750B04447', '76BF3852B313', '5B20D8C3781D', '3B9A70FC3744', 'A4D269D38816', '2D89AB0B68A3', '0B1B547521F1', '5D724F93F765', '62EA3F855DB2', '51DEA81EDBA4', '3017001AD49F', 'B562D63C7817', 'C2ABDAC2BC2C', '64547CA6B2BF', '45775C79FA64', '779EADA2AEF5', '603407534EAE', 'A0DEA37279E9', '93E94C3BFE5B', '202C16A5B108', '2C8EA34FDC34', '95B09C17403B', 'D4BC5A89A5CD', 'FA2BC6C3A09B', 'A22DFE5E7337', '196E73AAF08F', 'A20D6E252177', 'B24EE1A93A0B', '18A22DA09EF0', 'C2A6C69714EC', '1D7EEE65F780', '343A95F8B1A2', '2E62A47405BC', 'E2E3F9182F8D', '688B11ADB595', '8BEC48C6C5E9', 'FDBC5C68ED9F', 'CB86688CF491', '95E622E22E04', 'F163B4249C07', '5EB99924C996', 'CCD4A267FA68', '6589766A4C78', '95CB51270760', 'EDE0E6181F61', 'A455B4430327', '75001A5FDEEF', '23AA5F45C05B', 'E5B076EB95A7', '75022F40463D', '988D787A291E', '911F68B99394', 'C5C6DED9C790', '5B6F224A710F', 'EFF473414159', 'C51752A9035C', 'F668F7A36DBF', '5816C1DCD336', '60B21E600112', '0347FB2B37A9', 'DA277EEDBA0C', 'FCF3A5BD8371', '13CFDB110C7C', '3245CF7D6A64', '5341197C5A59', '371DCD0EF85F', 'BE3B47A38A44', 'AFCE5307F69A', 'E099AB8BFFC5', 'D9C0995BC85E', '84956A4FA804', '4E1CF079ECDD', '3B2A626DE2B7', '9ABABB404DEE', '9407DB8BEDB6', '68BF11B32272', '410957337087', '504EF8AA4837', 'E2A75A854E28', '267F9C38F948', '2308726310EE', 'A8411CE08C68', '96A0B5E5A5CB', '014BF1790D44', 'B2F69B97BC2A', '148BB6325B8A', '31A3040F487C', '87A974F87B12', 'ABB7625615F2', 'D778E2E477F1', '5C094CEAB69B', 'A41214DF1DC5', '23F0ABEF8370', '9019F9FEA638', '01AEAC17451B', '7A347C69D3D1', '3F32AA4DA912', 'B67D550009E8', '7FB5A53C6DD8', '19E44CA7DF5A', '79CDD9460FB4', 'CE120A8B4336', '4BBC2E84ED08', '43596925AF2A', '3E5E0038C60A', 'C86676509204', 'AFFD150170C8', '83FA1948A3F4', '7476076AF500', '6D751D1BEB8D', 'E680D448BBA4', '475060D04C52', '69A0715FA8A5', 'EC1DD5C739B9', '8B3DB5B8F0B7', 'DFB0362070E2', '57807B412545', '15BDCD238E51', '2022B8E2B360', '459F27598FA9', '3F0E512E243F', '7BB808935B65', '0E6BB199BF93', 'C00E1BA4B107', '4D4A2F5E1948', '6E2EA3B14180', '83D5036778E7', '3C2E56DBA570', '7B751C9BE162', '3B9E54F5907B', 'CC4EEADA5262', 'ECFBEE2C79D7', '4D8415D5FABD', '1456B98CAF05', '8D242A52F9B3', '6397F69A35AB', '483A53143CF3', '7A5D38D49C13', 'D2DC98C026FD', '35D9FB44EBF8', 'DCB7655A11AC', '3CFA497859E6', '258C8CB1D40C', 'E1EC4869981F', '66CD59D78A9B', '1AA55FB101FD', '40A475C95D6B', 'BF2E86982E3A', '86787CB300DC', '52A09816A434', '5EF70094623C', '43EC1533429B', '9C6B4DBF759B', '27C1F6B19E89', '6C30C0076FAD', '20162733792B', '227853D28137', '28ED24DB496C', 'CD32C611650D', 'AA1F3740A24D', 'E6243EA8D1B0', '89D5A08E50C9', '7247BBE354B9', '1A067085A035', '62FC97D29846', '946581583AA8', '20A42E81AC2B', 'DA8D2BFAC0EA', '292FF100BEF8', 'A247FCC8D696', 'AB811AA2AB4A', '5D47B9B734D2', '4631AFEED2F7', 'E86C6679AAF2', '25A752FB734B', 'B8748AECBD72', 'EE580368CD1D', '8E7A674BF659', '857D1D3BC2C6', 'CAC4E6021973', '5681C8C42F1B', 'B65361F90B72', 'BE4E34FD45B6', 'A99895FA8F4B', 'B25D365F5C1C', '9E8925504E49', '9F12C5402E1C', 'B1AF3AD74555', 'C414E9F1DAA6', '90792E3137E6', '34FB9E0DF44A', 'E877D5AD16DA', 'BD128A3C6F33', '6081F7294031', '72279B65EB03', '10D09AB4D6C7', '78FFA7DA37CA', '9BE5C1E244EB', '0ABFA238F8BE', '8135441CED7B', 'AAA1665D47BD', '793800C02EE3', 'A65BB728AB32', 'ED8EE1F02203', '2DEC6F2AE21A', 'B36CF0CB30E5', '36D35B8A7915', 'D11AF7E28CDF', '081E861C5EE8', '4D2C67616599', '1A8074C87C0A', 'C6BB30D4C189', 'F4003A737E0A', '1232E9091138', '1A6338E69BC6', 'DBBF3EF47E93', '508DCB180063', 'D0279F114F87', 'BB4B5D896E96', '2F0BC63ECAF4', '20FE9FABD1C2', '127E7F587FBD', '60204ED9F4ED', 'D85690C1836A', '12C4FDD062F3', 'A2D7019DACF5', '8E0A4CFAEA9C', 'ECF8296772E9', '13BD2F675384', '63C087C70C54', '037C6126FDE4', 'D62E2B64236C', '9E61674DE1D4', '718800CC3C50', '229CBC81F9B7', '65E089A32CBB', '1065173BBE31', '43B12AB4BCAE', 'F44A8B59CF6A', 'F1EEA3FC1490', '2D23892ACDA0', '50DA28892569', '20E1ED11F61C', '91B840A41637', '65871B0D00E9', '38283A07296C', '1424DEF8AEA5', '2B7416B5A789', 'C755C1F6B4A7', '8D83D8D108F2', '4C3F39412E92', '56D513B3BBD4', '9D53E0729D4B', '824DAEDF9E75', '8E668341EE05', 'A8A25F94F5B6', '00B144412785', '4858280EA8BC', 'F2068036F26E', 'D8DC48B022F5', '6D0960F54CC0', '66DA2F1F5213', '32799982E5E3', '89183CAB903F', 'C5569B2725B4', '4920125BC38A', '76684944F17B', '442CFAE61CC3', 'AF077034AC88', 'ACABEA88860C', '7B6440A3F279', '9F0C2B822B76', '51DB76880081', 'E1412D9D1200', '961822095512', '1F20EF428962', '7C8094BF63DE', 'CE3DBEAAEAA8', '899CC108A4DB', 'BFF1386AFBE2', '099764B21768', '204E5D4294C2', 'EE426846F3D3', 'D225223D0260', '219670F16AF1', '163A7C9FCF34', '957EE90E67F9', '9C8126B25A26', '74FF3575E386', 'C9CAB778C94E', 'C93DCEE52172', '8C4DA60A580E', 'FEEA679DCBB3', '6786EE6112A7', '53C9F177EB03', '998E0F68318B', '2CEA555561C1', '7D89CBF638CE', '9A1DC81E4FB5', 'B0D5228E9FBD', 'FF7164909305', '155ADE36AE80', 'AB5EBB04C48D', 'D3C9713AEDC1', '5BB2AD58AF12', '6F8081D34E75', 'DA0FBE6F11C9', '5F897C00DB43', '304B55907101', '42017C1963DB', '99833F73DE44', '86A3A166AA67', 'AD7525091D85', '4776005B77F0', '057AF0C42467', '18DD6D2A09F2', '504CC3493DAF', '861588AA9001', '6770A2F02671', 'C28B8FFF66DE', '5E95EF84B113', '0B8A0777A6E5', '93E028526579', 'AE6DBB2AE6DB', 'F5EFE72B5E63', 'FF9E0379CD98', '768885A4B61A', 'C8C708EC29F7', '3AB30AB267DA', '9BBEDD9BD0C9', '2C344E66F358', '252C111E462C', '059DFFE3A3A9', '0FE5C9C13E7A', 'E4FC29A88EDD', 'A8DFF4D30133', '8E7975A0FCB5', 'FE40A50DF994', 'DB9838545463', 'B0F645CD20FF', 'D8FA1F4C6D5C', '2F19735DD6EC', '3DA81D432A0F', '6DD430E0A7CF', 'E03ACCEF14C2', '99216EEE458B', 'AD34E4ED347B', '2AAE2550DDAD', 'B4F6234EE053', '01267F2F5B01', '5218172D792D', '0C0E56A1FB05', '6794A463D2C5', 'E934DAA3C0BF', '96253054EFBB', 'FD97228DE00F', 'AE6BABB53D5E', '9AA193ECC201', 'B0D9B3EA4E5A', 'FE477BD61B22', 'DD0CD2B6F7CB', 'D150C1CD72D5', '31BC49520E9D', '754C88B5CA3C', 'A1A778253D39', '0326BEE07DE1', '2DFBA75B6C37', '3C4313611426', 'D20BEF9C2CD7', '39179028B2DA', '6660129B132A', '1F2351768406', 'E9BB013E0F33', '2D4643197395', '1B1E2883EDE6', 'DAD11E157730', 'A157CD4FC4C8', '4ECE014D0B37', '5B9E1E0553AB', 'EDBDDA26B49B', 'C20C1032FA1C', 'F763B20CBBF2', '2EDC00346850', '867A58ACE30E', '0DF37DD497A2', '8780E3CE2A3E', '9A03E46C0A51', '603D38C864DD', '3E89989D6585', '478D40552C87', 'F4E4C8EEF637', '515244292F97', '0C7890BF0A4E', 'A83032C6800B', '3DD7D9A53E39', '054C082D0304', '2F282FA175FF', '1F9C60A47E3D', '089DE1EB8CAF', '9DA131E0F720', 'FC27ECB471A4', 'B6AB1B1F7364', '5934838B26AD', '655EC70088AA', 'C8E9D6F3C80A', 'BEDC43C0FE6A', 'BF619ADE9BF4', '9F457CB98667', '2D95CCCD4DC0', '634A2AD49A62', '49AEA23D2E9A', '81AD1F069407', '077EA8162D03', '51FC77F6D507', '98947B33E164', '604B7BE00CDB', '0E66C9CE5B7C', '5171E1D8A82A', '5C5EB17E8684', 'AB45B8A7959A', '70B291155F0E', 'F748CE6BB514', '1A4194E02D4E', '10B5AC99E165', '57D5D51D6D7A', 'D485BE90E01E', 'A4C33694BE0C', 'CC3B51667B02', '8EB22AFF2A96', '3388A8CBA2A1', 'F949490671FA', '57859DBF4B4D', '54F6AD35A8D1', '90981BB41313', 'E8BF036DCFA5', '534835158D10', '367111CA290A', '88FD7FAAFA90', 'F883844CADD0', '372B34B48E84', '811DE32FCE53', 'DD8649A7235B', '6973D20B45C0', '1C9693E27251', 'F3E71A1A4F8B', 'BF9A3A65135E', '9B57F670E6DA', '32FE31495988', '7794085FE26D', 'FEA516299C94', '3DD1CCDA43A4', '985B2CF2C08F', 'AE7DE878CED5', '6AB7A38DBD70', '258F5D734299', '1706A44CC656', 'F6A92E43251E', '70F52F235E9A', 'A846B232FB69', '305F209C9A19', 'D4517AB595DB', '0961B819A4E9', '3BDE1763ECDA', 'AC1E6307E966', '3542AB52FF6C', 'A4FB165016A2', 'DEE347FC135C', '58DEF94A4FAB', '9A6F2CF32BFB', 'C944AAD86BA6', '6C394E3B51F2', '9C22F6D8C827', '2710BC110FD2', '89AB334EE7ED', '271011381AB9', '8D0E03A58CC9', '5E5A892CC4F5', '2BB14634D9FE', 'F32A0A41B697', '8846B3225DEB', 'CBF22594F6C2', '3063BD6D280D', 'ED42630A9FFF', '73AE93492275', '11C341083061', '6B34AE0F8DBF', '42B977C6E77F', 'F535CB6993A9', 'DA6BC6194FFF', 'E96117D22492', '5273831FA984', 'AD005493F9BF', '31A655925A8A', '86D1F207868B', '4BB5ADD5A1FE', '56CE93447F80', 'FA1E56F220C0', 'C7F1256D1E7F', 'F776F9881474', 'E09853233CCF', '656F48B15786', '16B7CA362E71', 'DC5C9B011657', 'B68ED8EDBA5D', '31CAE911027A', 'F955CE7D650C', '7A3E4D5F7F6D', 'B330C76B790E', 'A90544B36B14', '058CA87825F6', 'F5F96EBC0FDE', 'AA0B42CF00A6', 'BE68B9E53CCB', 'C5A3278EB458', '4F51DB73DF1E', 'D69D856DF697', '8F8B11DD9ACC', '159FA6D54C9B', '04084A8CDF16', '665D11F129DF', '98B95C21BEF8', '3009A8F385F7', 'D6B7EE7B040D', '2275932FD0B7', '4885E69D7C24', '9172F30C29E8', '139B1A57D6A3', '6BF54A9F8383', '8D3CFD31B4EE', '20A0C1C766DF', '060D9E77A5D2', 'DC9CBB208C6D', '4BC4DC58DB30', 'BE62C5534E8B', 'C43F61E4FFAA', '101256FB2FB2', 'AE6462B51991', 'F34DD475F104', '360F4E7D2631', '5C44B0B1C2E3', '44442D65C858', '23C514D3D801', 'F854C455EFBC', 'D0859D4FA7A8', 'AFB2995F886F', 'EB763115B6CD', '62185A8460AB', '201EC18E0A04', '322B33C931CA', '20EE7E422C2D', 'EC1FE04E8EE2', '08FBDF474AA2', 'AE021F85F4D5', '80031EC3D9B6', '14543F568419', 'B49ED6AC9AF3', '43D612E69031', '955C86F2C441', 'ECFCAAECAD31', 'D83490F6B0B5', 'EC0988E6BB43', '57A4BDE06D1B', 'D401F5D87E45', '5789A415F29D', 'F6DA839FE9AF', 'A97DE0D49AEA', '2629ECCFFE67', '58EBDB3621E5', '31DDE751E2BD', 'FA5C17345A74', '59A2F81ECDB5', 'D7D4F8C7FDFD', '64C09A835374', '255C21BE7CFB', '2AFDDA4D5D6F', '9252D20095BE', '3A240B2C1380', 'D7FFBE6E0434', '66B20410E2E1', '57691ED21F01', 'DC364EC72968', '161DEDCDD17D', 'D9A51EAE079C', '7C1EAA5096AC', '9D263A8A768B', '0E08D2567318', 'DA4EF8FD6A18', '0119F710D008', 'AB8EFBD82820', '560F96E36796', '6B9F6BDE002B', '0A6C0B6D3925', 'B04BEFD69D01', '3ED58343E367', '46CFE621048E', 'BBB0C6DD06E5', '6EEACF2CBF27', '17E36E840DAC', 'FA205817C3C5', '18A2E82D5F05', '01457B62F341', '16F6DD028926', '1E0E94F07481', 'ABE633932976', '650A40BBAB6D', '36E3A712A2CD', '0E37E20C3021', '78E39B7962F1', 'F566EA761D0E', 'F9470AD105B3', 'B1EF45488EA7', '17E0D40B3CB8', '6AB345CECE0B', '4E97D92F2119', '689A9ACF5486', '8498B3A8642D', '03A87384F260', '66ABB3042298', '63A014130F9E', 'A1F48D16C67B', '85D0379B23AF', 'AF744782945F', 'C4018603C469', '14DF107E8ADD', '675083F24FFB', '41395A333EE6', 'D5736E7D21F9', '0B606C33AC4D', '49604EAA151E', 'A300C28B76F2', 'B33BCDF39B04', 'E7E8F2F6314B', '3B3A47EA684D', '793D97BCA3C4', '5EF660E481FF', '1BB1E58F694A', '278C1879DA87', '60137928C48B', 'C160AFBD9D19', '9788FF5FBA3B', '89E1708E4BCA', '8A0A3FE01FE7', 'DBA1BE0F3883', 'C4F760DBDD16', '6DFB980B835D', 'C848C44E3004', 'EA9FCAE3C575', '03A9CF7C7141', '6B0402639A88', 'F9F27CA527E7', '3CF52C3ED074', 'CE97B1D61822', 'EAC03FA78D0C', '6B5D04317738', 'C1F74C98535E', '65AB218C2765', 'E5C0680158B6', '24E0B6DB7B49', '4F829961E964', '4F4C3717EEE8', 'F42FF5B7D208', 'BFC4CADADD49', 'A15649448050', '9B90E0AB50AF', '871287046F85', '180CC35C87D9', 'A9F764775832', '9451FC111896', 'B4BC1C61F5DA', '9D40E94D6935', '9BAE97BE9789', '9A7F7ACCC1D9', '77F3A3BCB5DD', '0504EEB01AD9', '63E8EF40EF1D', '0C751C31DE85', '75E100F355BB', '0459F1BBF74A', '547C43CF3291', 'BFB4A021437E', '2C7EDA83B0D4', '554DAEE7BBCF', 'A38F35A9F711', '54FB3EA961E4', 'B18CC50F5A1A', '7D391E32A173', 'AEB7346EA9A4', '0D944DDF9492', '92C8A41A0412', '0AC09FD13E72', '0D221F0798ED', 'DC6181711B08', '63B0C1CE1743', '8391AA01FFCF', '2BAA8312A66B', '5E1A660E4E8D', 'BA596EB98749', 'FC3BD2A0D23E', '03755AB0A62C', '3648A95056F3', '205A433D0CAC', '8726B24BA724', '33D741DE2BCA', '4125C404E34B', '1A243FC51E59', '4A19AA269331', '4EB6E7B8DEC0', '05A474FBA68F', 'A4FF5E47645F', '0322EFDA120C', '896F322AE281', '523EBD9ECA47', '97A506FAF875', 'D2946A413E6C', '3ACABC081389', 'C06037835919', 'DAAE03562B58', 'DBFD15D537CA', 'E0CF4993483B', '78B951722074', '9D7DC07877A6', '0CB2758A3289', 'EBF9131A638F', 'B736B35EC849', 'C6278E01D797', '286C3B565456', '26F0870BE37F', '2BB9A0AD9F82', 'D6E91124E3C1', '85D7C9ADFE4F', '42F0BEBB42B4', '453F56D5BB3E', 'A3285BFDEAA6', '39DD51C9D811', '411AE0653A8C', '0CB40DD64679', '569CBA2B3B8F', '150C879555C6', 'C7A316555DF7', '3C28743B377D', 'D2ACE090B6E0', '1719A6C849E0', 'A2A749DEC4BE', '9FC925C6A20D', 'FF7AFA7FF8B6', '644F08941EDD', '62E08644BD3B', 'EDF68039869C', 'DF1AA45CD325', '4B9F8C01BFA1', 'D63BA0463CD5', 'DB1F750E70E1', '15F4FA869635', 'BFF7296A29FE', '1CF95A009AE5', 'D932E9477A5A', 'BF7C67E62F30', 'E00E5656B607', 'F04A513C2963', 'C32D1DC4B036', '6A11804E60B6', 'B3165EE32FDC', '1529F615275D', 'BBBB5C4705E5', 'B80BB8B7FC15', 'AC594194F01C', 'F7CB662CA783', 'A4FA2C4BB8C8', '353CAC9AC658', 'E2630767E36B', 'E7F369EC8050', '7330313ED3F0', '62274F02BC50', 'ED830B1151E7', 'DFB4C78A64F1', '836BE35D2E45', '1D620B5FACBD', '242207118314', '8DCA8AB16C24', 'D20C40B1CBF3', 'B84A7F39D6A1', '56121F282D95', 'D2F390598EE0', '51E6646E0733', '123722DA6464', 'F60ED753621E', 'C39B288CDD5F', 'FC8DD899209B', '2C6E6657FBFA', '62AFBD98AFA9', 'A1B5025267D1', '39568660BDA5', 'CFDCEE813DF6', 'C8C168A9F3CC', 'F2825D154E51', 'B92B378E1264', 'A7EC6F462F8B', 'EF907722A512', '7648DFDA439B', '4AFC0853D08D', 'FC4FBC43EF32', '8B4B5C6DD5DA', 'C5C8126C1E65', '7C1D05780C33', '72D646FEEBCD', 'A29CD7941E1D', 'CE38BA1D5ECB', '92DFAC1BCB02', 'C3EEDCA21B42', 'BAF6A144BAAF', '072F9AEEC965', '60714BE8146A', '7767F0989650', 'B1B0D8237ED0', 'DA228B928B61', '6A97E65BBE95', '0878DADF1B3C', 'CCABE8EEA6F1', '15FD2E229EA8', '46425944AD00', 'A3BB032558E2', '0408B654C48C', '13299580C8AD', '9D1F1FD728E3', '22C152CE5B3E', 'E106B08D9BD5', '477B00F899E2', '5FD6686E9FE8', '0CAF8CEA967C', 'CAA042CADA38', 'DA757F0B202B', '09F7F5FAB692', 'CAE7D71D66A3', 'CC02E6A858C4', '1B4B715EAC9A', '19E57B0657A6', 'DFBB3878349A', '8CEB8AAC385C', '75C4CC1ABECF', '4B4A73217C3A', '99AE1C6789B0', '3923909DE518', '3F51F5840A15', '2CE725C8E928', 'DAA1B0DC9448', '4B64E914C68B', '51C6FEE51683', 'EA4763734037', '7D08A1D7086A', '86FB71F29A12', '214BC7CE00D1', '140434C2B180', '0B25AA5D6644', 'CAB6E91445DF', 'D514ED6A3665', '01D04F3DF6CA', '46EB864A3362', 'E04E83CEF5DA', '5A4B52D6101D', '38F38B0E6F05', '48D4153FEA5C', 'C6ACC74915ED', '9DF9D4AC395A', '21A359FEFC20', '606B556FD61E', '9FCF605CC740', 'E5C524E8ECB8', '4C1B24495AB7', '40FDE82C864D', '134BAA2D43CF', '7AFF516471C0', '3CBB0AD46687', '19B5612E4AC4', '8EB7CE98B8EA', 'CD7714C1464D', '9EF555637FBC', '3BA0E26A657F', '6D8AB204C4AA', '2505E3B0777A', '1D549C77CB16', '2F740510B666', '848CD0BA745D', '784ADFA51958', 'B90C3ED7033F', '1830EFB03CF9', 'E659800B906C', '8E446F4DC56A', 'BB92579586BF', '1842B5E46878', '044D921A8C99', '0FC97658A027', '01DB053153E0', 'BB3615AA4AF1', '53B68DAFE78B', 'DA6D07A177C7', '942ECB176B3A', '649B2A197C4A', '378005A2C405', 'ACF8D85BE4F4', '9510F68977D7', '1CE967B8E5C9', 'C86A77CEF091', 'B7BB7AA2B5BC', 'BA5C438CC087', '081A0EDC2648', 'B4F3E5D8D7D2', 'E8C1C58F8A0A', 'C9EF98006357', '50E905BAF3AD', '89E3EAF87D82', '1BDDBCF9B071', '4D30DAAE61A0', '0333283809F6', '198ECA5E2E5A', '4000B8222A07', '1FBDEE7FC78D', '603D3019DCC5', '70E008E55102', 'BD0D7907E4D7', '2D508D3CD27E', '202A2848311C', '859CA9917AE5', 'B92A891555CA', 'AC134C9AB3CC', 'EAD0F017B5D4', '3828201E7783', '1CCDBD707F65', '13C09EA02E56', 'C2FA7273B525', '8A33C5C1A579', '45A87F46D89C', '5086805A319F', '61F3B1F549CD', '2AEF9CECD887', '7EECF753189D', '1343B0769174', 'E9E9AEC9100C', '71D831DE643E', '0222E19C41BC', '9FDD8A146288', 'AFCF10293B9A', '7863164BA896', '80E78F9EB937', 'FD8B18756C68', 'D84271FC252A', '4A0B12C1D10B', '4A383338D870', 'CC5894978F31', '929CEA8DDB32', 'E4F9AEC51ECF', 'F1A9A822A619', 'C4BCD14367F4', '387FEC6C2CB7', 'FB0053511F2F', 'D8406ADB1493', '766376432498', 'C2A49B846D05', '8248E7E6E2A0', 'CE3B81539AE0', '65E3280D899A', '29F5392497DA', '8742BE58A671', '51C3D7E06ED2', '5A510998E08B', 'FF9202FAA566', '6A290D8D6729', '70ED78541A7E', '95D8D99C517E', '36FCA185E7E3', '6B56C630FFC5', '2EEA9454E26C', '403C86C75FEB', '5F931A3FF8F9', 'A0928D4B7EC7', 'BEC88463148B', 'EE194115BAAF', 'A9E9851F3E7E', '0168325D0E24', '30AA3CFC1A36', '62D796BB3724', 'A8683674019F', '35242F4AB789', 'D828BE60BE57', '46DFE5507DA3', '38F4CE28F1FC', '7359BD785A25', 'E459422C794C', 'B4AC3F641AFF', '05EC08C05019', 'C318777ECE9F', 'DE17AE0AFF7D', '0EE9980F4C4B', 'BECA14914CFB', '3C315C4345C7', '4155D50399D1', '8CA7DA405671', '2D9010823094', 'D8824F42C5CF', '38066765F81C', '09F5B52032A1', '5CE46A992889', 'D962FD67FF4C', 'F11D449380E6', '94DDDE49F8DA', 'EDE56797770A', 'FE129177A067', '6805FE96E548', '675D63C3D265', '94810298EAEB', '43B24ED85767', '33008B95DE9C', 'FDB27AC9357D', 'BD7F5D94AFED', '8E915F00ACCE', '2A2CDE97B76F', 'C70E3EEED307', 'FE3CA06DDCA1', 'C9B23C5B1064', '24C5ED476177', 'ACA7AD2E5716', '2B4BE109A9D7', '530F622827DE', '163AC455E1DF', '94DF4BBF522F', 'BCC1433ECAEB', 'FED359DCA0BF', 'BF3EE6D66E3D', '1A068661D19E', '4336E25D1D6C', '73B8CA4B266A', '26432FDB0644', 'D6A32ABCAB53', 'E22D53ADC06A', 'BA5056B8C67C', '3FCA40583533', '7CF24BC81BE6', 'AB0403712FA9', 'FB8A6D53974D', 'CC68BA9DCB2F', '52835AA731FA', '6587820089F3', '377548575048', '9C4444C557FB', '46527BA000FB', '01F69867D1BE', '18EAB669FDE2', '8DECBFDC0EB8', '2DFFBEFF2B10', '9399DBE21A35', 'DB4FE19B8FDA', 'D3B5E0743515', 'B7685AA86F85', '01A4C886D048', '1C346211657F', 'AE47AB399CF4', '503E1602666C', 'BD304A35EE44', '64DF7B801614', 'BE6FC06B8795', '288183B3184A', '1C3440A3FFD6', '331588D6432C', '4779C9C9093F', '89822D57FE56', '6E1B4AF3CDC3', '3919F3440FB6', '4FFA8AA9820C', '28797216EAA7', '8B4BBB3F601D', '25C37AACECA3', '67680A4C81E7', '0BE6935B5A54', '0BD82CC1FC8C', 'DD95B403F95F', '6B3AF4AD81EA', 'A27F8CE559C8', '63C72E480CA7', '098353275507', 'EBA1B05B6624', '18554EA0420F', 'FF62E6972211', '22916F4BFF97', '5B9FE6CD8325', '0BB0E957B6FE', 'F362313AF4DE', '23A195AB438D', 'A7BD93C544AE', 'BD33192DA18E', 'FFACF4B90AB1', '4FF7C90E8071', '8F46AC3C99AA', 'FE62304F4B86', 'BAC681996825', 'AE5B6C4FD5B7', 'E87BDAB97148', '3799343B6C0A', '87ABD416A9CF', 'F955CADBB6B4', 'E418CA5F2D7A', 'B9347CDF2461', 'B38B82E3F538', '27D34E27FD4A', '3BA15254E27F', '4A28DE0DB485', 'DDB424217004', 'DDE62E929C6B', 'CED03E44340E', 'F98C8AAA5BE3', '3206E3F5CA39', '12C40C11C57C', '8CB1593BFA5F', '8B095EB24288', '022DD0BE5A38', 'F9E1434B2151', 'C1BDFD85234D', '308DA02A219C', 'ABBF7FEB056D', 'C75B9C4FFEAD', '4B3862D5B402', '62AFFAE17116', 'EF08234FA6BE', 'C48F269A6ED8', 'DD259D664877', '987ECCF90CE8', '0F13AFF35C0C', '9127434E8CF8', 'A282A18AA03D', '3DB6FCCA1445', '437E9376FB71', 'FDC5C6E0B4A7', '71DF40C12652', '04DEFB0B11C5', '008C191EF21C', '8D509BBA7DD5', 'CA92495BE92E', '532D74D761F5', '91FA9C603A6B', '83948102B382', '17C13BE317AF', '0EF91AC5D457', '5173FB3C75A9', 'F9A5B72FAD82', '043E9B5AFC5F', '908906779AB1', '934168169D89', '109FC36735A0', '8A00BBFCFABB', 'FB9569BDE4BB', 'F3454E01E31C', '9FEEEA2734D2', '83D8A875FA7A', 'A550B8EFC039', '313229323B6E', '75B94E37D75A', '655A3B3F19E1', 'C74D540B715B', 'E223E52018C1', '87328D07C574', '2AEEA0A97907', '4425CB6C8585', '4A4ED75E7112', '2B3601A69135', '7DC8112AA69C', 'C20A6EB7FBCA', '0E5278E12B82', '558BB83BB400', '916117BFE2A1', '201EB1D0F1D5', '94A5C5B0F0AC', 'BEAA8D8A3EFD', 'F5BC72AE5196', 'D4A3E7EC982E', '9D0665A39328', '8A533A6E8808', 'FD48976C9913', '542FD18E9ADA', '905A76FFA540', '2446D9BE78B8', '922038AC1578', '6EC2740A88BB', 'C1B32D871830', '5DDEE376DDFD', '138BB07570E0', '91A358248CFC', 'BB207C714EBA', '6F8286D752B2', 'E125A0E0FD41', '6BFE834C4FD5', '7A532625FFDC', 'AED200889835', '1DCF42D2B503', 'DEAF202F9EB2', '0EBD16DD8BBD', '3EB880F4A4FE', '65994ACB31EF', '8D3BE9EAB1D5', '2333F75DB5E9', '8783DB652376', '11823089B61E', '26981B151FD2', '4CD7E8A7BA22', '96E0F82A7BCF', '31FFFAFD3987', '46462FB89C84', '8CD442C14869', '61BABBBB9822', '3A9F1E5CEF61', '2FAE6DFF45F1', '03C54838ED36', '586269D6471F', '23156B9E3E53', '37B05EDB1C50', 'DCDE7152B94E', '154897339487', 'E94074738406', 'D4108B56F2DF', 'A66F364491FF', '471611351FAB', '81F1DBD56E0D', '89039D21E960', '614C338B9D97', '268675CF9C2B', '135D6AEDDA9F', '856462BFBBB1', '7F63036D0C59', 'AE359CF88362', 'DC2EE6A2F024', '791E0902EAD5', '65F5C8D8AAEC', '91125D5EEA38', '6C4D40695123', 'E397850CD3D2', 'F7E9EF1AC55A', 'C541FA8BBFDB', 'B18624117239', '83B1B27DE86E', 'F106A7FB6C5E', 'B5B5BB3CB4EC', '7D9F1CFFFC95', '4D0233F6C1E8', '0226E802EEEA', '58C092C01C79', '10F4C4CDB76D', 'AA38015B5E93', '8C2D0E1314DB', '4F7F24918F42', '2C871443D686', '25664FBAC604', 'FBFAA80ADC60', '2B605EED5E4E', 'B0A19F407570', '82BB2259D709', '35011B0C0225', '9680FAE8D811', 'A53E02BB0A41', '1B061D85405F', '2B49B48C3B2C', 'C10C521A47C3', '7ACEACD37436', 'B0C7779B7276', '89FCD2B81990', '1613BD216385', '5448E486707D', '9F1CC7603B64', '1033B94279B0', 'E88B3A355149', '49ED588E79B1', 'E7B3A115D111', '5D3F76E36715', '2FDB1B6086C9', '7804C4E627BF', '8BDA09CC9FD2', '66E83D44F4A3', 'E4DE4FCBFA63', '5CD4B139FCE1', '8E6166FA732A', '763AE37F4BD7', '715555B20F91', 'E7C8FB41C6BB', '15B1E4516018', '52432D25C86B', '665A2D50F034', '2E1C25D6F265', 'AF938D285908', 'C879A485810D', 'D35AE4A7CE83', '5B14E2CB36CB', 'CBC776495BE0', '0AD7535C58C0', '0D89C8189022', 'A67D3F549FAD', '86D78A2CD591', '2B2696900DDF', 'EAD34355445F', '20B696B5B9EE', '1D45838B86D6', 'AE82BD8A926A', '2E4A8624CB67', '03F81E8838D0', 'BA9B801E3540', 'D88B4501DA8E', 'A70A57F5752D', '0F7DB6BD1AD5', '83F0695E035E', '6FF14DD83450', '44C7E9C4260F', '0BAF08001514', '8732FA1BF586', '74324AD36BD0', '6F817602FD7E', '3A38C6310D54', '281B30A5EB30', '55C612B32725', '5E7DCF4A5293', '7E37567419F5', '8945D160056D', '0AA050C570B0', '1202C1C507A0', 'FF7BA63A1993', 'CD86F7930D58', '8AB07A248878', '0B54CC968D4A', '24FA36D49DB7', '4918D9A545C5', 'B0E17505BCB3', '47B65C7E16F9', 'D14F3F93CF54', 'A00DAFA2E4E8', '7E026B760EA0', '4BD9A9FE0FA4', '93A9B27859BB', '0D3737E9F443', '60B9CA06540D', '58FE015253B7', '2FE09CD5CD8C', '336F8093CF86', 'C212B1F1C14B', '25F9B9BAA02A', '7DD34B45537F', '317BC7AD88EF', 'FACA3592FCD0', '9DAE6D2462A8', '86A74374E724', 'B3C8CE820C5D', 'FF21CAA2F125', '2C2CF03355C9', '8A895792F3C7', '66031F0D0329', '75EDF0FEA5F1', '5E9B17A298AB', '9263362F551D', 'CC3A5AD37340', 'A602D45D22B2', '12725EFCD35C', 'DC5B8E6C23E6', 'F24858D7F8F7', '3C879A5C86D9', '727F83C82B08', '5C29D02F268A', 'ED1EFE97C40F', '9C20E2584A8B', '5394485B02AB', '00D719F00D9F', 'A851E38A7344', 'E4630B623165', 'FAA1621106BE', '86E1F1C251F8', 'C68C60B718D4', 'A2745099DA0B', 'A975647F8219', 'C1CB555641B6', '688370E9E6F1', 'BFCE87556F2A', 'F7561C027920', '312C67D3E8B1', '824B1332670B', '29203AAA6CC5', '952569CCDD6E', 'E5F47974A6EF', 'EE6FAC759013', '0CE521F8D172', '0CCFB5F4E6D4', '45A5FD941F4A', 'F10FDD44873B', 'BA3D6BB860C6', 'F18B05581BAA', '505E1F0E9AEA', '4CDA5FCEDDB1', 'B9688F9C3150', 'D703C053AEC6', 'B6261FF12B56', '7A2D0B189298', '98A9BF4E795A', 'F407D70D45AD', 'FA6FD7F076C6', 'EBFB5DD26BF0', '2F889A4F325B', 'E1B4C22E7785', '22C6C0356324', 'B056CFFBFF3C', 'A5DEB6E371EB', '7782DC357595', '1C41622B0A7A', 'B3D92EA18E0F', '8E83FB707B87', '5F58F22A24B5', 'CA0C21200F78', 'EF9E9C4E5D1E', '18860E0DA892', '2CD086E87F2D', '10E592ECBDB0', '34A463C4B1B4', '0B4FAC7A4A8B', 'FED2903D0697', 'C07E3C23CE7B', 'B8A754A0B9DD', 'F17D56052066', '2275D39DC5AD', 'EF814D7762B0', 'A7F8EA8D1E7A', '4BD30FA12457', '31C7E435A083', 'DF071F7D4B04', '68E7029BE2C0', 'A6FCBC1C476D', '4C60FE6BE07C', '91C1E1BA845E', '6B34BD2CA144', '47E3B3E558E1', 'F9BFE8C8AD17', '703C3A3D102B', 'C7C9C01597EA', 'A4436BAB9532', '66FE148BE39B', '008B27E80228', '04AAE2E8EFB4', 'B9D9B02EFC86', '3F30E7FDE6F1', '4698AB6B27C5', '269E80DF1073', '1BB48664CC6A', '0D6F9BEBACE4', 'C3D4F36886E5', 'F98C913B5446', '849911F5126B', '99451900AE17', 'D6278021E119', '95D479E8C7E8', '79B0B907CF87', '34AB53DA73EF', '40C8F8E9E046', 'AB1AAD6295FC', 'C9C826197027', 'A5CDE9900195', '7F513E5DA945', 'AA04FD5FC3DA', '75614E82027E', '857F67692BC4', '0F571C3502D3', '887CE761179C', '45CBE29D23E0', 'C8EBB096262B', '7B4FAE7A57C1', 'E018497ED277', '45DB353F7471', '422A3A47C33E', '06A9B4BC2E90', '546C0401D25D', '5B2822CE075A', '7440926E8961', 'AFF882607973', 'E52E56670B24', '0C7C06A6D656', '46D1A8FC5D49', '7D14E283380C', '13F1DF1DB4DE', '95ED30690B8C', '4F39C001C736', '2983A5DC1E8E', 'BE5D3ADC4F2B', '7CB443BFD73C', '20D2FEBD3853', 'EB5021B098DD', 'C4D39AEFAA87', 'DE0275A222C1', '99FB1D4C4F5A', 'B51E027B51AF', 'CD33A5B0A307', '2E7B7DA84C7A', '31A3727D27B8', 'D889C48DD497', 'CC2FAE6B5135', '94EFC5F3ABB2', 'D44639122EAF', '9411BF4CF714', 'F65618376429', 'C59ED272B57C', 'BA991FE296BD', '2FE7098FEF58', 'DE842CB71C02', '5AF8EE583A33', 'DF723D10CC23', 'A46BF87BBB48', '564FEF97AC52', '2BB45C0C7802', 'B15B3D1D9CB5', '5E8BECE0067B', 'FBB35C3EF339', 'A5FCA4CE2EC3', '158F2C1BBEF3', 'BFEDA9FE9F1E', '1C9E388C17B8', '2DE70542C32D', '3FDC2CD9FB9F', 'BEB797E37AB6', '9A6C18B25037', 'B996860E00A4', 'F0E863BDEB82', 'FAA7B824C589', '2B6B50852E0B', '9898C2839018', 'D2A1577EB7A8', '9BFBA5356167', 'D3DB341F4932', 'F45A719EB22B', 'C55EA5FFDB2D', '910B844ACCA1', '3E914F66058C', '718EEB2328FC', 'C2BF4C553358', 'DD40AE6E1E6F', '761B16CEE6C1', '6E919842050C', 'E7C5E12F4E84', '13E8A2F76C57', 'C6E43B28660D', 'DA41A7B07DF0', 'A07A05AD9104', '97E8C5E4AEA9', 'D75B4D14CCAB', '956B5E2F3BEB', 'AF642CA41345', '62E0E0159843', '70B6F3CF220A', '52043A84C3BC', 'AC871BBF14B5', 'BBEB4231A8AF', '7CEB4836BAFA', 'AE8CA585DC1F', '8975E6D0380E', 'BE243683124A', '09FF4F0D2B79', '288312CEAF6E', '4DCE9956E3DE', 'C7A018905E52', 'F038AB4D788D', '64077B385B48', '65C1010EFD70', 'CBF57C499977', '493DE86CC0CD', '7E20F422A3EB', '1BC811EC52DD', '980850873AB7', '3F65AF00FC67', 'ACBB0618CF1A', '5E75902533E2', '53EEE7A5B5A1', '02A6AE54EB79', '05E82CD3394E', '855FC08FC096', 'C4AC99D9CF4A', 'AD3821286500', 'C646479A45F3', '5445F0F5D9ED', 'E518FBD13C3A', 'E04DC60DB4F5', '8CFD338AFB98', '25AB1A453D24', '29687C1F98F1', 'DECAC443C667', 'EC37D36974CC', '2114A12D2095', 'F6CA5589DCDA', '0581A6F4E804', '6F326F032E91', 'F01B0824E489', '6DEFC88D4E0B', '04AA83899997', 'B864F3DC71FC', '7E26851E8EA9', '571BE3159B48', '1D369D51E5ED', '4A5AB644C41E', '4579B625E1E8', '60FFF26260C1', '79CF90622262', 'DAE3E4057810', '8352B07F2708', 'AB6284069E18', 'E4BFE6EC3275', '58C4A1F4BE4C', 'FFE46004CDFB', '20C0FA255C6E', 'A7B788E51FF5', '971158C1AE69', '3D64CDCC5B12', 'C5606BE37BD0', 'BDDD49DF0C7D', '51464A01F6FC', 'AD19FCCF4D66', '6A2FCEB4F2D1', '943370C19A7A', 'ED27B5020694', '67683A36D7BF', '51F05D3678EA', '1A88DC345340', '793517BB1BB3', '2C8BAD341085', '079F9B97A2AD', '882808C92A6E', '22075172920B', '2CB0A75DDF7F', '4B51E3917E0E', '71D3F6E61C14', '5E57B80BFC66', 'A6B3770935B6', '2C191F7F9FA1', '4EEB3CAA6F48', '97EA28FC8ECE', 'D784E06655CB', 'D2BD6E5B0D1F', 'BEEE3AC55C0E', '9E1B8D4EA6B0', '93FAE079492B', 'DDDB09DD580A', '8855A0898835', '26788B95F2F2', 'B1F95DB388DB', '4896AD6DD91E', '9483857CAA80', 'E5F3EC28F670', 'F0095A6449E9', '84CD770A3BA1', 'E67A118F9AFA', 'EBCABB635458', '16D3EFBE11FD', '4FFC4DB08B42', 'A02D5B12D66D', '29BE895CD900', 'A603DBD5D182', 'B0E846E42C45', '2FA02A1E3654', '2C3E3E48A880', '00A4CB36E006', 'BFBD47C22FB3', '9F7BBA5FC646', '4ACB29936C94', '0A23334002E0', '3A748C1D650A', '48B50A9C7D0F', '352FAD192BB9', '6FA5454B11D4', 'E7DA3EBD8366', 'F4EC42B28077', '448D92E762E2', '840A520F0D99', 'F40F9FB07FAD', '3F7A0163C8E7', '4DF55925F544', 'BE3908E29A1D', '22E223A16B82', '7DE381011700', '1871E7C9919E', '6B3C70C249F0', 'FEB736959AD7', '923043ECE82F', '29BB45AA907C', 'AC525B50F33C', 'AB69A8EDFA20', '0433D61E8652', '0B720B5EA0E9', '273863D43441', '07C6107B301F', 'A2B0C80404FD', '3B355996A407', 'CC50C8238771', 'A7AB35E04C19', 'D54F90711EA3', '5672F9F49336', '5CD2BCF64F4C', '0B508FD837EF', '6D681ACAF7F7', '0A5BA91AA8B5', '0C0C473FA794', '44D73D95E310', 'C94D88474063', '446DFCDAD7F4', '192A27F0608C', '738E215A237A', '13D83C05F4D8', 'FACF20D0D847', '85B5621E1876', '44E343ECD9B7', '0F6250B6D784', '58358B109222', '0AFAD06336F3', '2252B166CE0D', '75146054E72F', '7992F4E1E3D3', '5E1DEB2AAE39', 'D9A50E9D3E5A', '0DDCAFAEA487', '46DC70997CEB', '84DB11599420', 'EDC00F1B2823', '0F43EDB760C4', '6CCF20B2E727', '36929B945E29', '2056B171116E', '2AC878B831DE', 'C5565E4761B7', '37019C8C1ECD', '5F27253980BB', '65DB75B187F2', 'AE587E8BC0D3', '2BB327C90883', '651CE6F22A06', '4E1F37A9728A', '21C30597D17F', 'A1B2F28757E3', 'C7BB1DCC0537', 'B44F4F2E6178', '5B762EBD64C1', 'CCD422BE9DBB', '20498D2BFDBE', '1788B6653312', '27C2C3000FBC', 'D77794179080', '7FCBE68F5923', '0646F7BFC681', 'E059AC481E5D', 'C321DF7BCA1B', '126747A179A8', 'A5977AF16661', '6E691E18CED4', '64BED9417DB2', 'BEF5E55E29CB', '463F959EAF89', 'FA15A1E3BF2C', 'F6056CD0CCCA', '3F59617806D4', 'C55E03052E48', '25DECA78E234', '9DC3CADE8953', '5729D5AE055C', '0B998187123D', 'E8D55D0C4BAF', '2AD8A9FEE4C0', 'E06960D4A6C1', '3BBF8A0059DE', '61B15EC2F7B6', '9C3DE25AF92A', '70D2F22E29D4', '53BA0A8722FF', '80E2B7016611', 'B23941DFCB72', '6F220B8B2495', 'EA3B8E4F74DB', '3316349275E3', '825C7DDC0008', 'CF0214901725', '232A55A50FF0', '49F6198DE921', '6E154A9B7943', '552ABBDE1296', '55B4B2BFF231', 'D22D39A0C3AE', 'B2EE7ADD5BFA', 'DE01C3FDCA0A', 'A7441C4D1FF3', '3A9A95F48F86', '3B2C8BB36BD4', 'F19978B8C2A7', 'A34FF5F00E3A', 'D354980E4F07', '1FC7106860A2', '5DB4FE5D6A00', '8D4DEA77D545', '8FE284BA4D25', '1BB2EB85C41C', '5656301109BE', 'C8F035B467A7', 'F3ABEAC1432D', '8BA8BA5260AD', 'E797D7F9B452', '1E4261B396CD', '0985602832CD', 'B727B720430B', 'DCDE1C43C218', 'B6F12A1662D4', 'A15776CBECB7', 'EA6471CC4E00', '9EF6AC7590A2', '5AA83726798F', '7A56F29D9707', '6FB96EE78D6F', '04810832D23A', 'F6F8929C3714', 'D470DB16B85E', 'A54C0AA815DE', 'B6C009160238', 'D4375D1ECD7D', 'FC9BC150809F', 'B133FCABD0AF', '90345B5B064D', '84425797BC28', '1D6220DAB645', '1B4AFEB6E86E', '160C8B8C0E64', 'D85B93C13556', '47FCD0F878B6', '32E3A5BDD0B2', '80CAF7FFFD56', 'D195E4354F21', '8F899DE6880E', 'DE34EBAA18EE', '6F096C626578', '37A77BEAD718', '1E68FF259297', '0BCA18283DBE', 'F1860C4149BF', '3C4368A07D88', '56774BA72B7B', '25086B7087D6', '52B24BAEDC8C', '9DDC8FDAC3DA', '89C921B360F7', 'DECECA25BCBD', '58D5E21C7B46', '4B14E9E6D731', '7422D711C2FC', 'CD17678005B9', 'FD311907E3DB', 'AF05AA1A49DA', '0CF568439ADB', 'EF5579B524B3', '69ADC83F4BBE', '410754A81FB2', 'D58DD07BC0D6', 'E63689D747FA', 'A4C42F2B3A57', 'A550B41E7CD7', '097FE8053507', 'F86616365497', '5403979D49E5', 'EB16CD7EC3E6', 'C4E233F5D9A9', 'DC93D8BDE631', '60E98C89CF04', '232A864E5A15', '82FF684D4DFE', '8F625951F759', '9BA24165EE28', 'CF84C9D788A9', '080E2CA04B67', 'BC73CEB66875', '0306244BF63F', '5843D4C6EFCC', 'E95621C07D95', 'EAC3596461E7', '6796C7366713', '113942A18C27', '446F3C285AD0', '9D000F734774', '06936C8AA35D', 'D23F65F4652B', '8B696D4A84A0', '399C14D08042', '0C1D8179D5F3', 'F6C40C564E5E', 'EC7C81E59E9E', 'DE7F02980A1D', 'AC8F2F26AB2E', '1B2083B9F74C', '30B617145AF6', '5AC16E188F1B', '030A70B9F194', 'A5B9C9F26FD6', '2D10F2D7A580', '24FCC5A84ED7', '7AB72C44704C', '8EEB5226121C', '24241413242F', '79BFD0B9910D', '1B87EDD72ED2', '8CDFCA2C42DA', 'C7BFC84A7A01', '005026E0386C', '7D6BE3FB87CA', '18D68AEC46D6', '3D84D6011B1E', '49D008420FDE', 'AF80EE1F7435', '44F6DC9C8326', '151645FFA225', '31EBF68660C1', '0BC531D9A1FD', '927708B6ACDF', 'FCA553D1B819', '4B048F74F3E7', '24CB17912088', '457D56541EA3', '0FF049B01070', '468D150FEC7E', '8C57C1D980FD', '159424F57C24', '2F159741CBBE', '58F2F77D8FD6', '6848515B048E', '7F1E6FE5D698', '970171A0AC8E', '831F54B2B19D', '0949D328CAC8', '413384DAACAB', '8A57EE89F26C', 'B86D09F6A43A', '45CF27F8EB92', '49CAABCFD99D', '38EA1B4F88C8', '84C6F616721B', '6058BEF565C3', 'DEB6EF459B10', '4B1EE53F33D4', '111D45750F1E', 'B1FC2FA3AFB3', '8D260545092F', '0FF781FB6050', '1F8DEBD4D55C', '623D70660238', '0D1493FDAAD3', 'E9621B1CE4F0', 'D84CB197450F', '3F6E2E7E7734', 'C8F378A4AC05', '7F7B48A486BE', '79766A7A3911', '1C1D49349242', '9C52EAA3F005', '5C89C0506A97', '8DD8AC01540E', '4D55CD8CCF85', 'D4620A5134E7', 'D5D8E7DFCF5B', '531C5AD9451A', 'A11D007DBECD', 'DE499912068F', 'C2B40B0DCC34', 'CFF21231DFEC', 'FE91D719ADAC', '8C887CDDBA32', '2BA142AF4BEF', 'D2ED07A7CE6C', '01AFEC143032', 'F789157CB6E6', '3D5C564AACDD', 'F2A3BE5019F0', '31F08BEA89C9', '2FF6A393234F', '71DFDFAB8621', 'C535ACEE801E', 'F492EFD91D3C', 'BD0A4A94AC7B', '85983B2516C5', '93B4EABFBBF3', '36BD7E8B0A6E', '3A0B15335CAE', '7BA3A439D22E', '0F5879D60008', 'DA645EE46524', '3AC9C50357E9', '1F11C3F72282', 'EC36E0BE58D8', 'B181B21A4FA6', '0FC32E083BDE', '080912B0093E', 'B50B1FFB4697', '5A3EB7A7A627', '4EA35584BB87', '986C125C24A1', 'BE58ABE09429', '8F8BCC081DC6', '8F85E4509835', '44B06B4A6D7E', 'FCF9802D7F21', 'F538469756EF', '6B6A7138292B', '2408A9D495EB', 'F21A02C66DAB', '84D2A6C75C67', 'C5B52323339E', 'DD586A625C89', '44975DEF87EC', '5B1C8A89B4DB', 'EF9D79A33BE7', '5BB702C07F75', 'BF00A0DFDA72', '4B7B7D384FFF', '2ADE4CD09C5B', '93C0100883BB', '838005D1D7DC', '9940EBE441DA', '57846986862C', '6783CF0808E5', '6141232BA814', '6910A7FECEEA', '939E66CC3A0C', 'C51E0C551CE7', '10C813389A74', 'E758B77AC952', '16180FD0AEF8', 'C3C371CA957E', '93106B20FE87', 'E30209138F4A', 'E51DC9CFC464', 'E76E617CBAA9', 'D3AFCB51AA40', '233969D3AD1C', '21EFAE02832D', 'EF1A6D00C3D1', 'DE78893DB845', 'E60CBA7670DE', '416FEFFA5DC6', '883880ACBA01', 'EAF6BCA737BD', '26C28DEA2690', '43CBE96B374E', '49083564C202', '052BD823F2E5', '011DDBA781B0', 'D6668C2DBD4D', 'BA1E2A47DC1B', '88B0F016492E', 'F788D4C475D4', '79F319143CA6', '288CB26FB8F0', 'E85C45D90D1D', '54D9E8287B96', 'DF5281802CF1', '84D821F541E7', 'EB9C5ADE320C', '0A43A08B43DA', '1CA9E28FCB00', '0E16BB8C40AE', '318C3D719F7C', 'E9E6AC0ABFB1', 'B5BFAD94E4E7', '32AA7D5E04E0', '27ACC3AD950F', 'A8BBE9026DC2', '23CC3629CEC0', 'A3EF457B5F61', 'D1FC358F8C68', '62BF23872BFE', '2568B64DB374', '25DE215F8B82', '0E8CBECB7CCB', '6F271187D67D', 'BEBB31733A38', '69E3314D361E', 'B8164EA79177', '602A632736D2', '3DAE999D059A', 'ABF30AB2C625', 'F983FFA8BB50', 'F460F653B33D', '4E0179E2F024', '6097EE3B178B', '0A7B1C9CEF07', '6C83EED20357', '9F93DDF89408', '98FD2BF096A9', '218490FD2E1B', 'C5382404A84B', 'F82207B7F26C', '203C3C0F1250', '11854A885ED6', 'B3B624F3F792', '302D33AA69E8', 'DFE2D3C4E33C', '34262AC688A1', '6DC268B2F2F3', '424419B0718B', '4C30EEDA3A8F', '04F277F6562D', '627AF801730D', '2C2C377C45D0', '905BDB331B7A', '286D497ADBEF', '45EA1B11A1F7', '1EB2AB9D50B1', '767676854264', 'F760756904D5', '35C0AB9FA522', '33E25E6A8028', 'B4CB3063482A', 'F5DD72FA313E', '435A90A63FEE', '71EF8337424C', '73886FC7FAA1', 'C739DE328A1A', '2554E3E50652', '8E98853DF374', 'A112011A5F17', '294CA62F98EC', '233A9C9D748F', '37A2E6EA2337', 'E036E3908A19', '71DFA360D5EC', '0BEF6D61E359', 'B6D833F8240B', '2017D32A0639', '1CA7B8FAA58E', 'AEFDD857A5D9', '7CF49BA27233', '91A00DFD46DC', 'E55C5624FD0E', '5E9E3C993595', '9BD314B82CC0', '16C808F5F642', '68F6A6DB3E6C', '71A434779420', '5EA13764DA34', '21B7E5DC72D6', 'EB3398FD0FF5', '902A59ECB525', '74ACC8E357FE', '301B3A868A3E', '57596DF3AD20', '30045307DB37', '878BE2C1F6FB', '871CE2BE3370', 'D8F3781C0ADC', '9B1CF41C7F5D', '121DCFFBC671', '34969C5FF82C', '440790E93DC0', 'EBE406D40544', 'C731FDC59282', '82856A9600F1', 'EB901EEC8E98', '6872BC56971B', 'DAC21768A84A', '780B8C757C48', 'E84C23AA6CF1', '7C12F166B5F3', 'D4FC7257FBA8', '6BCBEA10B6F8', 'EFB0DDC8EA77', 'B7075CD10BCC', '3FCDB5437831', 'D2C7D096655F', '1CAC3BAE4A39', 'A4FBD7A27089', '63263E72A196', '8AC33F8E9C35', '296D2E892341', 'C7AD757540AE', 'A4B47ECED161', '85C8AC30C12F', '82DEE52546A1', '47F7E409D879', 'EBA2F18D9666', '95874BE012EB', '49A315660C2E', 'DFE90DC3F01F', '39539B71D857', '2BD0B513E03D', 'AD8BBB12682E', '3193A59E5543', '7936460A2B7B', 'B8E816DA2049', '6E1194215953', '6EB681AEA81C', 'F7B9D106E6DC', '78A6A188B132', 'DE67BE4E7A3E', 'AF9079B5EF21', 'AAFC3D84F9A1', 'F3C14C00A0CD', 'FBC4F9ADED9A', '88B016E70B03', 'EB1500B8EA75', '5BC582396E5B', 'FA3E37900E3D', '077394BE6603', 'FD6E0C78E9BB', 'D840AC3957E5', 'F32665CC66E7', '69C237AC88FF', '6A11844C8AD2', 'E4DC7BF5147E', 'A6504C463F7E', 'EE440E670405', '96141BA9C7BA', 'A2291A8A41F3', '02D6A23E0CEB', 'E7741CA1BDC2', 'CEA16ADD393C', '2EB94A444839', '709EF21B80F2', 'FA87416EF173', '6B4829C2E5E2', '2761843E3DB6', '44A5406282B8', '4F822DEAEE6E', '5FB1A7C1D694', 'ED7F7B6E4C8E', '38B237B2F190', '0AA8E5C89F0F', 'F3A980C4613A', 'C4379D733479', '91A7412303BD', '6515C1CC0F57', 'AA4CF5537352', 'F6FFC6846BEC', 'D33CD8778DE7', '00F87647163F', 'A7036C8E5E6E', '2B39201EEEE0', '92A78941DF77', '8727FFFF8441', '71C6704EA7FD', 'DB0E754610DB', 'E1C5F6D592F4', '4F7E14B7D72F', '3DAE21FF789C', '8EDD5A7B2310', 'DE68828CC67E', '8A7B4B823C6E', '42B5C14AEDAA', 'F52B9A0882BB', 'C8D4EB858DA4', '1BA120ABF733', 'B0DB49483D7C', 'A3A00BA86199', 'D1A2D5B3B446', 'F8ABA40201AB', '4E07EED92E2E', 'C0E55316F183', '9986CAEDD2B7', 'E1DA1E513BE1', '18DFC8ABFA92', 'D88E55D44EAD', '0F2FF9C6091C', '00E3F86E3E6A', '7C236B88E0E6', '17B15F103D26', '86E5ADAE8206', '849BE041D1C8', '004AC288D833', '8A2EC5DE84D4', 'F6E5F858A19A', '2690D4B28188', 'ECE1DAD2A886', '66CF61A7F865', '8B7E97D3310D', '4EE35E322F9F', 'B74EC94193F8', '693503BD8CD6', 'C30B52D6E340', '179A2FE8AF05', '931D5E447FA7', '0299B6FC9E9C', '3862E2B8AD54', 'C76E52421ACB', 'F2D6B8E26072', '5A94F91C9AA4', '24AD94A6AB3E', 'DC76E46519A7', '15F149B4A87F', 'A932B7BEAC6A', '658428760D94', 'FD5AF33900D9', 'D0EEB2FA5D83', '0428DEB82659', '32E2949B0C66', '415672DB1873', '4510E0D4FDEE', '7044BE538016', '1F07CBB60940', 'ECAD3D8FAB97', '78F04BE79449', '996FD5290FB3', 'ABC648A86887', '1B2F469FBC51', '2388E2CAC922', 'A6405231F460', '3A9853B9390D', '4C1F76298E24', '08C05F2480E4', '5BED2E95A983', '6EE9F61F7CDA', 'D2851DC6AF41', 'BB705FBA29C7', '9114D5C32D36', 'E55640B069F5', '9BE033155063', 'CCF388E0F66C', '87258EA0A73C', '155CAAB3DC95', 'ABE45B3CCAD9', 'E7F67FEC3CA1', '38043B171DAC', '53B50E488C81', 'E0ED571AED2A', '7987F926DA46', '7C8C9A176F90', '40E7A7FDF48D', '3E95CBF3E357', 'C323ACC08DC7', '28DDECBB02BD', '9BC192B91D83', '759754F07F7F', 'E10D5E79B0A0', 'ECFAD2825EC6', '669C59404DC1', 'B4E8CF251A0D', 'A165288534B1', '3B067F759865', '7E7B3B164338', 'E22E187E1FF7', '9D0934987831', '4A58986914C8', '99D8B323FBB5', 'D815B0814A62', 'B867F487B9F3', 'A25945256A69', 'A885B3EA9C99', '44E1D37DCE9B', '504FFF4C522D', 'C1442695A931', '83CAAFEF8194', 'AB5D516FC4C3', '30687EEA6586', 'DE11D98D685E', 'C68A69F99946', 'C46CDD7E69A4', '710EFFD2AFE1', 'F2B82ED3AAFE', 'CA4E45151A4A', '4397809A2D94', '8E9007F863C7', '97E4E42863A3', '7348AB38D68E', '0D29A98FE8FB', '5312458F4FC2', '5E5678C3195A', 'DF9580284892', 'AB3053E08AEC', 'B150E66D6C5B', '193591CD2E86', '868D445794AB', '48C1CBC6D003', '3C7F30DAE13A', '09861D1CE966', '0FB32337D223', 'B9FFF4311016', '694238280A93', 'FC73A83CF59C', 'E6E97420C195', 'AB0F4F1767F8', '92C09304882D', 'AA6D6F1B1B54', '11F14A342E56', '743F1B934A96', 'E0D3580584ED', '3D613F2FF95F', '9EBA120196BC', 'F73ADEA6A74B', '958229D0AE9F', '899FE6A07507', '4C2FA98111DA', 'D7DF114BB271', 'D395BD24262F', 'A459492C73CE', '3901A0BB6B92', '54CB8D1DFDD0', '883E45198895', 'A99C834B122D', 'A3A08FDF9D3C', '434C1F133BA0', '5538A4817C01', 'CEFC37AFA236', 'B0D2190F7312', 'ADD3D2BDCC6B', '1A9B01EDB6C4', 'F320A6767646', 'AB59EE521967', 'A6527612255D', '7CF49F831E85', 'F71859A3B336', '4C2B18838D9D', '784E5F6EF6E1', 'F5152BBB18F6', '7D83374556C1', '5C503E0A2A01', 'C068F7168129', '506BF8E786CE', '6E6BCEE073C6', 'D698ED5E6B70', 'CEA8008D03D4', 'E286AD066204', 'F7E8813101B4', 'B39F2225875C', 'B57C860BE032', 'A817A13D651B', '6FB60C290F5A', 'F74B4B5496F0', '4354903A1D8A', 'C0916625AAB0', 'B36BDB586EEF', '363F69979EAB', '10D7845FC5D4', 'DCF72C5A4C3B', 'A5FF36F08845', '253F3DB00C5A', 'B7BC85276656', 'DBE16C234CF3', '466103CE7D5D', 'C66E655CF34A', 'FB7A0BB5DBE4', '144E2BFD59A3', 'DD861E42F43C', 'C74CBEFD0B9E', 'F1973E9DB4CF', '4BB688100D15', 'DECDC46197E4', '62EA05528EFF', '83AAAD315F57', '937CDD403137', 'A2237589F0FE', '9D9A5677B299', 'E125061E7C8A', '8A7F6AD46E4E', '98C855AF1F41', 'D47240EFB904', '2E86ACD1A766', 'E0641441E7B6', '8C928B2D6AB9', 'EC488F9F8910', '6E18981CEAAD', '871B7DB49A3F', '22FBB7ED7597', '656C7C849144', 'D97105156839', 'C5BAE31EFC62', '0C71097D3AE1', 'CE47E51746E3', '9E4100586EDC', '8F6F0CD4986A', '2E9563678CAA', '359A2C8A46C4', 'C80F42A2B53B', '11A4E36A48AF', '8B2B8A57D84B', '9ABAA03ACB4B', '19A9B6881B50', '3B7E7C18280B', '007812CC14B2', '3864A8B9C6B5', '37EA4D44939C', '9DB6D2ADE58A', 'EEB58B11D40C', '485E595428A2', '371C9C47A787', '89E73E0B8EF5', 'EC29A4F154B1', '759C3FAFA8BF', 'B2FF6A761EC9', 'F4C2261D0379', '31F19F915306', '84A0F6E847E0', '216054C99572', '7A0D5E468AB3', 'CF48D9415493', '6743F5B9985B', '51D70845DFA5', '83CEF7164BBB', '2A665076893F', 'EF478C3A8264', '1308F5C3347E', '2EEB19BB460B', 'D85A53759452', '12C119C37C62', '82D32B4D6852', '5C8E678EACDD', 'D8F3209E84CF', '505316DE125D', '1D374A0B26EA', 'B526BA896931', 'D8DBF197A624', '55664500EFAA', '8A4D99910F10', '8DA0993625FF', '229D813111B2', 'C2D8F1B0A984', '2657B9F9CCDC', 'D8C149300D31', '5CC94F3773B1', '555EC033AAE4', '37E07916B614', '0814426B27DF', 'BD300B07564D', '9B948850F6A8', '10F1ACB5559E', '95A6F680F078', '81B6F6345F55', 'E2F92BCD20E5', '7F1905176793', '8C003FEE68D7', '6F868B506EB5', 'AD0F893FA473', '20D81D1BE1A6', 'BE28ACA50D03', '6FF3C3A95428', '0527DB04DCC0', 'A9EBC5E40694', '99504CB04F5D', 'B082F1A267EE', '41BEF59908B1', '900A879708F0', 'EFAD20A5F4A0', '0F71E5F49E17', 'F0DF7FC426DB', '6A49432C3001', '672DB1A41215', '312860BEC6C5', 'B5C3DC4A876E', '04D4F9F350F6', '60772C514A37', 'AC0B87BB101A', '16364DA86C3D', 'F29E589C11F2', '54CCF07F322A', '977103A9DFE1', '5B1604AE437E', '8A1A6161F47C', '27DE9583F170', '07C7A31386A0', 'B7C3128E0973', '39B6908F453E', 'D2E2D4CCBF8C', 'CFE773F1600B', '527EB3C89F2F', 'A66B88F03338', 'B936994E140D', '497079209629', '102F13317092', 'C4ED4217DA09', '45BF01B7EF4D', '3BEFA845FC27', 'F166DA36A4A3', '8F33151F0334', 'DDA2A181F486', 'ACCD71550365'], 'df_val_eval':                  id  discourse_id  discourse_start  discourse_end  \\\n",
            "0      A8445CABFECE  1.622576e+12             18.0           85.0   \n",
            "1      A8445CABFECE  1.622576e+12             86.0          202.0   \n",
            "2      A8445CABFECE  1.622576e+12            203.0         1030.0   \n",
            "3      A8445CABFECE  1.622576e+12           1031.0         1243.0   \n",
            "4      A97DE0D49AEA  1.622645e+12             63.0          129.0   \n",
            "...             ...           ...              ...            ...   \n",
            "28992  0814426B27DF  1.617896e+12           1440.0         1563.0   \n",
            "28993  0814426B27DF  1.617896e+12           1564.0         1955.0   \n",
            "28994  0814426B27DF  1.617896e+12           1956.0         2003.0   \n",
            "28995  0814426B27DF  1.617896e+12           2004.0         2075.0   \n",
            "28996  0814426B27DF  1.617896e+12           2076.0         2359.0   \n",
            "\n",
            "                                          discourse_text  \\\n",
            "0      Drivers should not be able to use phones while...   \n",
            "1      Drivers who used their phone while operating a...   \n",
            "2      According to an article by the Edgar Snyder Fi...   \n",
            "3      In conclusion, drivers should not able to work...   \n",
            "4       Driver's should desist from using their Cell ...   \n",
            "...                                                  ...   \n",
            "28992  that one of the people you asked for advice ma...   \n",
            "28993  Some don't think all the same things if you we...   \n",
            "28994    Some people will disagree with my three reasons   \n",
            "28995  but I don't like to listen to the people who d...   \n",
            "28996  Maybe this helped you maybe it didn't.\\n\\nTo s...   \n",
            "\n",
            "             discourse_type      discourse_type_num  \\\n",
            "0                  Position              Position 1   \n",
            "1                     Claim                 Claim 1   \n",
            "2                  Evidence              Evidence 1   \n",
            "3      Concluding Statement  Concluding Statement 1   \n",
            "4                  Position              Position 1   \n",
            "...                     ...                     ...   \n",
            "28992                 Claim                 Claim 5   \n",
            "28993              Evidence              Evidence 3   \n",
            "28994          Counterclaim          Counterclaim 1   \n",
            "28995              Rebuttal              Rebuttal 1   \n",
            "28996  Concluding Statement  Concluding Statement 1   \n",
            "\n",
            "                                        predictionstring  \\\n",
            "0                           3 4 5 6 7 8 9 10 11 12 13 14   \n",
            "1      15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 3...   \n",
            "2      36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 5...   \n",
            "3      177 178 179 180 181 182 183 184 185 186 187 18...   \n",
            "4                          11 12 13 14 15 16 17 18 19 20   \n",
            "...                                                  ...   \n",
            "28992  290 291 292 293 294 295 296 297 298 299 300 30...   \n",
            "28993  315 316 317 318 319 320 321 322 323 324 325 32...   \n",
            "28994                    391 392 393 394 395 396 397 398   \n",
            "28995  399 400 401 402 403 404 405 406 407 408 409 41...   \n",
            "28996  413 414 415 416 417 418 419 420 421 422 423 42...   \n",
            "\n",
            "                                           text_by_index  new_start  new_end  \\\n",
            "0      Drivers should not be able to use phones while...         18       86   \n",
            "1      Drivers who used their phone while operating a...         86      202   \n",
            "2      According to an article by the Edgar Snyder Fi...        203     1030   \n",
            "3      In conclusion, drivers should not able to work...       1031     1231   \n",
            "4       Driver's should desist from using their Cell ...         64      129   \n",
            "...                                                  ...        ...      ...   \n",
            "28992  that one of the people you asked for advice ma...       1440     1563   \n",
            "28993  Some don't think all the same things if you we...       1564     1955   \n",
            "28994    Some people will disagree with my three reasons       1956     2004   \n",
            "28995  but I don't like to listen to the people who d...       2004     2075   \n",
            "28996  Maybe this helped you maybe it didn't.\\n\\nTo s...       2076     2359   \n",
            "\n",
            "                                       text_by_new_index  \\\n",
            "0      Drivers should not be able to use phones while...   \n",
            "1      Drivers who used their phone while operating a...   \n",
            "2      According to an article by the Edgar Snyder Fi...   \n",
            "3      In conclusion, drivers should not able to work...   \n",
            "4      Driver's should desist from using their Cell P...   \n",
            "...                                                  ...   \n",
            "28992  that one of the people you asked for advice ma...   \n",
            "28993  Some don't think all the same things if you we...   \n",
            "28994   Some people will disagree with my three reasons    \n",
            "28995  but I don't like to listen to the people who d...   \n",
            "28996  Maybe this helped you maybe it didn't.\\n\\nTo s...   \n",
            "\n",
            "                                    new_predictionstring  \n",
            "0                           3 4 5 6 7 8 9 10 11 12 13 14  \n",
            "1      15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 3...  \n",
            "2      36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 5...  \n",
            "3      177 178 179 180 181 182 183 184 185 186 187 18...  \n",
            "4                          11 12 13 14 15 16 17 18 19 20  \n",
            "...                                                  ...  \n",
            "28992  290 291 292 293 294 295 296 297 298 299 300 30...  \n",
            "28993  315 316 317 318 319 320 321 322 323 324 325 32...  \n",
            "28994                    391 392 393 394 395 396 397 398  \n",
            "28995  399 400 401 402 403 404 405 406 407 408 409 41...  \n",
            "28996  413 414 415 416 417 418 419 420 421 422 423 42...  \n",
            "\n",
            "[28997 rows x 13 columns], 'ds_val': <__main__.FeedbackPrizeDataset object at 0x7faf5a76b410>, 'dl_train': <torch.utils.data.dataloader.DataLoader object at 0x7faf5a5c8ad0>, 'dl_val': <torch.utils.data.dataloader.DataLoader object at 0x7faf5a5c8550>, 'best_val_loss': inf, 'criterion': CrossEntropyLoss(), 'epoch': 1, '_i29': 'try:\\n  print(gc.get_referrers(model))\\n  del model\\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i30': 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', 'KaggleDataset': <class '__main__.KaggleDataset'>, '_i31': 'try:\\n  print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i32': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i33': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    print(df_train.head())\\n    raise Exception()\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'MAPPING': {'O': 0, 'B-Lead': 1, 'B-Position': 2, 'B-Evidence': 3, 'B-Claim': 4, 'B-Concluding Statement': 5, 'B-Counterclaim': 6, 'B-Rebuttal': 7, 'I-Lead': 8, 'I-Position': 9, 'I-Evidence': 10, 'I-Claim': 11, 'I-Concluding Statement': 12, 'I-Counterclaim': 13, 'I-Rebuttal': 14}, '_i34': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i35': 'class KaggleDataset(Dataset):\\n    \"\"\"\\n    Class for loading data in batches after it has been processed\\n    \"\"\"\\n    def __init__(self, dataframe, tokenizer, max_length):\\n\\n        super().__init__()\\n\\n        # -- prepare data\\n        assert sorted(dataframe.columns) == [\\'labels\\', \\'text\\'], f\"Please make sure input dataframe has the columns (text, labels)\"\\n        # data must be in the correct format\\n        self.inputs = dataframe.text.values\\n        self.targets = dataframe.labels.values\\n        #if not is_string_dtype(self.inputs): raise TypeError(\\'Text data must be string type\\')\\n        # TODO assertion below is bug; not deleting so remember to add correct assertions\\n        #if not is_integer_dtype(self.targets): raise TypeError(\\'Label data must be integer type\\')\\n\\n        # -- prepare tokenizer\\n        self.tokenizer = tokenizer\\n        self.max_length = max_length\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, index):\\n        # self.inputs anf self.targets must be of a type that is indexible as shown\\n        inputs = self.inputs[index]\\n        targets = self.targets[index]\\n\\n        inputs = self.tokenizer(\\n            # consider parametrising these\\n            inputs.split(),\\n            is_split_into_words=True, # this means that extra \\\\n should be ignored\\n            padding=\\'max_length\\',\\n            truncation=True,\\n            max_length=self.max_length\\n        )\\n\\n        word_ids = inputs.word_ids()\\n        word_id_mask = [word_id is not None for word_id in word_ids]\\n        word_ids = [word_id for word_id in word_ids if word_id is not None]\\n\\n        inputs = {\\n            key: torch.as_tensor(val, dtype=torch.long) for key, val in inputs.items()\\n        }\\n        targets = torch.as_tensor(targets, dtype=torch.long)\\n        expanded_targets = torch.zeros(self.max_length, dtype=torch.long)\\n        expanded_targets[word_id_mask] = targets[word_ids]\\n        inputs[\\'labels\\'] = expanded_targets\\n        inputs[\\'word_ids\\'] = torch.tensor(word_ids, dtype=torch.long)\\n        return inputs\\n\\n  ', '_i36': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i37': \"class FeedbackModel(nn.Module):\\n    def __init__(self):\\n        super(FeedbackModel, self).__init__()\\n        \\n        # init config of transformer model of choice:\\n        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\\n        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\\n        print(model_config)\\n        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\\n        \\n        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\\n        self.model_config = model_config\\n        self.dropout1 = nn.Dropout(0.1)\\n        self.dropout2 = nn.Dropout(0.2)\\n        self.dropout3 = nn.Dropout(0.3)\\n        self.dropout4 = nn.Dropout(0.4)\\n        self.dropout5 = nn.Dropout(0.5)\\n        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\\n    \\n    def forward(self, input_ids, mask):\\n        x = self.backbone(input_ids, mask)\\n        logits1 = self.head(self.dropout1(x[0]))\\n        logits2 = self.head(self.dropout2(x[0]))\\n        logits3 = self.head(self.dropout3(x[0]))\\n        logits4 = self.head(self.dropout4(x[0]))\\n        logits5 = self.head(self.dropout5(x[0]))\\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\\n        return logits\", '_i38': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 1024))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i39': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i40': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n            print(raw_logits.shape)\\n            print(word_ids.shape)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        logits = active_logits(raw_logits, word_ids)\\n        labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i41': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i42': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i43': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i44': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', '_i45': \"def train_fn(model, train_data_loader, optimizer, epoch, criterion):\\n    print(f'Training for epoch {epoch} started. GPU utilisation: {get_gpu_utilization()}')\\n    model.train()\\n    train_loss = 0\\n    train_accuracy = 0\\n    stream = tqdm(train_data_loader)\\n    # Init gradscaler to ensure everything works smoothly on cuda\\n    scaler = GradScaler()\\n    print(f'GradScaler initialised. GPU utilisation: {get_gpu_utilization()}')\\n    for batch_idx, batch in enumerate(stream, start = 1):\\n        ids = batch['input_ids'].to(device, dtype = torch.long)\\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\\n        raw_labels = batch['labels'].to(device, dtype = torch.long)\\n        word_ids = batch['word_ids'].to(device, dtype = torch.long)\\n        print(f'Tensors added to GPU. GPU utilisation: {get_gpu_utilization()}')\\n\\n        optimizer.zero_grad()\\n        print(f'Optimizer Reset. GPU utilisation: {get_gpu_utilization()}')\\n\\n        # Calculate output with autocast for cuda support\\n        with autocast():\\n            raw_logits = model(input_ids = ids, mask = mask)\\n        print(f'Model trained. GPU utilisation: {get_gpu_utilization()}')\\n\\n        #logits = active_logits(raw_logits, word_ids)\\n        #labels = active_labels(raw_labels)\\n\\n        logits = raw_logits\\n        labels = raw_labels\\n        sf_logits = torch.softmax(logits, dim=-1)\\n        preds, preds_prob = active_preds_prob(sf_logits)\\n        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(logits, labels)\\n\\n        print(f'Loss calculated. GPU utilisation: {get_gpu_utilization()}')\\n\\n        scaler.scale(loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n        train_loss += loss.item()\\n        print(f'Optimization step occured. GPU utilisation: {get_gpu_utilization()}')\\n\\n        \\n        if batch_idx % HyperParameters.verbose_steps == 0:\\n            loss_step = train_loss / batch_idx\\n            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\\n\\n        if batch_idx == 3:\\n          raise Exception()\\n            \\n    epoch_loss = train_loss / batch_idx\\n    epoch_accuracy = train_accuracy / batch_idx    \\n    print_gpu_utilization()\\n    # Cleanup\\n    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\\n    torch.cuda.empty_cache()\\n    gc.collect()\\n    \\n    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\\n    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\", '_i46': 'try:\\n  #print(gc.get_referrers(model))\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])', 'model': FeedbackModel(\n",
            "  (backbone): LongformerModel(\n",
            "    (embeddings): LongformerEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): LongformerEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): LongformerLayer(\n",
            "          (attention): LongformerAttention(\n",
            "            (self): LongformerSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): LongformerSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): LongformerIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): LongformerOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): LongformerPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.2, inplace=False)\n",
            "  (dropout3): Dropout(p=0.3, inplace=False)\n",
            "  (dropout4): Dropout(p=0.4, inplace=False)\n",
            "  (dropout5): Dropout(p=0.5, inplace=False)\n",
            "  (head): Linear(in_features=768, out_features=15, bias=True)\n",
            "), '_i47': 'try:\\n  print(gc.get_referrers(model))\\n  raise Exception()\\n  del model, \\n  torch.cuda.empty_cache()\\n  gc.collect()\\n  print(\\'Model existed. Deleting...\\')\\n  print_gpu_utilization()\\nexcept:\\n  print(\\'Model NOT found... new model will be created\\')\\n  print_gpu_utilization()\\n\\nMAPPING = {\\n    \"O\": 0,\\n    \"B-Lead\": 1,\\n    \"B-Position\": 2,\\n    \"B-Evidence\": 3,\\n    \"B-Claim\": 4,\\n    \"B-Concluding Statement\": 5,\\n    \"B-Counterclaim\": 6,\\n    \"B-Rebuttal\": 7,\\n    \"I-Lead\": 8,\\n    \"I-Position\": 9,\\n    \"I-Evidence\": 10,\\n    \"I-Claim\": 11,\\n    \"I-Concluding Statement\": 12,\\n    \"I-Counterclaim\": 13,\\n    \"I-Rebuttal\": 14,\\n}\\noof = pd.DataFrame()\\n\\nfor i_fold in range(HyperParameters.n_fold):\\n    print(f\\'=== fold{i_fold} training ===\\')\\n    print(f\\'First run: {print_gpu_utilization()}\\')\\n    model, tokenizer = build_model_tokenizer()\\n    model = model.to(device)\\n    print(f\\'Model and tokeniser: {print_gpu_utilization()}\\')\\n\\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\\n    print(f\\'Optimizer: {print_gpu_utilization()}\\')\\n\\n    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\\n    df_train = df_train.assign(labels=lambda x: x[\\'entities\\'].apply(lambda x: [MAPPING[key] for key in x]))\\n    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\\n    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\\n    val_idlist = df_val[\\'id\\'].unique().tolist()\\n    df_val_eval = df_alltrain.query(\\'id==@val_idlist\\').reset_index(drop=True)\\n    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\\n    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\\n    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\\n\\n    dl_train = DataLoader(KaggleDataset(df_train[[\\'text\\', \\'labels\\']], tokenizer, 532))\\n    best_val_loss = np.inf\\n    criterion = nn.CrossEntropyLoss()\\n    print(f\\'Prior to training, GPU utilisation is: {print_gpu_utilization()}\\')\\n    for epoch in range(1, HyperParameters.n_epoch + 1):\\n        train_fn(model, dl_train, optimizer, epoch, criterion)\\n        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\\n        if valid_loss < best_val_loss:\\n            best_val_loss = valid_loss\\n            _oof_fold_best = _oof\\n            _oof_fold_best[\"kfold\"] = i_fold\\n            model_filename = f\\'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin\\'\\n            \\n            # Saving the boy\\n            torch.save(model.state_dict(), model_filename)\\n            print(f\\'{model_filename} saved\\')\\n\\n    oof = pd.concat([oof, _oof_fold_best])'}]\n",
            "Model NOT found... new model will be created\n",
            "GPU memory occupied: 8527 MB.\n",
            "=== fold0 training ===\n",
            "GPU memory occupied: 8527 MB.\n",
            "First run: None\n",
            "LongformerConfig {\n",
            "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 9131 MB.\n",
            "Model and tokeniser: None\n",
            "GPU memory occupied: 9131 MB.\n",
            "Optimizer: None\n",
            "GPU memory occupied: 9131 MB.\n",
            "Prior to training, GPU utilisation is: None\n",
            "Training for epoch 1 started. GPU utilisation: 9131\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eef7a90afeb44c448532e10c8ef5a20e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/12477 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradScaler initialised. GPU utilisation: 9131\n",
            "Tensors added to GPU. GPU utilisation: 9131\n",
            "Optimizer Reset. GPU utilisation: 9131\n",
            "Model trained. GPU utilisation: 10505\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-aefc84d23aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Prior to training, GPU utilisation is: {print_gpu_utilization()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHyperParameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_oof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-be461fdb2349>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, train_data_loader, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msf_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_preds_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput is not supported"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  print(gc.get_referrers(model))\n",
        "  raise Exception()\n",
        "  del model, \n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  print('Model existed. Deleting...')\n",
        "  print_gpu_utilization()\n",
        "except:\n",
        "  print('Model NOT found... new model will be created')\n",
        "  print_gpu_utilization()\n",
        "\n",
        "MAPPING = {\n",
        "    \"O\": 0,\n",
        "    \"B-Lead\": 1,\n",
        "    \"B-Position\": 2,\n",
        "    \"B-Evidence\": 3,\n",
        "    \"B-Claim\": 4,\n",
        "    \"B-Concluding Statement\": 5,\n",
        "    \"B-Counterclaim\": 6,\n",
        "    \"B-Rebuttal\": 7,\n",
        "    \"I-Lead\": 8,\n",
        "    \"I-Position\": 9,\n",
        "    \"I-Evidence\": 10,\n",
        "    \"I-Claim\": 11,\n",
        "    \"I-Concluding Statement\": 12,\n",
        "    \"I-Counterclaim\": 13,\n",
        "    \"I-Rebuttal\": 14,\n",
        "}\n",
        "oof = pd.DataFrame()\n",
        "\n",
        "for i_fold in range(HyperParameters.n_fold):\n",
        "    print(f'=== fold{i_fold} training ===')\n",
        "    print(f'First run: {print_gpu_utilization()}')\n",
        "    model, tokenizer = build_model_tokenizer()\n",
        "    model = model.to(device)\n",
        "    print(f'Model and tokeniser: {print_gpu_utilization()}')\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\n",
        "    print(f'Optimizer: {print_gpu_utilization()}')\n",
        "\n",
        "    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\n",
        "    df_train = df_train.assign(labels=lambda x: x['entities'].apply(lambda x: [MAPPING[key] for key in x]))\n",
        "    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\n",
        "    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\n",
        "    val_idlist = df_val['id'].unique().tolist()\n",
        "    df_val_eval = df_alltrain.query('id==@val_idlist').reset_index(drop=True)\n",
        "    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\n",
        "    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    dl_train = DataLoader(KaggleDataset(df_train[['text', 'labels']], tokenizer, 532))\n",
        "    best_val_loss = np.inf\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(f'Prior to training, GPU utilisation is: {print_gpu_utilization()}')\n",
        "    for epoch in range(1, HyperParameters.n_epoch + 1):\n",
        "        train_fn(model, dl_train, optimizer, epoch, criterion)\n",
        "        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\n",
        "        if valid_loss < best_val_loss:\n",
        "            best_val_loss = valid_loss\n",
        "            _oof_fold_best = _oof\n",
        "            _oof_fold_best[\"kfold\"] = i_fold\n",
        "            model_filename = f'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin'\n",
        "            \n",
        "            # Saving the boy\n",
        "            torch.save(model.state_dict(), model_filename)\n",
        "            print(f'{model_filename} saved')\n",
        "\n",
        "    oof = pd.concat([oof, _oof_fold_best])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlmZeM47UxuT"
      },
      "outputs": [],
      "source": [
        "oof.to_csv(f'{HyperParameters.output_dir}/oof_{HyperParameters.name}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO8hcaQ8UxuT"
      },
      "source": [
        "Looking at performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOcFtSUhUxuT"
      },
      "outputs": [],
      "source": [
        "if HyperParameters.is_debug:\n",
        "    idlist = alltrain_texts['id'].unique().tolist()\n",
        "    df_train = df_alltrain.query('id==@idlist')\n",
        "else:\n",
        "    df_train = df_alltrain.copy()\n",
        "print(f'overall cv score: {score_feedback_comp(df_train, oof)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Longformer.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "be336ab25ba919cf0a65f4be83b938febf47f529e9f75dfb16359f541885e1c6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('machinevision')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eef7a90afeb44c448532e10c8ef5a20e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ca3198d71a3f4887874da99cc3e78882",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c4d33c2a02644f85a146f390d6aa0736",
              "IPY_MODEL_9d549d2487dd4197a978761bd72be2ad",
              "IPY_MODEL_ec2dd4649f114c6fad2430c245fe68aa"
            ]
          }
        },
        "ca3198d71a3f4887874da99cc3e78882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4d33c2a02644f85a146f390d6aa0736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81cfe2eb173e40fea93d20dc2f171f03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_433b7e11f87442198edef448a8c16074"
          }
        },
        "9d549d2487dd4197a978761bd72be2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22c91851758d425383c6653f877dbc1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 12477,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25ffd2bde996468a9c003a1883191813"
          }
        },
        "ec2dd4649f114c6fad2430c245fe68aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45645e8eb5af4fca8c6a84fa3b99dab3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/12477 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d81a205322c84e7e8830374196e00a36"
          }
        },
        "81cfe2eb173e40fea93d20dc2f171f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "433b7e11f87442198edef448a8c16074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22c91851758d425383c6653f877dbc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25ffd2bde996468a9c003a1883191813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45645e8eb5af4fca8c6a84fa3b99dab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d81a205322c84e7e8830374196e00a36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}