{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e625f2f2",
   "metadata": {},
   "source": [
    "# quick inference poc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709017a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e522863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- public imports\n",
    "\n",
    "from transformers import BigBirdTokenizer, AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b1b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import of ArgMiner successful\n"
     ]
    }
   ],
   "source": [
    "# -- private import\n",
    "from argminer.data import KaggleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77ce1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# -- dev imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eff33ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = 'google/bigbird-roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, add_prefix_space=True)\n",
    "config_model = AutoConfig.from_pretrained(model) \n",
    "config_model.num_labels = 3\n",
    "model = AutoModelForTokenClassification.from_pretrained(model, config=config_model)\n",
    "optimizer = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ee5553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "            'Hi my name is Yousef Nami',\n",
    "            'Hi I am Shirin'\n",
    "        ]\n",
    "labels = [\n",
    "    ['O', 'O', 'O', 'O', 'B-PERS', 'I-PERS'],\n",
    "    ['O', 'O', 'O', 'B-PERS']\n",
    "]\n",
    "\n",
    "# TODO see if bert can accept text inputs?\n",
    "labels_numeric = [\n",
    "    [0, 0, 0, 0, 1, 2],\n",
    "    [0, 0, 0, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dd5d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'labels': labels_numeric\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f971c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KaggleDataset(\n",
    "    df,\n",
    "    tokenizer,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65e8b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   65, 16003,   717,  1539,   419,   676,   617,   992,   500,  6378,\n",
      "            66,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   65, 16003,   415,   817,  1012, 47489,    66,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'word_ids': tensor([[-1,  0,  1,  2,  3,  4,  4,  4,  5,  5, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1],\n",
      "        [-1,  0,  1,  2,  3,  3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1]]), 'word_id_mask': [tensor([False, False]), tensor([True, True]), tensor([True, True]), tensor([True, True]), tensor([True, True]), tensor([True, True]), tensor([ True, False]), tensor([ True, False]), tensor([ True, False]), tensor([ True, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False]), tensor([False, False])]}\n",
      "tensor([[[-0.1642,  0.0992,  0.3482],\n",
      "         [-0.0166, -0.3431,  0.7926],\n",
      "         [ 0.0151,  0.1921,  0.6123],\n",
      "         [ 0.0664, -0.0830,  0.5713],\n",
      "         [ 0.0521, -0.2243,  0.7010],\n",
      "         [ 0.2475, -0.2039,  0.3427],\n",
      "         [ 0.0468, -0.2063,  0.4431],\n",
      "         [ 0.3500, -0.1089,  0.4890],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [ 0.2735, -0.3232,  0.6088],\n",
      "         [ 0.0164, -0.2838,  0.7500],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [-0.3618, -0.0890,  0.4821],\n",
      "         [-0.1642,  0.0992,  0.3482],\n",
      "         [ 0.0758, -0.3785,  0.5925],\n",
      "         [-0.1642,  0.0992,  0.3482]],\n",
      "\n",
      "        [[-0.1558,  0.0520,  0.3420],\n",
      "         [-0.1565,  0.0522,  0.3423],\n",
      "         [-0.1566,  0.0523,  0.3424],\n",
      "         [-0.1557,  0.0502,  0.3406],\n",
      "         [-0.1565,  0.0522,  0.3423],\n",
      "         [-0.1569,  0.0525,  0.3428],\n",
      "         [-0.1566,  0.0523,  0.3425],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1565,  0.0523,  0.3424],\n",
      "         [-0.1565,  0.0522,  0.3423],\n",
      "         [-0.1565,  0.0523,  0.3424],\n",
      "         [-0.1564,  0.0522,  0.3423],\n",
      "         [-0.1564,  0.0522,  0.3423],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422],\n",
      "         [-0.1564,  0.0522,  0.3422]]], grad_fn=<AddBackward0>)\n",
      "tensor([-1,  0,  1,  2,  3,  4,  4,  4,  5,  5, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1])\n",
      "tensor([[-0.0166, -0.3431,  0.7926],\n",
      "        [ 0.0151,  0.1921,  0.6123],\n",
      "        [ 0.0664, -0.0830,  0.5713],\n",
      "        [ 0.0521, -0.2243,  0.7010],\n",
      "        [ 0.2475, -0.2039,  0.3427],\n",
      "        [ 0.0468, -0.2063,  0.4431],\n",
      "        [ 0.3500, -0.1089,  0.4890],\n",
      "        [-0.1642,  0.0992,  0.3482],\n",
      "        [ 0.2735, -0.3232,  0.6088]], grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.0166, -0.3431,  0.7926],\n",
      "        [ 0.0151,  0.1921,  0.6123],\n",
      "        [ 0.0664, -0.0830,  0.5713],\n",
      "        [ 0.0521, -0.2243,  0.7010],\n",
      "        [ 0.6442, -0.5192,  1.2748],\n",
      "        [ 0.1094, -0.2241,  0.9570]], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([-1,  0,  1,  2,  3,  3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1])\n",
      "tensor([[-0.1565,  0.0522,  0.3423],\n",
      "        [-0.1566,  0.0523,  0.3424],\n",
      "        [-0.1557,  0.0502,  0.3406],\n",
      "        [-0.1565,  0.0522,  0.3423],\n",
      "        [-0.1569,  0.0525,  0.3428]], grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.1565,  0.0522,  0.3423],\n",
      "        [-0.1566,  0.0523,  0.3424],\n",
      "        [-0.1557,  0.0502,  0.3406],\n",
      "        [-0.3134,  0.1048,  0.6851]], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(loader):\n",
    "    print(inputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # for evaluation purposes may need to save the embedding weights??\n",
    "    \n",
    "    word_ids = inputs['word_ids']\n",
    "    word_id_mask = inputs['word_id_mask']\n",
    "    loss, outputs = model(\n",
    "        labels=targets,\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        return_dict=False\n",
    "    )\n",
    "    print(outputs)\n",
    "    inference(outputs, word_ids, 'avg')\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3245b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch, word_id_batch, agg_strategy):\n",
    "    # TODO need to have assertions to prevent certain strategies from happening together!\n",
    "\n",
    "\n",
    "    for (predictions, word_ids) in zip(batch, word_id_batch):\n",
    "        # TODO probably no need to store mask?\n",
    "        mask = word_ids != -1\n",
    "        print(word_ids)\n",
    "        word_ids = word_ids[mask]\n",
    "        predictions = predictions[mask]\n",
    "        print(predictions)\n",
    "        unique_word_ids, word_id_counts = torch.unique_consecutive(word_ids, return_counts=True)\n",
    "        agg_predictions = torch.zeros((len(unique_word_ids), predictions.shape[-1]), dtype=float)\n",
    "        start_id = 0\n",
    "        for i, (unique_word_id, word_id_count) in enumerate(zip(unique_word_ids, word_id_counts)):\n",
    "            end_id = start_id + word_id_count\n",
    "            agg_predictions[i] = predictions[start_id: end_id]\n",
    "            start_id = end_id\n",
    "        print(agg_predictions)\n",
    "    return agg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "803ba66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b749b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argument-mining",
   "language": "python",
   "name": "argument-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
