{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zIVWaSRUxuA"
      },
      "source": [
        "# Foreword:\n",
        "To run in Colab, we need to have the files in Drive. \n",
        "To do this easily, we have to follow a couple of steps:\n",
        "1. Go to Federico's `NLP_project` folder in Drive [here](https://drive.google.com/drive/folders/16Gm33Ckb_YoX_z_x9xVITt2afa-aAPLX?usp=sharing), and Add a shortcut to your drive.\n",
        "2. Mount google drive on Colab by running the code cells that will follow.\n",
        "3. Done, the directory structure will look like this:\n",
        "```\n",
        "YOUR_GOOGLE_DRIVE/\n",
        "└── COMP0087/\n",
        "    ├── data/\n",
        "    │   ├── test\n",
        "    │   ├── train\n",
        "    │   ├── train.csv\n",
        "    │   └── sample_submission.csv\n",
        "    ├── model\n",
        "    └── output\n",
        "```\n",
        "4. Make sure you change the directory you are using in the `HyperParameters` class defined below to `/content/drive/MyDrive/NLP_project`\n",
        "I have already done this automatically by setting a cd to that folder if we are on colab, I am writing this just so that you are aware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifzzOtkEUxuC",
        "outputId": "2a9731b7-e07f-4b0f-d6a4-42477467bd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "ON_COLAB = True\n",
        "if ON_COLAB:\n",
        "  # Mount drive:\n",
        "  from google.colab import drive, files\n",
        "  # mount Google Drive\n",
        "  drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5K5Cr2fUxuE"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eriNZqVPVW9a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# if on Colab, we need to install missing stuff!\n",
        "if ON_COLAB:\n",
        "  !pip install transformers\n",
        "  !pip install iterative-stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OtO0xcw1UxuF"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i9N0xf4RXlVL"
      },
      "outputs": [],
      "source": [
        "if ON_COLAB:\n",
        "  !cd /content/drive/MyDrive/NLP_project\n",
        "\n",
        "\n",
        "# DATA DIR ---- TO CHANGE\n",
        "DATA_DIR = 'drive/MyDrive/NLP_project/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lgNlqCIUxuF"
      },
      "source": [
        "Config class containing all necessary hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "COkK9LFUUxuG"
      },
      "outputs": [],
      "source": [
        "class HyperParameters:\n",
        "    \n",
        "    # Here we choose model type. Can be changed for others\n",
        "    name = 'longformer'\n",
        "    model_savename = 'longformer'\n",
        "    model_name = 'allenai/longformer-base-4096'      # this is the most important: determines what transformer is used in training\n",
        "    \n",
        "    # Directory hyperparameters: make sure to change with what you are using! Only needed to change here\n",
        "    base_dir = DATA_DIR\n",
        "    data_dir = os.path.join(base_dir, 'data')\n",
        "    pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\n",
        "    model_dir = os.path.join(base_dir, f'model/{name}')\n",
        "    output_dir = os.path.join(base_dir, f'output/{name}')\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    is_debug = False\n",
        "    n_epoch = 2 # not to exceed runtime limit\n",
        "    n_fold = 5\n",
        "    verbose_steps = 500\n",
        "    random_seed = 42\n",
        "\n",
        "    # Model specific hyperparameters\n",
        "    max_length = 1024\n",
        "    inference_max_length = 4096\n",
        "    train_batch_size = 4\n",
        "    valid_batch_size = 4\n",
        "    lr = 4e-5\n",
        "\n",
        "    # Task hyperparameters\n",
        "    num_labels = 15\n",
        "    label_subtokens = True\n",
        "    output_hidden_states = True\n",
        "    hidden_dropout_prob = 0.1\n",
        "    layer_norm_eps = 1e-7\n",
        "    add_pooling_layer = False\n",
        "    verbose_steps = 500\n",
        "    if is_debug:\n",
        "        debug_sample = 1000\n",
        "        verbose_steps = 16\n",
        "        n_epoch = 1\n",
        "        n_fold = 2\n",
        "\n",
        "if not os.path.exists(HyperParameters.model_dir):\n",
        "    !mkdir $HyperParameters.model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF5bW6aVUxuG"
      },
      "source": [
        "Constant for the task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "08glsD47UxuH"
      },
      "outputs": [],
      "source": [
        "IGNORE_INDEX = -100\n",
        "NON_LABEL = -1\n",
        "OUTPUT_LABELS = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
        "                 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
        "LABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\n",
        "IDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\n",
        "\n",
        "MIN_THRESH = {\n",
        "    \"I-Lead\": 9,\n",
        "    \"I-Position\": 5,\n",
        "    \"I-Evidence\": 14,\n",
        "    \"I-Claim\": 3,\n",
        "    \"I-Concluding Statement\": 11,\n",
        "    \"I-Counterclaim\": 6,\n",
        "    \"I-Rebuttal\": 4,\n",
        "}\n",
        "\n",
        "PROB_THRESH = {\n",
        "    \"I-Lead\": 0.7,\n",
        "    \"I-Position\": 0.55,\n",
        "    \"I-Evidence\": 0.65,\n",
        "    \"I-Claim\": 0.55,\n",
        "    \"I-Concluding Statement\": 0.7,\n",
        "    \"I-Counterclaim\": 0.5,\n",
        "    \"I-Rebuttal\": 0.55,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WJbWh4GUxuI"
      },
      "source": [
        "Taming randomness and setting device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u_8GvKvUxuI",
        "outputId": "9f70db7b-2294-4423-9c1c-63c86d78d676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed=HyperParameters.random_seed):\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    random.seed(seed)\n",
        "    \n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    torch.backends.cudnn.deterministic =True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Set proper device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXW7QdTPUxuK"
      },
      "source": [
        "# Data importing and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh1Bdo7UUxuK"
      },
      "source": [
        "Importing corrected data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cDhwkU7MUxuK"
      },
      "outputs": [],
      "source": [
        "df_alltrain = pd.read_csv(f'{HyperParameters.data_dir}/corrected_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O7hy7b6uUxuL"
      },
      "outputs": [],
      "source": [
        "def agg_essays(train_flg):\n",
        "    \"\"\"\n",
        "    Splits every word in an essay and adds the text of each essay to a dataframe.\n",
        "    \"\"\"\n",
        "    folder = 'train' if train_flg else 'test'\n",
        "    names, texts =[], []\n",
        "    for f in tqdm(list(os.listdir(f'{HyperParameters.data_dir}/{folder}'))):\n",
        "        names.append(f.replace('.txt', ''))\n",
        "        texts.append(open(f'{HyperParameters.data_dir}/{folder}/' + f, 'r').read())\n",
        "        df_texts = pd.DataFrame({'id': names, 'text': texts})\n",
        "\n",
        "    df_texts['text_split'] = df_texts.text.str.split()\n",
        "    print('Completed tokenizing texts.')\n",
        "    return df_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "av_kvU8eUxuL"
      },
      "outputs": [],
      "source": [
        "def ner(df_texts, df_train):\n",
        "    \"\"\"\n",
        "    Maps discourse type to each word of the text, according to the train.csv file.\n",
        "    \"\"\"\n",
        "    all_entities = []\n",
        "    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\n",
        "        total = len(row['text_split'])\n",
        "        entities = ['O'] * total\n",
        "\n",
        "        for _, row2 in df_train[df_train['id'] == row['id']].iterrows():\n",
        "            discourse = row2['discourse_type']\n",
        "            list_ix = [int(x) for x in row2['predictionstring'].split(' ')]\n",
        "            entities[list_ix[0]] = f'B-{discourse}'\n",
        "            for k in list_ix[1:]: entities[k] = f'I-{discourse}'\n",
        "        all_entities.append(entities)\n",
        "\n",
        "    df_texts['entities'] = all_entities\n",
        "    print('Completed mapping discourse to each token.')\n",
        "    return df_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Sq2Rco3eUxuL"
      },
      "outputs": [],
      "source": [
        "def preprocess(df_train = None):\n",
        "    \"\"\"\n",
        "    Generates the dataframe we will use for training.\n",
        "    Splits essays into words, assigns a token name to each word, and adds everything to a dataframe.\n",
        "    \"\"\"\n",
        "    if df_train is None:\n",
        "        train_flg = False\n",
        "    else:\n",
        "        train_flg = True\n",
        "    \n",
        "    df_texts = agg_essays(train_flg)\n",
        "\n",
        "    if train_flg:\n",
        "        df_texts = ner(df_texts, df_train)\n",
        "    return df_texts\n",
        "\n",
        "# Make sure we only run pre-processing if we did not do it in the past:\n",
        "\n",
        "if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \n",
        "    alltrain_texts = preprocess(df_alltrain)\n",
        "    test_texts = preprocess()\n",
        "else:\n",
        "    alltrain_texts = pd.read_csv(f\"{HyperParameters.data_dir}/train_folds.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZLa0fkXbUxuM",
        "outputId": "48be61dc-e476-48d6-b5ff-67a0b9bad7d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9f686b97-510f-418d-9903-edc4b54ff739\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>text_split</th>\n",
              "      <th>entities</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3321A3E87AD3</td>\n",
              "      <td>I do agree that some students would benefit fr...</td>\n",
              "      <td>[I, do, agree, that, some, students, would, be...</td>\n",
              "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DFEAEC512BAB</td>\n",
              "      <td>Should students design a summer project for sc...</td>\n",
              "      <td>[Should, students, design, a, summer, project,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-Position, I-Positio...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2E4AFCD3987F</td>\n",
              "      <td>Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...</td>\n",
              "      <td>[Dear, State, Senator, ,, In, the, ruels, of, ...</td>\n",
              "      <td>[O, O, O, O, B-Position, I-Position, I-Positio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EB6C2AF20BFE</td>\n",
              "      <td>People sometimes have a different opinion than...</td>\n",
              "      <td>[People, sometimes, have, a, different, opinio...</td>\n",
              "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A91A08E523D5</td>\n",
              "      <td>Dear senator,\\n\\nAs you know the Electoral Col...</td>\n",
              "      <td>[Dear, senator,, As, you, know, the, Electoral...</td>\n",
              "      <td>[O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f686b97-510f-418d-9903-edc4b54ff739')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f686b97-510f-418d-9903-edc4b54ff739 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f686b97-510f-418d-9903-edc4b54ff739');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             id                                               text  \\\n",
              "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
              "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
              "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
              "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
              "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
              "\n",
              "                                          text_split  \\\n",
              "0  [I, do, agree, that, some, students, would, be...   \n",
              "1  [Should, students, design, a, summer, project,...   \n",
              "2  [Dear, State, Senator, ,, In, the, ruels, of, ...   \n",
              "3  [People, sometimes, have, a, different, opinio...   \n",
              "4  [Dear, senator,, As, you, know, the, Electoral...   \n",
              "\n",
              "                                            entities  kfold  \n",
              "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      2  \n",
              "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...      4  \n",
              "2  [O, O, O, O, B-Position, I-Position, I-Positio...      0  \n",
              "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...      3  \n",
              "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...      1  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize preprocessing result:\n",
        "parse_string = lambda x: [string[1:-1] for string in x[1:-1].split(', ')]\n",
        "alltrain_texts.entities = alltrain_texts.entities.apply(parse_string)\n",
        "alltrain_texts.text_split = alltrain_texts.text_split.apply(parse_string)\n",
        "\n",
        "alltrain_texts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ-g9VB3UxuM"
      },
      "outputs": [],
      "source": [
        "# Same for testing:\n",
        "test_texts.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRFI_KLMUxuN"
      },
      "source": [
        "# Preparing cross validation\n",
        "\n",
        "Generate proper folds so that the essays we use in each fold have roughly the same number of discourse types overall.\n",
        "Only compute if we don't have the file in directory already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOkXJ5UTUxuN"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \n",
        "    # Transform categorical labels to dummy variables. Group by id. Sum over dummy. \n",
        "    dfx = pd.get_dummies(df_alltrain, columns=[\"discourse_type\"]).groupby([\"id\"], as_index=False).sum()\n",
        "\n",
        "    # Generate name for the dummy columns\n",
        "    dummy_cols = [c for c in dfx.columns if c.startswith(\"discourse_type_\") or c == \"id\" and c != \"discourse_type_num\"]\n",
        "    # dfx is now only the dataset with dummy columns selected: don't need to pass the data to do the splits\n",
        "    dfx = dfx[dummy_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1AuTst8UxuN"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f\"{HyperParameters.data_dir}/train_folds.csv\"): \n",
        "    # Generate cross validation object\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Extract labels\n",
        "    labels = [c for c in dfx.columns if c != \"id\"]\n",
        "    dfx_labels = dfx[labels]\n",
        "\n",
        "    # Dummy kfold assignment\n",
        "    dfx[\"kfold\"] = -1\n",
        "\n",
        "    # Split\n",
        "    for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\n",
        "        print(len(trn_), len(val_))\n",
        "        \n",
        "        # Change the value of the kfold column at the validation index to the value of the fold\n",
        "        # This will tell us when to use the current entry in the validation set\n",
        "        dfx.loc[val_, \"kfold\"] = fold\n",
        "\n",
        "    # merge back to original dataframe\n",
        "    alltrain_texts = alltrain_texts.merge(dfx[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")\n",
        "    print(alltrain_texts.kfold.value_counts())\n",
        "\n",
        "    # Save so next time we import it directly\n",
        "    alltrain_texts.to_csv(f\"{HyperParameters.data_dir}/train_folds.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7fcRANPUxuO"
      },
      "source": [
        "# Model and Dataset classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gDGkTBtUxuO"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APIkM7YwUxuO"
      },
      "outputs": [],
      "source": [
        "# need help with this\n",
        "class FeedbackPrizeDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len, has_labels):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_labels = has_labels\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.text[index]\n",
        "        encoding = self.tokenizer(\n",
        "            text.split(),\n",
        "            is_split_into_words = True,\n",
        "            padding = 'max_length',\n",
        "            truncation = True,\n",
        "            max_length = self.max_len\n",
        "        )\n",
        "        word_ids = encoding.word_ids()\n",
        "\n",
        "        # targets\n",
        "        if self.has_labels:\n",
        "            word_labels = self.data.entities[index]\n",
        "            prev_word_idx = None\n",
        "            labels_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    labels_ids.append(IGNORE_INDEX)\n",
        "                elif word_idx != prev_word_idx:\n",
        "                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n",
        "                else:\n",
        "                    if HyperParameters.label_subtokens:\n",
        "                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n",
        "                    else:\n",
        "                        labels_ids.append(IGNORE_INDEX)\n",
        "                prev_word_idx = word_idx\n",
        "            encoding['labels'] = labels_ids\n",
        "        # convert to torch.tensor\n",
        "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
        "        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\n",
        "        item['word_ids'] = torch.as_tensor(word_ids2)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE6pC1LdUxuP"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9UQQRt2UxuP"
      },
      "outputs": [],
      "source": [
        "class FeedbackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeedbackModel, self).__init__()\n",
        "        \n",
        "        # init config of transformer model of choice:\n",
        "        # NOTE: All hyperparameters of the transformer, INCLUDING THE SLIDING WINDOW, are accessible in here!\n",
        "        model_config = AutoConfig.from_pretrained(HyperParameters.model_name)\n",
        "        self.backbone = AutoModel.from_pretrained(HyperParameters.model_name, config=model_config)\n",
        "        \n",
        "        # There's a paper on why this weird dropout strategy is beneficial: https://arxiv.org/abs/1905.09788\n",
        "        self.model_config = model_config\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.dropout4 = nn.Dropout(0.4)\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "        self.head = nn.Linear(model_config.hidden_size, HyperParameters.num_labels)\n",
        "    \n",
        "    def forward(self, input_ids, mask):\n",
        "        x = self.backbone(input_ids, mask)\n",
        "        logits1 = self.head(self.dropout1(x[0]))\n",
        "        logits2 = self.head(self.dropout2(x[0]))\n",
        "        logits3 = self.head(self.dropout3(x[0]))\n",
        "        logits4 = self.head(self.dropout4(x[0]))\n",
        "        logits5 = self.head(self.dropout5(x[0]))\n",
        "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RslNIsyMUxuP"
      },
      "outputs": [],
      "source": [
        "def build_model_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(HyperParameters.model_name, add_prefix_space = True)\n",
        "    model = FeedbackModel()\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PriptyGpUxuP"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmRQM_TPUxuQ"
      },
      "source": [
        "What does this do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJQ0r-yUxuQ"
      },
      "outputs": [],
      "source": [
        "# Need help with this: used in training to transform raw logits to labels needed\n",
        "def active_logits(raw_logits, word_ids):\n",
        "    word_ids = word_ids.view(-1)\n",
        "    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], HyperParameters.num_labels)\n",
        "    active_mask = active_mask != NON_LABEL\n",
        "    active_logits = raw_logits.view(-1, HyperParameters.num_labels)\n",
        "    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\n",
        "    active_logits = active_logits.view(-1, HyperParameters.num_labels) \n",
        "    return active_logits\n",
        "\n",
        "def active_labels(labels):\n",
        "    active_mask = labels.view(-1) != IGNORE_INDEX\n",
        "    active_labels = torch.masked_select(labels.view(-1), active_mask)\n",
        "    return active_labels\n",
        "\n",
        "def active_preds_prob(active_logits):\n",
        "    active_preds = torch.argmax(active_logits, axis = 1)\n",
        "    active_preds_prob, _ = torch.max(active_logits, axis = 1)\n",
        "    return active_preds, active_preds_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkokQcdsUxuQ"
      },
      "source": [
        "F1 scoring functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfhEUmMcUxuQ"
      },
      "outputs": [],
      "source": [
        "def calculate_overlap(set_pred, set_gt):\n",
        "    \"\"\"\n",
        "    Calculates if the overlap between prediction and\n",
        "    ground truth is enough fora potential True positive\n",
        "    \"\"\"\n",
        "    # Length of each and intersection\n",
        "    try:\n",
        "        len_gt = len(set_gt)\n",
        "        len_pred = len(set_pred)\n",
        "        inter = len(set_gt & set_pred)\n",
        "        overlap_1 = inter / len_gt\n",
        "        overlap_2 = inter/ len_pred\n",
        "        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n",
        "    except:  # at least one of the input is NaN\n",
        "        return False\n",
        "\n",
        "def score_feedback_comp_micro(pred_df, gt_df, discourse_type):\n",
        "    \"\"\"\n",
        "    A function that scores for the kaggle\n",
        "        Student Writing Competition\n",
        "        \n",
        "    Uses the steps in the evaluation page here:\n",
        "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
        "    \"\"\"\n",
        "    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n",
        "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
        "    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n",
        "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
        "    pred_df['pred_id'] = pred_df.index\n",
        "    gt_df['gt_id'] = gt_df.index\n",
        "    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n",
        "    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n",
        "    \n",
        "    # Step 1. all ground truths and predictions for a given class are compared.\n",
        "    joined = pred_df.merge(gt_df,\n",
        "                           left_on='id',\n",
        "                           right_on='id',\n",
        "                           how='outer',\n",
        "                           suffixes=('_pred','_gt')\n",
        "                          )\n",
        "    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \n",
        "                                                     joined.predictionstring_gt)]\n",
        "    \n",
        "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
        "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
        "    # the prediction is a match and considered a true positive.\n",
        "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
        "    # we don't need to compute the match to compute the score\n",
        "    TP = joined.loc[overlaps]['gt_id'].nunique()\n",
        "\n",
        "    # 3. Any unmatched ground truths are false negatives\n",
        "    # and any unmatched predictions are false positives.\n",
        "    TPandFP = len(pred_df)\n",
        "    TPandFN = len(gt_df)\n",
        "    \n",
        "    #calc microf1\n",
        "    my_f1_score = 2*TP / (TPandFP + TPandFN)\n",
        "    return my_f1_score\n",
        "\n",
        "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
        "    \"\"\"\n",
        "    Final helper function for model evaluation.\n",
        "    \n",
        "    Args:\n",
        "    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: ['id','class','predictionstring']\n",
        "    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\n",
        "    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\n",
        "    \n",
        "    Returns:\n",
        "    f1                      (float): F1 score of the model\n",
        "    (optional) class_scores  (dict): Dictionary of per-class F1 score\n",
        "    \"\"\"\n",
        "    class_scores = {}\n",
        "    for discourse_type in gt_df.discourse_type.unique():\n",
        "        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\n",
        "        class_scores[discourse_type] = class_score\n",
        "    f1 = np.mean([v for v in class_scores.values()])\n",
        "    if return_class_scores:\n",
        "        return f1, class_scores\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIQdqtzoUxuR"
      },
      "source": [
        "# Training and validation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GrUCjcbUxuR"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSeGDvufUxuR"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, train_data_loader, optimizer, epoch, criterion):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    stream = tqdm(train_data_loader)\n",
        "    # Init gradscaler to ensure everything works smoothly on cuda\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for batch_idx, batch in enumerate(stream, start = 1):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        raw_labels = batch['labels'].to(device, dtype = torch.long)\n",
        "        word_ids = batch['word_ids'].to(device, dtype = torch.long)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Calculate output with autocast for cuda support\n",
        "        with autocast():\n",
        "            raw_logits = model(input_ids = ids, mask = mask)\n",
        "        \n",
        "        logits = active_logits(raw_logits, word_ids)\n",
        "        labels = active_labels(raw_labels)\n",
        "        sf_logits = torch.softmax(logits, dim=-1)\n",
        "        preds, preds_prob = active_preds_prob(sf_logits)\n",
        "        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        if batch_idx % HyperParameters.verbose_steps == 0:\n",
        "            loss_step = train_loss / batch_idx\n",
        "            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\n",
        "            \n",
        "    epoch_loss = train_loss / batch_idx\n",
        "    epoch_accuracy = train_accuracy / batch_idx    \n",
        "    # Cleanup\n",
        "    del train_data_loader, raw_logits, logits, raw_labels, preds, labels\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\n",
        "    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRR3l8npUxuR"
      },
      "source": [
        "### Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj0dSxx0UxuR"
      },
      "outputs": [],
      "source": [
        "def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\n",
        "    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\n",
        "    f1score =[]\n",
        "    # classes = oof['class'].unique()\n",
        "    classes = ['Lead', 'Position', 'Claim','Counterclaim', 'Rebuttal','Evidence','Concluding Statement']\n",
        "    print(f\"Validation F1 scores\")\n",
        "\n",
        "    for c in classes:\n",
        "        pred_df = oof.loc[oof['class'] == c].copy()\n",
        "        gt_df = df_val_eval.loc[df_val_eval['discourse_type'] == c].copy()\n",
        "        f1 = score_feedback_comp(pred_df, gt_df)\n",
        "        print(f' * {c:<10}: {f1:4f}')\n",
        "        f1score.append(f1)\n",
        "    f1avg = np.mean(f1score)\n",
        "    print(f'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}')\n",
        "    return valid_loss, oof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSZ3iq8dUxuS"
      },
      "source": [
        "### Infer on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qtgxPTlUxuS"
      },
      "outputs": [],
      "source": [
        "def inference(model, data_loader, criterion, valid_flg):\n",
        "    stream = tqdm(data_loader)\n",
        "    model.eval()\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_accuracy = 0\n",
        "    all_logits = None\n",
        "    for batch_idx, batch in enumerate(stream, start = 1):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        with torch.no_grad():\n",
        "            raw_logits = model(input_ids=ids, mask = mask)\n",
        "        del ids, mask\n",
        "        \n",
        "        word_ids = batch['word_ids'].to(device, dtype = torch.long)\n",
        "        logits = active_logits(raw_logits, word_ids)\n",
        "        sf_logits = torch.softmax(logits, dim= -1)\n",
        "        sf_raw_logits = torch.softmax(raw_logits, dim=-1)\n",
        "        if valid_flg:    \n",
        "            raw_labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            labels = active_labels(raw_labels)\n",
        "            preds, preds_prob = active_preds_prob(sf_logits)\n",
        "            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "            loss = criterion(logits, labels)\n",
        "            valid_loss += loss.item()\n",
        "        \n",
        "        if batch_idx == 1:\n",
        "            all_logits = sf_raw_logits.cpu().numpy()\n",
        "        else:\n",
        "            all_logits = np.append(all_logits, sf_raw_logits.cpu().numpy(), axis=0)\n",
        "\n",
        "    \n",
        "    if valid_flg:        \n",
        "        epoch_loss = valid_loss / batch_idx\n",
        "        epoch_accuracy = valid_accuracy / batch_idx\n",
        "    else:\n",
        "        epoch_loss, epoch_accuracy = 0, 0\n",
        "    return all_logits, epoch_loss, epoch_accuracy\n",
        "\n",
        "\n",
        "def preds_class_prob(all_logits, data_loader):\n",
        "    print(\"predict target class and its probabilty\")\n",
        "    final_predictions = []\n",
        "    final_predictions_score = []\n",
        "    stream = tqdm(data_loader)\n",
        "    len_sample = all_logits.shape[0]\n",
        "\n",
        "    for batch_idx, batch in enumerate(stream, start=0):\n",
        "        for minibatch_idx in range(HyperParameters.valid_batch_size):\n",
        "            sample_idx = int(batch_idx * HyperParameters.valid_batch_size + minibatch_idx)\n",
        "            if sample_idx > len_sample - 1 : break\n",
        "            word_ids = batch['word_ids'][minibatch_idx].numpy()\n",
        "            predictions =[]\n",
        "            predictions_prob = []\n",
        "            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\n",
        "            pred_score = np.max(all_logits[sample_idx], axis=1)\n",
        "            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\n",
        "            prev_word_idx = -1\n",
        "            for idx, word_idx in enumerate(word_ids):\n",
        "                if word_idx == -1:\n",
        "                    pass\n",
        "                elif word_idx != prev_word_idx:\n",
        "                    predictions.append(pred_class_labels[idx])\n",
        "                    predictions_prob.append(pred_score[idx])\n",
        "                    prev_word_idx = word_idx\n",
        "            final_predictions.append(predictions)\n",
        "            final_predictions_score.append(predictions_prob)\n",
        "    return final_predictions, final_predictions_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5dGJEzDUxuS"
      },
      "outputs": [],
      "source": [
        "def get_preds_onefold(model, df, dl, criterion, valid_flg):\n",
        "    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\n",
        "    all_preds, all_preds_prob = preds_class_prob(logits, dl)\n",
        "    df_pred = post_process_pred(df, all_preds, all_preds_prob)\n",
        "    return df_pred, valid_loss, valid_acc\n",
        "\n",
        "def get_preds_folds(model, df, dl, criterion, valid_flg=False):\n",
        "    for i_fold in range(HyperParameters.n_fold):\n",
        "        model_filename = os.path.join(HyperParameters.model_dir, f\"{HyperParameters.model_savename}_{i_fold}.bin\")\n",
        "        print(f\"{model_filename} inference\")\n",
        "        model = model.to(device)\n",
        "        model.load_state_dict(torch.load(model_filename))\n",
        "        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\n",
        "        if i_fold == 0:\n",
        "            avg_pred_logits = logits\n",
        "        else:\n",
        "            avg_pred_logits += logits\n",
        "    avg_pred_logits /= HyperParameters.n_fold\n",
        "    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\n",
        "    df_pred = post_process_pred(df, all_preds, all_preds_prob)\n",
        "    return df_pred\n",
        "\n",
        "def post_process_pred(df, all_preds, all_preds_prob):\n",
        "    final_preds = []\n",
        "    for i in range(len(df)):\n",
        "        idx = df.id.values[i]\n",
        "        pred = all_preds[i]\n",
        "        pred_prob = all_preds_prob[i]\n",
        "        j = 0\n",
        "        while j < len(pred):\n",
        "            cls = pred[j]\n",
        "            if cls == 'O': j += 1\n",
        "            else: cls = cls.replace('B', 'I')\n",
        "            end = j + 1\n",
        "            while end < len(pred) and pred[end] == cls:\n",
        "                end += 1\n",
        "            if cls != 'O' and cls !='':\n",
        "                avg_score = np.mean(pred_prob[j:end])\n",
        "                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\n",
        "                    final_preds.append((idx, cls.replace('I-', ''), ' '.join(map(str, list(range(j, end))))))\n",
        "            j = end\n",
        "    df_pred = pd.DataFrame(final_preds)\n",
        "    df_pred.columns = ['id', 'class', 'predictionstring']\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAmJyKnHUxuS"
      },
      "source": [
        "# Finally getting some action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dHGvjpPUxuS"
      },
      "outputs": [],
      "source": [
        "oof = pd.DataFrame()\n",
        "\n",
        "for i_fold in range(HyperParameters.n_fold):\n",
        "    print(f'=== fold{i_fold} training ===')\n",
        "    model, tokenizer = build_model_tokenizer()\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=HyperParameters.lr)\n",
        "    \n",
        "    df_train = alltrain_texts[alltrain_texts[\"kfold\"] != i_fold].reset_index(drop = True)\n",
        "    ds_train = FeedbackPrizeDataset(df_train, tokenizer, HyperParameters.max_length, True)\n",
        "    df_val = alltrain_texts[alltrain_texts[\"kfold\"] == i_fold].reset_index(drop = True)\n",
        "    val_idlist = df_val['id'].unique().tolist()\n",
        "    df_val_eval = df_alltrain.query('id==@val_idlist').reset_index(drop=True)\n",
        "    ds_val = FeedbackPrizeDataset(df_val, tokenizer, HyperParameters.max_length, True)\n",
        "    dl_train = DataLoader(ds_train, batch_size=HyperParameters.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    dl_val = DataLoader(ds_val, batch_size=HyperParameters.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, HyperParameters.n_epoch + 1):\n",
        "        train_fn(model, dl_train, optimizer, epoch, criterion)\n",
        "        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion)\n",
        "        if valid_loss < best_val_loss:\n",
        "            best_val_loss = valid_loss\n",
        "            _oof_fold_best = _oof\n",
        "            _oof_fold_best[\"kfold\"] = i_fold\n",
        "            model_filename = f'{HyperParameters.model_dir}/{HyperParameters.model_savename}_{i_fold}.bin'\n",
        "            \n",
        "            # Saving the boy\n",
        "            torch.save(model.state_dict(), model_filename)\n",
        "            print(f'{model_filename} saved')\n",
        "\n",
        "    oof = pd.concat([oof, _oof_fold_best])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlmZeM47UxuT"
      },
      "outputs": [],
      "source": [
        "oof.to_csv(f'{HyperParameters.output_dir}/oof_{HyperParameters.name}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO8hcaQ8UxuT"
      },
      "source": [
        "Looking at performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOcFtSUhUxuT"
      },
      "outputs": [],
      "source": [
        "if HyperParameters.is_debug:\n",
        "    idlist = alltrain_texts['id'].unique().tolist()\n",
        "    df_train = df_alltrain.query('id==@idlist')\n",
        "else:\n",
        "    df_train = df_alltrain.copy()\n",
        "print(f'overall cv score: {score_feedback_comp(df_train, oof)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Longformer.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "be336ab25ba919cf0a65f4be83b938febf47f529e9f75dfb16359f541885e1c6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('machinevision')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
