{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "This notebook includes a set of utility functions (stolen from kaggle), to calculate performance metrics for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from: https://www.kaggle.com/cpmpml/faster-metric-computation \n",
    "\n",
    "def calculate_overlap(set_pred, set_gt):\n",
    "    \"\"\"\n",
    "    Calculates if the overlap between prediction and\n",
    "    ground truth is enough fora potential True positive\n",
    "    \"\"\"\n",
    "    # Length of each and intersection\n",
    "    try:\n",
    "        len_gt = len(set_gt)\n",
    "        len_pred = len(set_pred)\n",
    "        inter = len(set_gt & set_pred)\n",
    "        overlap_1 = inter / len_gt\n",
    "        overlap_2 = inter/ len_pred\n",
    "        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n",
    "    except:  # at least one of the input is NaN\n",
    "        return False\n",
    "\n",
    "def score_feedback_comp_micro(pred_df, gt_df, discourse_type):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n",
    "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n",
    "                      ['id', 'predictionstring']].reset_index(drop=True)\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n",
    "    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n",
    "    \n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on='id',\n",
    "                           right_on='id',\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    overlaps = [calculate_overlap(*args) for args in zip(joined.predictionstring_pred, \n",
    "                                                     joined.predictionstring_gt)]\n",
    "    \n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    # we don't need to compute the match to compute the score\n",
    "    TP = joined.loc[overlaps]['gt_id'].nunique()\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    TPandFP = len(pred_df)\n",
    "    TPandFN = len(gt_df)\n",
    "    \n",
    "    #calc microf1\n",
    "    my_f1_score = 2*TP / (TPandFP + TPandFN)\n",
    "    return my_f1_score\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n",
    "    \"\"\"\n",
    "    Helper function for model evaluation.\n",
    "    \n",
    "    Args:\n",
    "    pred_df  (pandas.DataFrame): dataframe containing model predictions. Needs to have columns: ['id','class','predictionstring']\n",
    "    gt_df    (pandas.DataFrame): dataframe of ground truth used for model training\n",
    "    return_class_scores  (bool): Boolean indicating if we want to return the F1 score for each predicted class.\n",
    "    \n",
    "    Returns:\n",
    "    f1                      (float): F1 score of the model\n",
    "    (optional) class_scores  (dict): Dictionary of per-class F1 score\n",
    "    \"\"\"\n",
    "    class_scores = {}\n",
    "    for discourse_type in gt_df.discourse_type.unique():\n",
    "        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\n",
    "        class_scores[discourse_type] = class_score\n",
    "    f1 = np.mean([v for v in class_scores.values()])\n",
    "    if return_class_scores:\n",
    "        return f1, class_scores\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data and scoring \n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "pred_df = train_df[['id','discourse_type','predictionstring']].copy()\n",
    "pred_df.columns = ['id','class','predictionstring']\n",
    "\n",
    "# Here scoring the train data onto itself\n",
    "score_feedback_comp(pred_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here scoring a sample of it\n",
    "pred_df2 = pred_df.sample(frac=0.7).reset_index(drop=True)\n",
    "score_feedback_comp(pred_df2, train_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be336ab25ba919cf0a65f4be83b938febf47f529e9f75dfb16359f541885e1c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('machinevision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
